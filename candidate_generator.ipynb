{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example from https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      " asin             object\n",
      "name             object\n",
      "rating            int64\n",
      "date             object\n",
      "verified           bool\n",
      "title            object\n",
      "body             object\n",
      "helpfulVotes    float64\n",
      "dtype: object\n",
      "Shape of database = (82815, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema:\\n\", df_idf.dtypes)\n",
    "print(\"Shape of database =\", df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love this phone this is a great reliable phone i also purchased this phone after my samsung a died the menu is easily comprehendable and speed dialing is available for around numbers voice dialing is also a nice feature but it takes longer than speed dialing the only thing that bothers me is the games nokia seems to have taken snake and off their phones there is a skydiving game bowling and tennis like pong the ringers are very nice and a feature is available to choose a different ringer for each person calling however ringtones are not available online to download to this phone you re pretty much stuck with what you have there are vibrating ringtones and regular midi polyphonic tones all they need are covers in a reasonable price range '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stop_words('stopwords.txt')\n",
    "docs = df_idf['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df = .85, stop_words=stopwords)\n",
    "wordCountVec = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'best',\n",
       " 'worst',\n",
       " 'samsung',\n",
       " 'awhile',\n",
       " 'absolute',\n",
       " 'doo',\n",
       " 'read',\n",
       " 'review',\n",
       " 'detect']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf.fit(wordCountVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1] , x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        \n",
    "        results = {}\n",
    "        for idx in range(len(feature_vals)):\n",
    "            results[feature_vals[idx]]=score_vals[idx]\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "doc = docs[1]\n",
    "\n",
    "tf_idf_vector = tfidf.transform(cv.transform([doc]))\n",
    "\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "keywords = extract_topn_from_vector(feature_names, sorted_items, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sprint': 0.442}\n"
     ]
    }
   ],
   "source": [
    "test = cv.transform([doc])\n",
    "\n",
    "print(keywords)\n",
    "# for idx in range(len(sorted_items)):\n",
    "#     print(feature_names[sorted_items[idx][0]], sorted_items[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82815, 34848) (82815,)\n",
      "(41407, 34848) (41407,)\n",
      "(41408, 34848) (41408,)\n"
     ]
    }
   ],
   "source": [
    "y = df_idf['rating']\n",
    "# fixing the labels, if > 3.5 is going to be 1 which is positive, else 0\n",
    "y = y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()\n",
    "x = wordCountVec.toarray()\n",
    "print(x.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[:, :], y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a binary file\n",
    "import pickle\n",
    "filename = 'model2.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from the binary file\n",
    "import pickle\n",
    "filename = 'model.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[:, 0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_p = clf.predict(X_train)\n",
    "# y_test_p = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y_test)):\n",
    "#     print(y_test_p[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# print(\"Accuracy in testing set:\", accuracy_score(y_test, y_test_p))\n",
    "# print(\"Accuracy in training set:\", accuracy_score(y_train, y_train_p))\n",
    "# print(confusion_matrix(y_test, y_test_p))\n",
    "# print(confusion_matrix(y_train, y_train_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 34848)\n"
     ]
    }
   ],
   "source": [
    "test = cv.transform([\"Hate\", \"Good\", \"Awful\", \"Best\"]).toarray()\n",
    "clf.predict(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(110, 120):\n",
    "#     test = cv.transform([docs[i]]).toarray()\n",
    "#     p = clf.predict(test)\n",
    "#     print(docs[i], p, y[i])\n",
    "    \n",
    "#     print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape):\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='relu'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               17842688  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 17,974,273\n",
      "Trainable params: 17,974,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 34848)             35719200  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 34848)             0         \n",
      "=================================================================\n",
      "Total params: 36,409,120\n",
      "Trainable params: 36,405,536\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "\n",
    "img_rows = 1\n",
    "img_cols = X_train[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(self, pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.587166, acc.: 65.62%] [G loss: 6.348205]\n",
      "1 [D loss: 0.574017, acc.: 67.97%] [G loss: 5.664258]\n",
      "2 [D loss: 0.555976, acc.: 67.19%] [G loss: 4.776833]\n",
      "3 [D loss: 0.560892, acc.: 62.50%] [G loss: 5.411107]\n",
      "4 [D loss: 0.593142, acc.: 66.41%] [G loss: 5.350187]\n",
      "5 [D loss: 0.555765, acc.: 68.75%] [G loss: 6.015429]\n",
      "6 [D loss: 0.565835, acc.: 65.62%] [G loss: 5.533709]\n",
      "7 [D loss: 0.520985, acc.: 71.88%] [G loss: 4.854037]\n",
      "8 [D loss: 0.586494, acc.: 63.28%] [G loss: 5.781159]\n",
      "9 [D loss: 0.548066, acc.: 67.19%] [G loss: 5.086875]\n",
      "10 [D loss: 0.540024, acc.: 66.41%] [G loss: 5.375452]\n",
      "11 [D loss: 0.582889, acc.: 66.41%] [G loss: 5.000913]\n",
      "12 [D loss: 0.543629, acc.: 66.41%] [G loss: 5.040783]\n",
      "13 [D loss: 0.603248, acc.: 62.50%] [G loss: 4.897422]\n",
      "14 [D loss: 0.623279, acc.: 60.16%] [G loss: 5.110558]\n",
      "15 [D loss: 0.579054, acc.: 67.19%] [G loss: 6.208938]\n",
      "16 [D loss: 0.596465, acc.: 63.28%] [G loss: 4.833700]\n",
      "17 [D loss: 0.544111, acc.: 68.75%] [G loss: 5.006203]\n",
      "18 [D loss: 0.622012, acc.: 60.16%] [G loss: 5.671313]\n",
      "19 [D loss: 0.544579, acc.: 68.75%] [G loss: 4.838529]\n",
      "20 [D loss: 0.646340, acc.: 63.28%] [G loss: 5.452820]\n",
      "21 [D loss: 0.563427, acc.: 64.84%] [G loss: 4.750044]\n",
      "22 [D loss: 0.527805, acc.: 70.31%] [G loss: 4.333166]\n",
      "23 [D loss: 0.598881, acc.: 63.28%] [G loss: 5.334025]\n",
      "24 [D loss: 0.534889, acc.: 67.97%] [G loss: 5.193111]\n",
      "25 [D loss: 0.589193, acc.: 62.50%] [G loss: 4.817771]\n",
      "26 [D loss: 0.613720, acc.: 64.84%] [G loss: 4.606196]\n",
      "27 [D loss: 0.587536, acc.: 65.62%] [G loss: 4.580135]\n",
      "28 [D loss: 0.560770, acc.: 67.97%] [G loss: 4.845551]\n",
      "29 [D loss: 0.617405, acc.: 64.06%] [G loss: 4.520958]\n",
      "30 [D loss: 0.521516, acc.: 67.97%] [G loss: 5.296923]\n",
      "31 [D loss: 0.585872, acc.: 63.28%] [G loss: 4.834558]\n",
      "32 [D loss: 0.549929, acc.: 65.62%] [G loss: 5.476198]\n",
      "33 [D loss: 0.563621, acc.: 63.28%] [G loss: 4.901671]\n",
      "34 [D loss: 0.576287, acc.: 60.94%] [G loss: 4.740188]\n",
      "35 [D loss: 0.556451, acc.: 64.84%] [G loss: 5.168393]\n",
      "36 [D loss: 0.538752, acc.: 67.19%] [G loss: 5.157356]\n",
      "37 [D loss: 0.567072, acc.: 63.28%] [G loss: 4.789657]\n",
      "38 [D loss: 0.516554, acc.: 69.53%] [G loss: 4.693504]\n",
      "39 [D loss: 0.607108, acc.: 61.72%] [G loss: 4.910538]\n",
      "40 [D loss: 0.579623, acc.: 65.62%] [G loss: 4.678666]\n",
      "41 [D loss: 0.578214, acc.: 64.84%] [G loss: 5.243235]\n",
      "42 [D loss: 0.527176, acc.: 67.97%] [G loss: 4.573357]\n",
      "43 [D loss: 0.545280, acc.: 65.62%] [G loss: 4.530940]\n",
      "44 [D loss: 0.527098, acc.: 68.75%] [G loss: 4.758790]\n",
      "45 [D loss: 0.540916, acc.: 67.97%] [G loss: 5.287327]\n",
      "46 [D loss: 0.571063, acc.: 65.62%] [G loss: 4.620697]\n",
      "47 [D loss: 0.603687, acc.: 59.38%] [G loss: 5.008417]\n",
      "48 [D loss: 0.547838, acc.: 63.28%] [G loss: 4.561736]\n",
      "49 [D loss: 0.614336, acc.: 60.94%] [G loss: 4.978655]\n",
      "50 [D loss: 0.590010, acc.: 62.50%] [G loss: 5.245819]\n",
      "51 [D loss: 0.637360, acc.: 53.91%] [G loss: 5.248256]\n",
      "52 [D loss: 0.630766, acc.: 64.84%] [G loss: 4.254433]\n",
      "53 [D loss: 0.546510, acc.: 66.41%] [G loss: 4.041527]\n",
      "54 [D loss: 0.558976, acc.: 64.84%] [G loss: 5.098327]\n",
      "55 [D loss: 0.535130, acc.: 67.97%] [G loss: 4.798057]\n",
      "56 [D loss: 0.557874, acc.: 63.28%] [G loss: 4.811357]\n",
      "57 [D loss: 0.652315, acc.: 57.03%] [G loss: 4.875569]\n",
      "58 [D loss: 0.583762, acc.: 60.16%] [G loss: 4.639204]\n",
      "59 [D loss: 0.549852, acc.: 60.94%] [G loss: 3.930347]\n",
      "60 [D loss: 0.633284, acc.: 57.81%] [G loss: 4.161218]\n",
      "61 [D loss: 0.619948, acc.: 60.16%] [G loss: 4.534938]\n",
      "62 [D loss: 0.659533, acc.: 54.69%] [G loss: 4.107024]\n",
      "63 [D loss: 0.557477, acc.: 66.41%] [G loss: 5.383061]\n",
      "64 [D loss: 0.550443, acc.: 64.06%] [G loss: 4.554290]\n",
      "65 [D loss: 0.509095, acc.: 65.62%] [G loss: 4.360021]\n",
      "66 [D loss: 0.628828, acc.: 59.38%] [G loss: 4.291086]\n",
      "67 [D loss: 0.583922, acc.: 62.50%] [G loss: 4.824054]\n",
      "68 [D loss: 0.526096, acc.: 63.28%] [G loss: 4.690115]\n",
      "69 [D loss: 0.534320, acc.: 67.97%] [G loss: 5.341020]\n",
      "70 [D loss: 0.552241, acc.: 67.19%] [G loss: 4.561244]\n",
      "71 [D loss: 0.509066, acc.: 68.75%] [G loss: 5.174272]\n",
      "72 [D loss: 0.582559, acc.: 57.81%] [G loss: 5.067722]\n",
      "73 [D loss: 0.589566, acc.: 63.28%] [G loss: 4.444193]\n",
      "74 [D loss: 0.561318, acc.: 61.72%] [G loss: 4.549556]\n",
      "75 [D loss: 0.566931, acc.: 64.84%] [G loss: 5.448100]\n",
      "76 [D loss: 0.521682, acc.: 67.97%] [G loss: 5.104513]\n",
      "77 [D loss: 0.557557, acc.: 63.28%] [G loss: 4.693254]\n",
      "78 [D loss: 0.531783, acc.: 64.06%] [G loss: 5.082307]\n",
      "79 [D loss: 0.604596, acc.: 59.38%] [G loss: 4.343960]\n",
      "80 [D loss: 0.570812, acc.: 61.72%] [G loss: 4.217484]\n",
      "81 [D loss: 0.524215, acc.: 71.88%] [G loss: 5.017139]\n",
      "82 [D loss: 0.527799, acc.: 67.19%] [G loss: 4.959549]\n",
      "83 [D loss: 0.622127, acc.: 56.25%] [G loss: 4.816664]\n",
      "84 [D loss: 0.527025, acc.: 76.56%] [G loss: 4.710853]\n",
      "85 [D loss: 0.519975, acc.: 67.19%] [G loss: 4.598953]\n",
      "86 [D loss: 0.543617, acc.: 67.19%] [G loss: 4.716280]\n",
      "87 [D loss: 0.581715, acc.: 61.72%] [G loss: 4.524453]\n",
      "88 [D loss: 0.535313, acc.: 75.00%] [G loss: 4.865006]\n",
      "89 [D loss: 0.584430, acc.: 64.84%] [G loss: 4.910286]\n",
      "90 [D loss: 0.547549, acc.: 64.84%] [G loss: 4.910984]\n",
      "91 [D loss: 0.567922, acc.: 60.16%] [G loss: 4.639808]\n",
      "92 [D loss: 0.550420, acc.: 63.28%] [G loss: 4.503195]\n",
      "93 [D loss: 0.594168, acc.: 61.72%] [G loss: 4.494020]\n",
      "94 [D loss: 0.621356, acc.: 54.69%] [G loss: 4.114093]\n",
      "95 [D loss: 0.614233, acc.: 60.16%] [G loss: 4.124449]\n",
      "96 [D loss: 0.585689, acc.: 59.38%] [G loss: 4.885483]\n",
      "97 [D loss: 0.612993, acc.: 61.72%] [G loss: 4.707603]\n",
      "98 [D loss: 0.566538, acc.: 64.84%] [G loss: 3.993531]\n",
      "99 [D loss: 0.626902, acc.: 57.81%] [G loss: 4.236678]\n",
      "100 [D loss: 0.589551, acc.: 57.03%] [G loss: 4.712063]\n",
      "101 [D loss: 0.609718, acc.: 57.03%] [G loss: 4.443808]\n",
      "102 [D loss: 0.544187, acc.: 61.72%] [G loss: 5.175163]\n",
      "103 [D loss: 0.574280, acc.: 59.38%] [G loss: 4.827868]\n",
      "104 [D loss: 0.643651, acc.: 51.56%] [G loss: 4.333345]\n",
      "105 [D loss: 0.541820, acc.: 73.44%] [G loss: 3.767009]\n",
      "106 [D loss: 0.657854, acc.: 57.03%] [G loss: 3.808982]\n",
      "107 [D loss: 0.630009, acc.: 60.94%] [G loss: 4.003784]\n",
      "108 [D loss: 0.576666, acc.: 57.81%] [G loss: 4.448130]\n",
      "109 [D loss: 0.545995, acc.: 67.97%] [G loss: 4.388168]\n",
      "110 [D loss: 0.534830, acc.: 64.06%] [G loss: 4.246476]\n",
      "111 [D loss: 0.590877, acc.: 60.16%] [G loss: 4.716610]\n",
      "112 [D loss: 0.626699, acc.: 57.03%] [G loss: 3.931889]\n",
      "113 [D loss: 0.621972, acc.: 58.59%] [G loss: 3.786166]\n",
      "114 [D loss: 0.578759, acc.: 58.59%] [G loss: 4.062823]\n",
      "115 [D loss: 0.557186, acc.: 62.50%] [G loss: 4.148377]\n",
      "116 [D loss: 0.580274, acc.: 57.81%] [G loss: 4.230550]\n",
      "117 [D loss: 0.612026, acc.: 60.16%] [G loss: 4.355486]\n",
      "118 [D loss: 0.592364, acc.: 64.06%] [G loss: 4.093306]\n",
      "119 [D loss: 0.586616, acc.: 65.62%] [G loss: 4.779794]\n",
      "120 [D loss: 0.602586, acc.: 63.28%] [G loss: 4.228181]\n",
      "121 [D loss: 0.592295, acc.: 70.31%] [G loss: 3.740600]\n",
      "122 [D loss: 0.582835, acc.: 63.28%] [G loss: 4.363952]\n",
      "123 [D loss: 0.583808, acc.: 62.50%] [G loss: 3.636088]\n",
      "124 [D loss: 0.643738, acc.: 56.25%] [G loss: 4.507567]\n",
      "125 [D loss: 0.581576, acc.: 60.94%] [G loss: 4.538650]\n",
      "126 [D loss: 0.598500, acc.: 55.47%] [G loss: 4.719786]\n",
      "127 [D loss: 0.672386, acc.: 51.56%] [G loss: 4.409481]\n",
      "128 [D loss: 0.606087, acc.: 57.03%] [G loss: 4.211123]\n",
      "129 [D loss: 0.541773, acc.: 64.06%] [G loss: 4.928741]\n",
      "130 [D loss: 0.578079, acc.: 57.03%] [G loss: 3.797022]\n",
      "131 [D loss: 0.603827, acc.: 58.59%] [G loss: 4.319589]\n",
      "132 [D loss: 0.524715, acc.: 72.66%] [G loss: 4.051644]\n",
      "133 [D loss: 0.624144, acc.: 56.25%] [G loss: 4.541136]\n",
      "134 [D loss: 0.521610, acc.: 81.25%] [G loss: 4.953824]\n",
      "135 [D loss: 0.514279, acc.: 79.69%] [G loss: 5.013945]\n",
      "136 [D loss: 0.590409, acc.: 63.28%] [G loss: 4.283481]\n",
      "137 [D loss: 0.547378, acc.: 63.28%] [G loss: 4.326698]\n",
      "138 [D loss: 0.575087, acc.: 56.25%] [G loss: 4.121490]\n",
      "139 [D loss: 0.599200, acc.: 57.03%] [G loss: 4.348728]\n",
      "140 [D loss: 0.543107, acc.: 62.50%] [G loss: 4.050796]\n",
      "141 [D loss: 0.543854, acc.: 65.62%] [G loss: 4.397711]\n",
      "142 [D loss: 0.548047, acc.: 69.53%] [G loss: 4.232943]\n",
      "143 [D loss: 0.585848, acc.: 57.81%] [G loss: 3.783317]\n",
      "144 [D loss: 0.635959, acc.: 51.56%] [G loss: 3.739324]\n",
      "145 [D loss: 0.613819, acc.: 56.25%] [G loss: 4.403040]\n",
      "146 [D loss: 0.563213, acc.: 61.72%] [G loss: 4.496834]\n",
      "147 [D loss: 0.561370, acc.: 67.97%] [G loss: 3.791517]\n",
      "148 [D loss: 0.562510, acc.: 57.81%] [G loss: 4.829983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.581010, acc.: 60.16%] [G loss: 4.953751]\n",
      "150 [D loss: 0.545897, acc.: 60.94%] [G loss: 4.071805]\n",
      "151 [D loss: 0.596154, acc.: 55.47%] [G loss: 3.619408]\n",
      "152 [D loss: 0.668439, acc.: 47.66%] [G loss: 4.422657]\n",
      "153 [D loss: 0.588760, acc.: 56.25%] [G loss: 3.907811]\n",
      "154 [D loss: 0.589330, acc.: 53.12%] [G loss: 4.541921]\n",
      "155 [D loss: 0.559779, acc.: 58.59%] [G loss: 4.025639]\n",
      "156 [D loss: 0.544348, acc.: 75.78%] [G loss: 4.023120]\n",
      "157 [D loss: 0.531327, acc.: 67.97%] [G loss: 4.241893]\n",
      "158 [D loss: 0.590813, acc.: 74.22%] [G loss: 3.711803]\n",
      "159 [D loss: 0.636484, acc.: 65.62%] [G loss: 4.533108]\n",
      "160 [D loss: 0.553205, acc.: 63.28%] [G loss: 3.970257]\n",
      "161 [D loss: 0.586125, acc.: 53.91%] [G loss: 3.891372]\n",
      "162 [D loss: 0.705902, acc.: 46.88%] [G loss: 3.370425]\n",
      "163 [D loss: 0.606973, acc.: 57.03%] [G loss: 3.481086]\n",
      "164 [D loss: 0.547182, acc.: 82.03%] [G loss: 4.745164]\n",
      "165 [D loss: 0.523916, acc.: 73.44%] [G loss: 3.964365]\n",
      "166 [D loss: 0.569041, acc.: 62.50%] [G loss: 3.787670]\n",
      "167 [D loss: 0.614791, acc.: 55.47%] [G loss: 3.985020]\n",
      "168 [D loss: 0.592927, acc.: 59.38%] [G loss: 3.407960]\n",
      "169 [D loss: 0.629307, acc.: 53.91%] [G loss: 4.009580]\n",
      "170 [D loss: 0.550121, acc.: 65.62%] [G loss: 4.766919]\n",
      "171 [D loss: 0.543369, acc.: 71.88%] [G loss: 3.412767]\n",
      "172 [D loss: 0.560869, acc.: 61.72%] [G loss: 4.316257]\n",
      "173 [D loss: 0.557842, acc.: 61.72%] [G loss: 4.202093]\n",
      "174 [D loss: 0.549090, acc.: 61.72%] [G loss: 4.170682]\n",
      "175 [D loss: 0.588981, acc.: 65.62%] [G loss: 3.976665]\n",
      "176 [D loss: 0.555513, acc.: 58.59%] [G loss: 4.233046]\n",
      "177 [D loss: 0.634385, acc.: 56.25%] [G loss: 3.876859]\n",
      "178 [D loss: 0.665341, acc.: 53.91%] [G loss: 3.310754]\n",
      "179 [D loss: 0.562551, acc.: 64.06%] [G loss: 5.052780]\n",
      "180 [D loss: 0.474978, acc.: 93.75%] [G loss: 3.844326]\n",
      "181 [D loss: 0.556294, acc.: 64.06%] [G loss: 4.576384]\n",
      "182 [D loss: 0.510037, acc.: 80.47%] [G loss: 3.925646]\n",
      "183 [D loss: 0.518415, acc.: 78.12%] [G loss: 3.720871]\n",
      "184 [D loss: 0.596193, acc.: 65.62%] [G loss: 4.517022]\n",
      "185 [D loss: 0.579411, acc.: 59.38%] [G loss: 4.758238]\n",
      "186 [D loss: 0.608116, acc.: 58.59%] [G loss: 4.347631]\n",
      "187 [D loss: 0.553101, acc.: 64.06%] [G loss: 4.392556]\n",
      "188 [D loss: 0.560872, acc.: 67.19%] [G loss: 4.059781]\n",
      "189 [D loss: 0.585001, acc.: 56.25%] [G loss: 4.043770]\n",
      "190 [D loss: 0.635319, acc.: 53.91%] [G loss: 4.005040]\n",
      "191 [D loss: 0.565758, acc.: 57.03%] [G loss: 3.974995]\n",
      "192 [D loss: 0.622883, acc.: 56.25%] [G loss: 4.347639]\n",
      "193 [D loss: 0.547876, acc.: 67.97%] [G loss: 3.526950]\n",
      "194 [D loss: 0.567353, acc.: 73.44%] [G loss: 4.568871]\n",
      "195 [D loss: 0.624158, acc.: 53.91%] [G loss: 3.844930]\n",
      "196 [D loss: 0.586394, acc.: 58.59%] [G loss: 4.357322]\n",
      "197 [D loss: 0.619642, acc.: 58.59%] [G loss: 3.817762]\n",
      "198 [D loss: 0.604178, acc.: 57.03%] [G loss: 3.319902]\n",
      "199 [D loss: 0.546798, acc.: 80.47%] [G loss: 4.422834]\n",
      "200 [D loss: 0.513411, acc.: 66.41%] [G loss: 4.433993]\n",
      "201 [D loss: 0.563939, acc.: 64.06%] [G loss: 4.139089]\n",
      "202 [D loss: 0.680908, acc.: 56.25%] [G loss: 3.755750]\n",
      "203 [D loss: 0.606237, acc.: 51.56%] [G loss: 3.508317]\n",
      "204 [D loss: 0.589325, acc.: 58.59%] [G loss: 3.978168]\n",
      "205 [D loss: 0.555315, acc.: 80.47%] [G loss: 4.420148]\n",
      "206 [D loss: 0.601699, acc.: 54.69%] [G loss: 3.544901]\n",
      "207 [D loss: 0.577052, acc.: 59.38%] [G loss: 4.300622]\n",
      "208 [D loss: 0.517725, acc.: 79.69%] [G loss: 3.901193]\n",
      "209 [D loss: 0.604367, acc.: 59.38%] [G loss: 3.960220]\n",
      "210 [D loss: 0.597051, acc.: 57.81%] [G loss: 3.481791]\n",
      "211 [D loss: 0.588546, acc.: 71.88%] [G loss: 3.692463]\n",
      "212 [D loss: 0.567701, acc.: 62.50%] [G loss: 3.631736]\n",
      "213 [D loss: 0.561404, acc.: 61.72%] [G loss: 3.869980]\n",
      "214 [D loss: 0.566813, acc.: 58.59%] [G loss: 4.541963]\n",
      "215 [D loss: 0.607891, acc.: 57.03%] [G loss: 3.747864]\n",
      "216 [D loss: 0.547057, acc.: 67.97%] [G loss: 3.687006]\n",
      "217 [D loss: 0.584080, acc.: 71.09%] [G loss: 3.381907]\n",
      "218 [D loss: 0.647653, acc.: 53.12%] [G loss: 3.802232]\n",
      "219 [D loss: 0.636604, acc.: 60.16%] [G loss: 3.724250]\n",
      "220 [D loss: 0.580162, acc.: 69.53%] [G loss: 3.585054]\n",
      "221 [D loss: 0.605969, acc.: 63.28%] [G loss: 4.411315]\n",
      "222 [D loss: 0.646196, acc.: 53.12%] [G loss: 3.799546]\n",
      "223 [D loss: 0.598672, acc.: 62.50%] [G loss: 3.934810]\n",
      "224 [D loss: 0.592935, acc.: 57.03%] [G loss: 3.544610]\n",
      "225 [D loss: 0.651797, acc.: 50.78%] [G loss: 3.564741]\n",
      "226 [D loss: 0.628247, acc.: 69.53%] [G loss: 3.618865]\n",
      "227 [D loss: 0.562960, acc.: 74.22%] [G loss: 3.788212]\n",
      "228 [D loss: 0.588556, acc.: 61.72%] [G loss: 3.967011]\n",
      "229 [D loss: 0.590017, acc.: 58.59%] [G loss: 3.281806]\n",
      "230 [D loss: 0.644153, acc.: 56.25%] [G loss: 4.082875]\n",
      "231 [D loss: 0.535933, acc.: 62.50%] [G loss: 3.683467]\n",
      "232 [D loss: 0.654655, acc.: 63.28%] [G loss: 3.820251]\n",
      "233 [D loss: 0.612208, acc.: 57.81%] [G loss: 3.540725]\n",
      "234 [D loss: 0.611577, acc.: 57.81%] [G loss: 2.812278]\n",
      "235 [D loss: 0.605385, acc.: 53.91%] [G loss: 4.005158]\n",
      "236 [D loss: 0.580424, acc.: 60.16%] [G loss: 3.639098]\n",
      "237 [D loss: 0.558011, acc.: 76.56%] [G loss: 4.525793]\n",
      "238 [D loss: 0.569986, acc.: 70.31%] [G loss: 3.903762]\n",
      "239 [D loss: 0.544654, acc.: 72.66%] [G loss: 3.534546]\n",
      "240 [D loss: 0.645692, acc.: 52.34%] [G loss: 3.447525]\n",
      "241 [D loss: 0.563530, acc.: 65.62%] [G loss: 3.615934]\n",
      "242 [D loss: 0.613048, acc.: 63.28%] [G loss: 3.738458]\n",
      "243 [D loss: 0.675571, acc.: 63.28%] [G loss: 3.797367]\n",
      "244 [D loss: 0.497204, acc.: 79.69%] [G loss: 3.776824]\n",
      "245 [D loss: 0.546395, acc.: 70.31%] [G loss: 4.351072]\n",
      "246 [D loss: 0.596786, acc.: 59.38%] [G loss: 3.422699]\n",
      "247 [D loss: 0.602035, acc.: 60.94%] [G loss: 3.713040]\n",
      "248 [D loss: 0.620214, acc.: 58.59%] [G loss: 3.454795]\n",
      "249 [D loss: 0.626151, acc.: 58.59%] [G loss: 3.971107]\n",
      "250 [D loss: 0.533706, acc.: 69.53%] [G loss: 3.672664]\n",
      "251 [D loss: 0.646048, acc.: 57.81%] [G loss: 3.341965]\n",
      "252 [D loss: 0.838517, acc.: 65.62%] [G loss: 3.669128]\n",
      "253 [D loss: 0.547485, acc.: 85.94%] [G loss: 3.668066]\n",
      "254 [D loss: 0.628228, acc.: 58.59%] [G loss: 4.610729]\n",
      "255 [D loss: 0.576315, acc.: 69.53%] [G loss: 3.273270]\n",
      "256 [D loss: 0.603732, acc.: 62.50%] [G loss: 3.264262]\n",
      "257 [D loss: 0.649441, acc.: 53.12%] [G loss: 4.214195]\n",
      "258 [D loss: 0.581436, acc.: 60.16%] [G loss: 3.460304]\n",
      "259 [D loss: 0.600720, acc.: 83.59%] [G loss: 3.479701]\n",
      "260 [D loss: 0.542337, acc.: 85.94%] [G loss: 3.287784]\n",
      "261 [D loss: 0.573737, acc.: 64.84%] [G loss: 3.557081]\n",
      "262 [D loss: 0.553654, acc.: 63.28%] [G loss: 3.790331]\n",
      "263 [D loss: 0.595862, acc.: 57.81%] [G loss: 3.828824]\n",
      "264 [D loss: 0.642053, acc.: 54.69%] [G loss: 3.868362]\n",
      "265 [D loss: 0.582246, acc.: 64.06%] [G loss: 3.392952]\n",
      "266 [D loss: 0.610242, acc.: 57.81%] [G loss: 3.057560]\n",
      "267 [D loss: 0.542930, acc.: 84.38%] [G loss: 4.074517]\n",
      "268 [D loss: 0.564922, acc.: 85.16%] [G loss: 3.109006]\n",
      "269 [D loss: 0.611581, acc.: 59.38%] [G loss: 3.773827]\n",
      "270 [D loss: 0.581497, acc.: 57.81%] [G loss: 3.494337]\n",
      "271 [D loss: 0.633561, acc.: 52.34%] [G loss: 3.450846]\n",
      "272 [D loss: 0.611324, acc.: 56.25%] [G loss: 3.752757]\n",
      "273 [D loss: 0.558552, acc.: 74.22%] [G loss: 3.320097]\n",
      "274 [D loss: 0.578961, acc.: 74.22%] [G loss: 3.603832]\n",
      "275 [D loss: 0.581173, acc.: 59.38%] [G loss: 3.463018]\n",
      "276 [D loss: 0.564999, acc.: 75.78%] [G loss: 3.613433]\n",
      "277 [D loss: 0.538905, acc.: 77.34%] [G loss: 3.733140]\n",
      "278 [D loss: 0.567009, acc.: 72.66%] [G loss: 3.815210]\n",
      "279 [D loss: 0.547026, acc.: 64.84%] [G loss: 3.341661]\n",
      "280 [D loss: 0.595021, acc.: 59.38%] [G loss: 3.251134]\n",
      "281 [D loss: 0.619510, acc.: 55.47%] [G loss: 3.337671]\n",
      "282 [D loss: 0.596618, acc.: 55.47%] [G loss: 3.903986]\n",
      "283 [D loss: 0.543495, acc.: 78.12%] [G loss: 3.434347]\n",
      "284 [D loss: 0.600353, acc.: 57.03%] [G loss: 3.135361]\n",
      "285 [D loss: 0.538682, acc.: 64.84%] [G loss: 3.139421]\n",
      "286 [D loss: 0.525629, acc.: 67.97%] [G loss: 3.353772]\n",
      "287 [D loss: 0.551709, acc.: 82.03%] [G loss: 3.900037]\n",
      "288 [D loss: 0.544768, acc.: 66.41%] [G loss: 3.265560]\n",
      "289 [D loss: 0.621224, acc.: 50.78%] [G loss: 3.327024]\n",
      "290 [D loss: 0.597995, acc.: 64.06%] [G loss: 3.294911]\n",
      "291 [D loss: 0.607013, acc.: 54.69%] [G loss: 3.274664]\n",
      "292 [D loss: 0.579086, acc.: 57.03%] [G loss: 3.188142]\n",
      "293 [D loss: 0.571813, acc.: 84.38%] [G loss: 3.126000]\n",
      "294 [D loss: 0.628201, acc.: 53.12%] [G loss: 3.135090]\n",
      "295 [D loss: 0.658225, acc.: 55.47%] [G loss: 2.474511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.589038, acc.: 64.06%] [G loss: 3.295136]\n",
      "297 [D loss: 0.651098, acc.: 48.44%] [G loss: 3.127608]\n",
      "298 [D loss: 0.613583, acc.: 79.69%] [G loss: 3.865799]\n",
      "299 [D loss: 0.577296, acc.: 64.06%] [G loss: 2.940751]\n",
      "300 [D loss: 0.629288, acc.: 49.22%] [G loss: 3.297644]\n",
      "301 [D loss: 0.615208, acc.: 57.03%] [G loss: 2.921267]\n",
      "302 [D loss: 0.578261, acc.: 62.50%] [G loss: 2.782526]\n",
      "303 [D loss: 0.596350, acc.: 71.09%] [G loss: 2.890146]\n",
      "304 [D loss: 0.660574, acc.: 49.22%] [G loss: 3.186441]\n",
      "305 [D loss: 0.610281, acc.: 58.59%] [G loss: 2.601128]\n",
      "306 [D loss: 0.570572, acc.: 72.66%] [G loss: 2.730935]\n",
      "307 [D loss: 0.565530, acc.: 60.94%] [G loss: 3.031898]\n",
      "308 [D loss: 0.618450, acc.: 73.44%] [G loss: 2.791296]\n",
      "309 [D loss: 0.598730, acc.: 67.19%] [G loss: 3.239827]\n",
      "310 [D loss: 0.665566, acc.: 58.59%] [G loss: 2.837789]\n",
      "311 [D loss: 0.515110, acc.: 81.25%] [G loss: 3.142740]\n",
      "312 [D loss: 0.559197, acc.: 84.38%] [G loss: 2.957182]\n",
      "313 [D loss: 0.577430, acc.: 75.00%] [G loss: 3.516971]\n",
      "314 [D loss: 0.590537, acc.: 62.50%] [G loss: 3.541573]\n",
      "315 [D loss: 0.614350, acc.: 55.47%] [G loss: 3.036094]\n",
      "316 [D loss: 0.545168, acc.: 74.22%] [G loss: 2.701920]\n",
      "317 [D loss: 0.557112, acc.: 84.38%] [G loss: 3.088339]\n",
      "318 [D loss: 0.587290, acc.: 61.72%] [G loss: 3.158773]\n",
      "319 [D loss: 0.561112, acc.: 64.84%] [G loss: 2.896228]\n",
      "320 [D loss: 0.560831, acc.: 64.06%] [G loss: 3.820565]\n",
      "321 [D loss: 0.577545, acc.: 58.59%] [G loss: 3.040603]\n",
      "322 [D loss: 0.558796, acc.: 63.28%] [G loss: 3.070192]\n",
      "323 [D loss: 0.583482, acc.: 63.28%] [G loss: 2.914695]\n",
      "324 [D loss: 0.594153, acc.: 59.38%] [G loss: 2.919304]\n",
      "325 [D loss: 0.570675, acc.: 76.56%] [G loss: 3.334948]\n",
      "326 [D loss: 0.606410, acc.: 60.16%] [G loss: 2.931957]\n",
      "327 [D loss: 0.554860, acc.: 73.44%] [G loss: 3.332963]\n",
      "328 [D loss: 0.510689, acc.: 88.28%] [G loss: 3.408037]\n",
      "329 [D loss: 0.541849, acc.: 83.59%] [G loss: 2.842357]\n",
      "330 [D loss: 0.540585, acc.: 79.69%] [G loss: 2.877927]\n",
      "331 [D loss: 0.584060, acc.: 53.91%] [G loss: 2.850245]\n",
      "332 [D loss: 0.559791, acc.: 87.50%] [G loss: 3.037074]\n",
      "333 [D loss: 0.550161, acc.: 88.28%] [G loss: 2.776343]\n",
      "334 [D loss: 0.547411, acc.: 67.97%] [G loss: 2.978060]\n",
      "335 [D loss: 0.682346, acc.: 52.34%] [G loss: 2.229326]\n",
      "336 [D loss: 0.540566, acc.: 90.62%] [G loss: 2.807652]\n",
      "337 [D loss: 0.549754, acc.: 82.03%] [G loss: 2.197769]\n",
      "338 [D loss: 0.659240, acc.: 56.25%] [G loss: 3.375296]\n",
      "339 [D loss: 0.561942, acc.: 61.72%] [G loss: 3.143431]\n",
      "340 [D loss: 0.534106, acc.: 82.03%] [G loss: 2.674753]\n",
      "341 [D loss: 0.597385, acc.: 64.84%] [G loss: 3.179352]\n",
      "342 [D loss: 0.561251, acc.: 67.97%] [G loss: 2.867660]\n",
      "343 [D loss: 0.562942, acc.: 73.44%] [G loss: 2.982852]\n",
      "344 [D loss: 0.579961, acc.: 58.59%] [G loss: 3.138805]\n",
      "345 [D loss: 0.682811, acc.: 62.50%] [G loss: 3.440863]\n",
      "346 [D loss: 0.587594, acc.: 59.38%] [G loss: 3.591454]\n",
      "347 [D loss: 0.648337, acc.: 46.09%] [G loss: 2.855644]\n",
      "348 [D loss: 0.541702, acc.: 82.03%] [G loss: 3.548546]\n",
      "349 [D loss: 0.563189, acc.: 85.94%] [G loss: 2.916747]\n",
      "350 [D loss: 0.578638, acc.: 80.47%] [G loss: 2.717031]\n",
      "351 [D loss: 0.582848, acc.: 82.81%] [G loss: 2.388763]\n",
      "352 [D loss: 0.624300, acc.: 78.91%] [G loss: 3.191557]\n",
      "353 [D loss: 0.558492, acc.: 78.91%] [G loss: 2.836773]\n",
      "354 [D loss: 0.596618, acc.: 78.12%] [G loss: 3.092775]\n",
      "355 [D loss: 0.551350, acc.: 76.56%] [G loss: 2.212136]\n",
      "356 [D loss: 0.572538, acc.: 62.50%] [G loss: 2.490247]\n",
      "357 [D loss: 0.601567, acc.: 64.06%] [G loss: 2.553825]\n",
      "358 [D loss: 0.564733, acc.: 77.34%] [G loss: 2.818781]\n",
      "359 [D loss: 0.611401, acc.: 55.47%] [G loss: 2.489312]\n",
      "360 [D loss: 0.644116, acc.: 50.00%] [G loss: 2.396190]\n",
      "361 [D loss: 0.604110, acc.: 51.56%] [G loss: 2.671534]\n",
      "362 [D loss: 0.572087, acc.: 66.41%] [G loss: 2.958422]\n",
      "363 [D loss: 0.443786, acc.: 84.38%] [G loss: 4.037112]\n",
      "364 [D loss: 0.472292, acc.: 79.69%] [G loss: 2.929931]\n",
      "365 [D loss: 0.567390, acc.: 83.59%] [G loss: 2.316222]\n",
      "366 [D loss: 0.595823, acc.: 62.50%] [G loss: 2.297032]\n",
      "367 [D loss: 0.561555, acc.: 75.00%] [G loss: 2.660159]\n",
      "368 [D loss: 0.563049, acc.: 79.69%] [G loss: 3.283309]\n",
      "369 [D loss: 0.613045, acc.: 53.12%] [G loss: 2.633322]\n",
      "370 [D loss: 0.592356, acc.: 70.31%] [G loss: 2.572632]\n",
      "371 [D loss: 0.596952, acc.: 51.56%] [G loss: 2.733898]\n",
      "372 [D loss: 0.613697, acc.: 53.12%] [G loss: 2.818291]\n",
      "373 [D loss: 0.534122, acc.: 83.59%] [G loss: 3.680286]\n",
      "374 [D loss: 0.564768, acc.: 73.44%] [G loss: 3.066136]\n",
      "375 [D loss: 0.558386, acc.: 75.00%] [G loss: 3.090826]\n",
      "376 [D loss: 0.577953, acc.: 53.91%] [G loss: 2.623335]\n",
      "377 [D loss: 0.633457, acc.: 78.91%] [G loss: 2.926647]\n",
      "378 [D loss: 0.467884, acc.: 80.47%] [G loss: 3.384517]\n",
      "379 [D loss: 0.639165, acc.: 50.00%] [G loss: 2.453822]\n",
      "380 [D loss: 0.617828, acc.: 72.66%] [G loss: 2.838583]\n",
      "381 [D loss: 0.541934, acc.: 76.56%] [G loss: 3.097160]\n",
      "382 [D loss: 0.599329, acc.: 67.97%] [G loss: 2.468459]\n",
      "383 [D loss: 0.562573, acc.: 71.88%] [G loss: 2.682717]\n",
      "384 [D loss: 0.597703, acc.: 62.50%] [G loss: 3.107598]\n",
      "385 [D loss: 0.596928, acc.: 68.75%] [G loss: 2.844994]\n",
      "386 [D loss: 0.563057, acc.: 82.81%] [G loss: 3.119147]\n",
      "387 [D loss: 0.532982, acc.: 84.38%] [G loss: 2.404678]\n",
      "388 [D loss: 0.542608, acc.: 76.56%] [G loss: 2.863197]\n",
      "389 [D loss: 0.642828, acc.: 57.03%] [G loss: 2.575974]\n",
      "390 [D loss: 0.598137, acc.: 62.50%] [G loss: 2.450817]\n",
      "391 [D loss: 0.611848, acc.: 67.19%] [G loss: 2.734090]\n",
      "392 [D loss: 0.582634, acc.: 62.50%] [G loss: 2.064772]\n",
      "393 [D loss: 0.619403, acc.: 60.94%] [G loss: 2.798433]\n",
      "394 [D loss: 0.597063, acc.: 78.91%] [G loss: 2.570537]\n",
      "395 [D loss: 0.499184, acc.: 82.81%] [G loss: 3.097380]\n",
      "396 [D loss: 0.585372, acc.: 76.56%] [G loss: 2.598325]\n",
      "397 [D loss: 0.575782, acc.: 77.34%] [G loss: 3.426466]\n",
      "398 [D loss: 0.555978, acc.: 77.34%] [G loss: 2.579668]\n",
      "399 [D loss: 0.597615, acc.: 75.78%] [G loss: 2.713391]\n",
      "400 [D loss: 0.568188, acc.: 68.75%] [G loss: 2.375973]\n",
      "401 [D loss: 0.567106, acc.: 78.91%] [G loss: 2.683551]\n",
      "402 [D loss: 0.578311, acc.: 76.56%] [G loss: 2.711973]\n",
      "403 [D loss: 0.551699, acc.: 78.91%] [G loss: 2.692353]\n",
      "404 [D loss: 0.579873, acc.: 73.44%] [G loss: 3.062162]\n",
      "405 [D loss: 0.606636, acc.: 75.00%] [G loss: 2.574686]\n",
      "406 [D loss: 0.590871, acc.: 72.66%] [G loss: 3.309183]\n",
      "407 [D loss: 0.694200, acc.: 78.12%] [G loss: 3.184365]\n",
      "408 [D loss: 0.589414, acc.: 73.44%] [G loss: 3.476238]\n",
      "409 [D loss: 0.593994, acc.: 73.44%] [G loss: 3.234957]\n",
      "410 [D loss: 0.661002, acc.: 59.38%] [G loss: 3.166913]\n",
      "411 [D loss: 0.616639, acc.: 53.12%] [G loss: 3.231058]\n",
      "412 [D loss: 0.515261, acc.: 79.69%] [G loss: 2.716946]\n",
      "413 [D loss: 0.573065, acc.: 71.88%] [G loss: 2.906392]\n",
      "414 [D loss: 0.619930, acc.: 60.94%] [G loss: 2.906742]\n",
      "415 [D loss: 0.585283, acc.: 67.97%] [G loss: 2.420178]\n",
      "416 [D loss: 0.545738, acc.: 83.59%] [G loss: 2.857192]\n",
      "417 [D loss: 0.524479, acc.: 83.59%] [G loss: 2.335741]\n",
      "418 [D loss: 0.545081, acc.: 81.25%] [G loss: 2.318423]\n",
      "419 [D loss: 0.643027, acc.: 58.59%] [G loss: 2.753570]\n",
      "420 [D loss: 0.649167, acc.: 74.22%] [G loss: 2.512444]\n",
      "421 [D loss: 0.603080, acc.: 58.59%] [G loss: 2.865922]\n",
      "422 [D loss: 0.623454, acc.: 67.97%] [G loss: 2.429626]\n",
      "423 [D loss: 0.604261, acc.: 72.66%] [G loss: 2.758655]\n",
      "424 [D loss: 0.518770, acc.: 82.81%] [G loss: 3.112203]\n",
      "425 [D loss: 0.647674, acc.: 68.75%] [G loss: 2.734861]\n",
      "426 [D loss: 0.575228, acc.: 74.22%] [G loss: 2.835166]\n",
      "427 [D loss: 0.658523, acc.: 67.19%] [G loss: 2.886228]\n",
      "428 [D loss: 0.600760, acc.: 64.84%] [G loss: 2.978582]\n",
      "429 [D loss: 0.603592, acc.: 69.53%] [G loss: 2.851483]\n",
      "430 [D loss: 0.595604, acc.: 71.88%] [G loss: 2.913794]\n",
      "431 [D loss: 0.601770, acc.: 64.84%] [G loss: 2.656477]\n",
      "432 [D loss: 0.620308, acc.: 67.97%] [G loss: 2.914483]\n",
      "433 [D loss: 0.562147, acc.: 78.91%] [G loss: 2.692282]\n",
      "434 [D loss: 0.547658, acc.: 71.88%] [G loss: 2.796075]\n",
      "435 [D loss: 0.576569, acc.: 75.78%] [G loss: 2.211360]\n",
      "436 [D loss: 0.586232, acc.: 74.22%] [G loss: 2.595703]\n",
      "437 [D loss: 0.557753, acc.: 82.03%] [G loss: 2.670932]\n",
      "438 [D loss: 0.512938, acc.: 78.91%] [G loss: 2.448699]\n",
      "439 [D loss: 0.597367, acc.: 67.97%] [G loss: 2.780799]\n",
      "440 [D loss: 0.557563, acc.: 82.81%] [G loss: 2.930892]\n",
      "441 [D loss: 0.610332, acc.: 72.66%] [G loss: 2.562026]\n",
      "442 [D loss: 0.600619, acc.: 72.66%] [G loss: 2.539191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 0.566173, acc.: 72.66%] [G loss: 3.056012]\n",
      "444 [D loss: 0.562867, acc.: 73.44%] [G loss: 2.403865]\n",
      "445 [D loss: 0.560103, acc.: 71.09%] [G loss: 3.073780]\n",
      "446 [D loss: 0.660042, acc.: 48.44%] [G loss: 2.885254]\n",
      "447 [D loss: 0.554357, acc.: 72.66%] [G loss: 2.571122]\n",
      "448 [D loss: 0.538771, acc.: 79.69%] [G loss: 2.186873]\n",
      "449 [D loss: 0.560763, acc.: 71.88%] [G loss: 2.494104]\n",
      "450 [D loss: 0.587617, acc.: 61.72%] [G loss: 2.660037]\n",
      "451 [D loss: 0.528356, acc.: 85.16%] [G loss: 2.716027]\n",
      "452 [D loss: 0.598535, acc.: 67.97%] [G loss: 2.989748]\n",
      "453 [D loss: 0.553343, acc.: 69.53%] [G loss: 2.795500]\n",
      "454 [D loss: 0.522192, acc.: 80.47%] [G loss: 2.903798]\n",
      "455 [D loss: 0.636518, acc.: 75.78%] [G loss: 2.859994]\n",
      "456 [D loss: 0.608851, acc.: 64.06%] [G loss: 3.566692]\n",
      "457 [D loss: 0.621771, acc.: 67.19%] [G loss: 3.578422]\n",
      "458 [D loss: 0.634117, acc.: 49.22%] [G loss: 2.659484]\n",
      "459 [D loss: 0.646504, acc.: 63.28%] [G loss: 2.675527]\n",
      "460 [D loss: 0.549695, acc.: 73.44%] [G loss: 2.426478]\n",
      "461 [D loss: 0.556899, acc.: 76.56%] [G loss: 2.522044]\n",
      "462 [D loss: 0.641990, acc.: 64.06%] [G loss: 2.367043]\n",
      "463 [D loss: 0.590685, acc.: 63.28%] [G loss: 2.105431]\n",
      "464 [D loss: 0.635683, acc.: 57.81%] [G loss: 2.703957]\n",
      "465 [D loss: 0.586950, acc.: 69.53%] [G loss: 2.339508]\n",
      "466 [D loss: 0.536393, acc.: 78.91%] [G loss: 2.366126]\n",
      "467 [D loss: 0.561315, acc.: 74.22%] [G loss: 2.777871]\n",
      "468 [D loss: 0.745945, acc.: 54.69%] [G loss: 2.757383]\n",
      "469 [D loss: 0.555105, acc.: 78.12%] [G loss: 2.899896]\n",
      "470 [D loss: 0.666684, acc.: 71.88%] [G loss: 2.673195]\n",
      "471 [D loss: 0.561972, acc.: 75.78%] [G loss: 3.117337]\n",
      "472 [D loss: 0.549548, acc.: 78.91%] [G loss: 3.022303]\n",
      "473 [D loss: 0.621352, acc.: 64.84%] [G loss: 2.745173]\n",
      "474 [D loss: 0.712618, acc.: 49.22%] [G loss: 2.640367]\n",
      "475 [D loss: 0.747520, acc.: 59.38%] [G loss: 2.761730]\n",
      "476 [D loss: 0.694682, acc.: 46.09%] [G loss: 2.018095]\n",
      "477 [D loss: 0.777278, acc.: 41.41%] [G loss: 2.033831]\n",
      "478 [D loss: 0.592941, acc.: 74.22%] [G loss: 3.036268]\n",
      "479 [D loss: 0.680201, acc.: 60.94%] [G loss: 2.461038]\n",
      "480 [D loss: 0.714751, acc.: 50.78%] [G loss: 2.781271]\n",
      "481 [D loss: 0.618080, acc.: 67.97%] [G loss: 2.581734]\n",
      "482 [D loss: 0.613963, acc.: 57.81%] [G loss: 2.219897]\n",
      "483 [D loss: 0.697482, acc.: 47.66%] [G loss: 2.580414]\n",
      "484 [D loss: 0.618567, acc.: 74.22%] [G loss: 2.441518]\n",
      "485 [D loss: 0.554087, acc.: 78.91%] [G loss: 2.731250]\n",
      "486 [D loss: 0.555492, acc.: 78.12%] [G loss: 2.775035]\n",
      "487 [D loss: 0.712022, acc.: 47.66%] [G loss: 2.331903]\n",
      "488 [D loss: 0.665242, acc.: 59.38%] [G loss: 2.370960]\n",
      "489 [D loss: 0.653868, acc.: 52.34%] [G loss: 2.765634]\n",
      "490 [D loss: 0.580171, acc.: 74.22%] [G loss: 2.982934]\n",
      "491 [D loss: 0.621521, acc.: 65.62%] [G loss: 1.895086]\n",
      "492 [D loss: 0.639012, acc.: 54.69%] [G loss: 3.182406]\n",
      "493 [D loss: 0.610677, acc.: 55.47%] [G loss: 3.136018]\n",
      "494 [D loss: 0.585555, acc.: 71.88%] [G loss: 2.300437]\n",
      "495 [D loss: 0.555557, acc.: 82.81%] [G loss: 2.845512]\n",
      "496 [D loss: 0.612605, acc.: 63.28%] [G loss: 2.326486]\n",
      "497 [D loss: 0.579159, acc.: 74.22%] [G loss: 2.352138]\n",
      "498 [D loss: 0.631301, acc.: 61.72%] [G loss: 1.896334]\n",
      "499 [D loss: 0.628662, acc.: 54.69%] [G loss: 2.582211]\n",
      "500 [D loss: 0.623708, acc.: 59.38%] [G loss: 2.205587]\n",
      "501 [D loss: 0.656442, acc.: 57.03%] [G loss: 2.605414]\n",
      "502 [D loss: 0.609241, acc.: 66.41%] [G loss: 2.313004]\n",
      "503 [D loss: 0.585571, acc.: 66.41%] [G loss: 2.113603]\n",
      "504 [D loss: 0.640711, acc.: 50.78%] [G loss: 1.843011]\n",
      "505 [D loss: 0.717198, acc.: 44.53%] [G loss: 2.554169]\n",
      "506 [D loss: 0.629314, acc.: 57.03%] [G loss: 2.225602]\n",
      "507 [D loss: 0.587624, acc.: 74.22%] [G loss: 2.768812]\n",
      "508 [D loss: 0.580097, acc.: 76.56%] [G loss: 2.905703]\n",
      "509 [D loss: 0.616917, acc.: 60.94%] [G loss: 1.908178]\n",
      "510 [D loss: 0.568963, acc.: 72.66%] [G loss: 2.285650]\n",
      "511 [D loss: 0.624173, acc.: 67.19%] [G loss: 2.414751]\n",
      "512 [D loss: 0.581270, acc.: 71.88%] [G loss: 2.467382]\n",
      "513 [D loss: 0.594722, acc.: 61.72%] [G loss: 2.117464]\n",
      "514 [D loss: 0.634589, acc.: 64.06%] [G loss: 2.379303]\n",
      "515 [D loss: 0.540948, acc.: 78.12%] [G loss: 2.015024]\n",
      "516 [D loss: 0.616927, acc.: 71.88%] [G loss: 2.344059]\n",
      "517 [D loss: 0.608195, acc.: 61.72%] [G loss: 2.628724]\n",
      "518 [D loss: 0.762238, acc.: 39.84%] [G loss: 2.266528]\n",
      "519 [D loss: 0.608445, acc.: 75.00%] [G loss: 2.272433]\n",
      "520 [D loss: 0.580176, acc.: 75.78%] [G loss: 2.451279]\n",
      "521 [D loss: 0.683026, acc.: 48.44%] [G loss: 2.386258]\n",
      "522 [D loss: 0.598678, acc.: 71.88%] [G loss: 3.022622]\n",
      "523 [D loss: 0.548864, acc.: 78.12%] [G loss: 2.474541]\n",
      "524 [D loss: 0.573081, acc.: 80.47%] [G loss: 2.195940]\n",
      "525 [D loss: 0.578015, acc.: 75.78%] [G loss: 2.588805]\n",
      "526 [D loss: 0.589206, acc.: 76.56%] [G loss: 2.267456]\n",
      "527 [D loss: 0.630933, acc.: 70.31%] [G loss: 2.408593]\n",
      "528 [D loss: 0.696543, acc.: 60.16%] [G loss: 2.170715]\n",
      "529 [D loss: 0.610989, acc.: 54.69%] [G loss: 2.919704]\n",
      "530 [D loss: 0.619060, acc.: 62.50%] [G loss: 2.369257]\n",
      "531 [D loss: 0.688358, acc.: 57.03%] [G loss: 2.279379]\n",
      "532 [D loss: 0.554736, acc.: 82.03%] [G loss: 2.897511]\n",
      "533 [D loss: 0.584490, acc.: 71.88%] [G loss: 2.866495]\n",
      "534 [D loss: 0.641293, acc.: 57.03%] [G loss: 2.348400]\n",
      "535 [D loss: 0.625964, acc.: 66.41%] [G loss: 2.053780]\n",
      "536 [D loss: 0.609115, acc.: 68.75%] [G loss: 2.312897]\n",
      "537 [D loss: 0.562392, acc.: 82.03%] [G loss: 2.541199]\n",
      "538 [D loss: 0.561266, acc.: 74.22%] [G loss: 2.267439]\n",
      "539 [D loss: 0.676757, acc.: 50.78%] [G loss: 2.162370]\n",
      "540 [D loss: 0.598368, acc.: 67.97%] [G loss: 2.354044]\n",
      "541 [D loss: 0.557680, acc.: 81.25%] [G loss: 2.192989]\n",
      "542 [D loss: 0.620275, acc.: 62.50%] [G loss: 2.363468]\n",
      "543 [D loss: 0.622433, acc.: 72.66%] [G loss: 2.046507]\n",
      "544 [D loss: 0.549552, acc.: 85.94%] [G loss: 2.287098]\n",
      "545 [D loss: 0.577136, acc.: 82.03%] [G loss: 2.485868]\n",
      "546 [D loss: 0.625222, acc.: 62.50%] [G loss: 2.020799]\n",
      "547 [D loss: 0.724723, acc.: 60.94%] [G loss: 3.380609]\n",
      "548 [D loss: 0.528344, acc.: 82.03%] [G loss: 2.841198]\n",
      "549 [D loss: 0.528841, acc.: 79.69%] [G loss: 2.909153]\n",
      "550 [D loss: 0.575920, acc.: 79.69%] [G loss: 2.084128]\n",
      "551 [D loss: 0.639024, acc.: 60.16%] [G loss: 2.366769]\n",
      "552 [D loss: 0.641032, acc.: 50.78%] [G loss: 2.475085]\n",
      "553 [D loss: 0.595588, acc.: 53.12%] [G loss: 2.128280]\n",
      "554 [D loss: 0.609582, acc.: 58.59%] [G loss: 1.996857]\n",
      "555 [D loss: 0.591035, acc.: 59.38%] [G loss: 1.683699]\n",
      "556 [D loss: 0.632881, acc.: 57.03%] [G loss: 2.149929]\n",
      "557 [D loss: 0.510380, acc.: 89.84%] [G loss: 2.514289]\n",
      "558 [D loss: 0.482225, acc.: 89.06%] [G loss: 1.824400]\n",
      "559 [D loss: 0.640282, acc.: 62.50%] [G loss: 2.629288]\n",
      "560 [D loss: 0.656851, acc.: 52.34%] [G loss: 1.931399]\n",
      "561 [D loss: 0.589432, acc.: 74.22%] [G loss: 1.986008]\n",
      "562 [D loss: 0.690468, acc.: 41.41%] [G loss: 1.904109]\n",
      "563 [D loss: 0.613068, acc.: 71.88%] [G loss: 2.497872]\n",
      "564 [D loss: 0.575390, acc.: 74.22%] [G loss: 2.923722]\n",
      "565 [D loss: 0.611994, acc.: 57.81%] [G loss: 1.603399]\n",
      "566 [D loss: 0.577079, acc.: 81.25%] [G loss: 2.674389]\n",
      "567 [D loss: 0.584741, acc.: 68.75%] [G loss: 1.838877]\n",
      "568 [D loss: 0.634002, acc.: 54.69%] [G loss: 2.312187]\n",
      "569 [D loss: 0.661986, acc.: 52.34%] [G loss: 2.442732]\n",
      "570 [D loss: 0.648242, acc.: 51.56%] [G loss: 1.504091]\n",
      "571 [D loss: 0.695845, acc.: 46.88%] [G loss: 2.051258]\n",
      "572 [D loss: 0.651798, acc.: 52.34%] [G loss: 2.147974]\n",
      "573 [D loss: 0.640514, acc.: 57.81%] [G loss: 2.333982]\n",
      "574 [D loss: 0.647139, acc.: 61.72%] [G loss: 1.792429]\n",
      "575 [D loss: 0.530089, acc.: 83.59%] [G loss: 1.833789]\n",
      "576 [D loss: 0.671702, acc.: 59.38%] [G loss: 1.656675]\n",
      "577 [D loss: 0.650444, acc.: 50.78%] [G loss: 1.692963]\n",
      "578 [D loss: 0.647290, acc.: 59.38%] [G loss: 2.073908]\n",
      "579 [D loss: 0.649661, acc.: 52.34%] [G loss: 1.841419]\n",
      "580 [D loss: 0.691731, acc.: 41.41%] [G loss: 1.562561]\n",
      "581 [D loss: 0.670536, acc.: 46.88%] [G loss: 2.108421]\n",
      "582 [D loss: 0.635772, acc.: 62.50%] [G loss: 1.878807]\n",
      "583 [D loss: 0.604209, acc.: 71.09%] [G loss: 1.844418]\n",
      "584 [D loss: 0.567997, acc.: 78.12%] [G loss: 1.880286]\n",
      "585 [D loss: 0.760176, acc.: 71.88%] [G loss: 2.154336]\n",
      "586 [D loss: 0.584478, acc.: 58.59%] [G loss: 1.667360]\n",
      "587 [D loss: 0.603501, acc.: 63.28%] [G loss: 2.697106]\n",
      "588 [D loss: 0.643291, acc.: 53.12%] [G loss: 1.693391]\n",
      "589 [D loss: 0.789017, acc.: 67.19%] [G loss: 1.989220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.543946, acc.: 80.47%] [G loss: 2.451404]\n",
      "591 [D loss: 0.629865, acc.: 63.28%] [G loss: 2.721171]\n",
      "592 [D loss: 0.579586, acc.: 71.09%] [G loss: 1.843076]\n",
      "593 [D loss: 0.655693, acc.: 78.12%] [G loss: 1.905042]\n",
      "594 [D loss: 0.567195, acc.: 82.03%] [G loss: 1.960489]\n",
      "595 [D loss: 0.566319, acc.: 86.72%] [G loss: 1.865016]\n",
      "596 [D loss: 0.572798, acc.: 80.47%] [G loss: 2.063351]\n",
      "597 [D loss: 0.609754, acc.: 60.94%] [G loss: 1.793812]\n",
      "598 [D loss: 0.621961, acc.: 57.03%] [G loss: 1.630969]\n",
      "599 [D loss: 0.577197, acc.: 77.34%] [G loss: 2.581708]\n",
      "600 [D loss: 0.576010, acc.: 72.66%] [G loss: 1.766149]\n",
      "601 [D loss: 0.621947, acc.: 57.81%] [G loss: 1.787879]\n",
      "602 [D loss: 0.634247, acc.: 53.91%] [G loss: 2.040864]\n",
      "603 [D loss: 0.638455, acc.: 52.34%] [G loss: 1.955602]\n",
      "604 [D loss: 0.591274, acc.: 68.75%] [G loss: 2.291584]\n",
      "605 [D loss: 0.596488, acc.: 81.25%] [G loss: 1.634437]\n",
      "606 [D loss: 0.683956, acc.: 48.44%] [G loss: 2.152349]\n",
      "607 [D loss: 0.568567, acc.: 78.12%] [G loss: 2.216944]\n",
      "608 [D loss: 0.573444, acc.: 75.78%] [G loss: 2.205260]\n",
      "609 [D loss: 0.549003, acc.: 81.25%] [G loss: 2.597976]\n",
      "610 [D loss: 0.579350, acc.: 75.00%] [G loss: 2.011292]\n",
      "611 [D loss: 0.569379, acc.: 75.78%] [G loss: 2.047385]\n",
      "612 [D loss: 0.651965, acc.: 48.44%] [G loss: 1.777882]\n",
      "613 [D loss: 0.564162, acc.: 66.41%] [G loss: 2.167180]\n",
      "614 [D loss: 0.587124, acc.: 73.44%] [G loss: 1.984485]\n",
      "615 [D loss: 0.565238, acc.: 80.47%] [G loss: 2.147093]\n",
      "616 [D loss: 0.603520, acc.: 56.25%] [G loss: 1.939732]\n",
      "617 [D loss: 0.610645, acc.: 64.84%] [G loss: 1.623652]\n",
      "618 [D loss: 0.675753, acc.: 46.09%] [G loss: 1.692067]\n",
      "619 [D loss: 0.622659, acc.: 56.25%] [G loss: 2.553749]\n",
      "620 [D loss: 0.518442, acc.: 87.50%] [G loss: 2.083717]\n",
      "621 [D loss: 0.491097, acc.: 85.16%] [G loss: 2.065339]\n",
      "622 [D loss: 0.571859, acc.: 89.06%] [G loss: 2.373407]\n",
      "623 [D loss: 0.542878, acc.: 75.78%] [G loss: 1.830589]\n",
      "624 [D loss: 0.578456, acc.: 78.12%] [G loss: 1.764126]\n",
      "625 [D loss: 0.640570, acc.: 74.22%] [G loss: 1.945155]\n",
      "626 [D loss: 0.576603, acc.: 70.31%] [G loss: 1.775807]\n",
      "627 [D loss: 0.602249, acc.: 64.06%] [G loss: 2.132941]\n",
      "628 [D loss: 0.592428, acc.: 76.56%] [G loss: 2.291811]\n",
      "629 [D loss: 0.555288, acc.: 69.53%] [G loss: 2.070928]\n",
      "630 [D loss: 0.581613, acc.: 68.75%] [G loss: 1.816480]\n",
      "631 [D loss: 0.610629, acc.: 59.38%] [G loss: 1.452674]\n",
      "632 [D loss: 0.577653, acc.: 72.66%] [G loss: 1.430683]\n",
      "633 [D loss: 0.598538, acc.: 70.31%] [G loss: 1.519528]\n",
      "634 [D loss: 0.574664, acc.: 67.19%] [G loss: 2.067792]\n",
      "635 [D loss: 0.600618, acc.: 65.62%] [G loss: 2.353164]\n",
      "636 [D loss: 0.687705, acc.: 50.00%] [G loss: 2.019339]\n",
      "637 [D loss: 0.524554, acc.: 85.94%] [G loss: 2.750004]\n",
      "638 [D loss: 0.467019, acc.: 82.81%] [G loss: 2.394444]\n",
      "639 [D loss: 0.524561, acc.: 77.34%] [G loss: 2.699442]\n",
      "640 [D loss: 0.655655, acc.: 65.62%] [G loss: 2.117266]\n",
      "641 [D loss: 0.557108, acc.: 76.56%] [G loss: 2.535167]\n",
      "642 [D loss: 0.507859, acc.: 83.59%] [G loss: 2.698057]\n",
      "643 [D loss: 0.523467, acc.: 79.69%] [G loss: 2.337072]\n",
      "644 [D loss: 0.584829, acc.: 68.75%] [G loss: 2.464260]\n",
      "645 [D loss: 0.610888, acc.: 57.03%] [G loss: 1.966191]\n",
      "646 [D loss: 0.632679, acc.: 58.59%] [G loss: 2.146144]\n",
      "647 [D loss: 0.591069, acc.: 75.78%] [G loss: 3.046985]\n",
      "648 [D loss: 0.628000, acc.: 70.31%] [G loss: 2.778536]\n",
      "649 [D loss: 0.631887, acc.: 64.06%] [G loss: 2.529449]\n",
      "650 [D loss: 0.593960, acc.: 72.66%] [G loss: 2.514340]\n",
      "651 [D loss: 0.659223, acc.: 72.66%] [G loss: 2.654763]\n",
      "652 [D loss: 0.678492, acc.: 53.91%] [G loss: 2.240634]\n",
      "653 [D loss: 0.617456, acc.: 72.66%] [G loss: 2.271605]\n",
      "654 [D loss: 0.606263, acc.: 64.06%] [G loss: 2.298676]\n",
      "655 [D loss: 0.644802, acc.: 66.41%] [G loss: 2.277692]\n",
      "656 [D loss: 0.646128, acc.: 71.09%] [G loss: 1.924734]\n",
      "657 [D loss: 0.615117, acc.: 67.19%] [G loss: 1.981174]\n",
      "658 [D loss: 0.610919, acc.: 67.97%] [G loss: 1.816017]\n",
      "659 [D loss: 0.620765, acc.: 68.75%] [G loss: 1.916510]\n",
      "660 [D loss: 0.669023, acc.: 54.69%] [G loss: 1.660302]\n",
      "661 [D loss: 0.610870, acc.: 71.09%] [G loss: 1.748829]\n",
      "662 [D loss: 0.618387, acc.: 67.19%] [G loss: 2.069516]\n",
      "663 [D loss: 0.577253, acc.: 77.34%] [G loss: 1.807459]\n",
      "664 [D loss: 0.564745, acc.: 80.47%] [G loss: 1.753917]\n",
      "665 [D loss: 0.573176, acc.: 78.12%] [G loss: 2.552691]\n",
      "666 [D loss: 0.577192, acc.: 73.44%] [G loss: 1.752899]\n",
      "667 [D loss: 0.594943, acc.: 67.19%] [G loss: 2.474787]\n",
      "668 [D loss: 0.552771, acc.: 76.56%] [G loss: 1.610598]\n",
      "669 [D loss: 0.547865, acc.: 83.59%] [G loss: 2.320579]\n",
      "670 [D loss: 0.605682, acc.: 72.66%] [G loss: 1.692955]\n",
      "671 [D loss: 0.650284, acc.: 70.31%] [G loss: 2.452558]\n",
      "672 [D loss: 0.566289, acc.: 82.03%] [G loss: 2.016410]\n",
      "673 [D loss: 0.584063, acc.: 76.56%] [G loss: 2.443413]\n",
      "674 [D loss: 0.642960, acc.: 57.03%] [G loss: 2.001541]\n",
      "675 [D loss: 0.614164, acc.: 65.62%] [G loss: 1.447154]\n",
      "676 [D loss: 0.617347, acc.: 66.41%] [G loss: 1.674313]\n",
      "677 [D loss: 0.623704, acc.: 68.75%] [G loss: 1.792559]\n",
      "678 [D loss: 0.614681, acc.: 53.12%] [G loss: 1.472972]\n",
      "679 [D loss: 0.835594, acc.: 46.09%] [G loss: 1.795743]\n",
      "680 [D loss: 0.577985, acc.: 70.31%] [G loss: 2.078663]\n",
      "681 [D loss: 0.608587, acc.: 67.97%] [G loss: 1.628154]\n",
      "682 [D loss: 0.673921, acc.: 51.56%] [G loss: 1.946194]\n",
      "683 [D loss: 0.647849, acc.: 63.28%] [G loss: 1.528365]\n",
      "684 [D loss: 0.676471, acc.: 44.53%] [G loss: 1.757995]\n",
      "685 [D loss: 0.898023, acc.: 42.19%] [G loss: 2.034418]\n",
      "686 [D loss: 0.615760, acc.: 63.28%] [G loss: 1.914228]\n",
      "687 [D loss: 0.565166, acc.: 76.56%] [G loss: 1.762372]\n",
      "688 [D loss: 0.588427, acc.: 73.44%] [G loss: 1.953063]\n",
      "689 [D loss: 0.696214, acc.: 44.53%] [G loss: 1.364672]\n",
      "690 [D loss: 0.692852, acc.: 42.97%] [G loss: 1.350488]\n",
      "691 [D loss: 0.675234, acc.: 55.47%] [G loss: 1.524164]\n",
      "692 [D loss: 0.697637, acc.: 39.84%] [G loss: 1.501704]\n",
      "693 [D loss: 0.699222, acc.: 39.06%] [G loss: 1.359948]\n",
      "694 [D loss: 0.662626, acc.: 53.12%] [G loss: 1.652978]\n",
      "695 [D loss: 0.586215, acc.: 75.78%] [G loss: 2.124380]\n",
      "696 [D loss: 0.545673, acc.: 76.56%] [G loss: 2.640020]\n",
      "697 [D loss: 0.593759, acc.: 75.00%] [G loss: 1.777781]\n",
      "698 [D loss: 0.624791, acc.: 67.19%] [G loss: 1.771094]\n",
      "699 [D loss: 0.642670, acc.: 62.50%] [G loss: 1.607938]\n",
      "700 [D loss: 0.615799, acc.: 64.06%] [G loss: 1.493422]\n",
      "701 [D loss: 0.778170, acc.: 64.84%] [G loss: 1.596931]\n",
      "702 [D loss: 0.592969, acc.: 72.66%] [G loss: 1.907228]\n",
      "703 [D loss: 0.611349, acc.: 71.88%] [G loss: 1.909137]\n",
      "704 [D loss: 0.566526, acc.: 78.12%] [G loss: 1.963686]\n",
      "705 [D loss: 0.650398, acc.: 50.00%] [G loss: 2.263925]\n",
      "706 [D loss: 0.594975, acc.: 80.47%] [G loss: 1.454071]\n",
      "707 [D loss: 0.579520, acc.: 81.25%] [G loss: 1.804868]\n",
      "708 [D loss: 0.586653, acc.: 74.22%] [G loss: 1.208207]\n",
      "709 [D loss: 0.621435, acc.: 63.28%] [G loss: 1.866937]\n",
      "710 [D loss: 0.653067, acc.: 48.44%] [G loss: 2.023931]\n",
      "711 [D loss: 0.624923, acc.: 66.41%] [G loss: 1.732084]\n",
      "712 [D loss: 0.769950, acc.: 80.47%] [G loss: 1.890206]\n",
      "713 [D loss: 0.526055, acc.: 82.81%] [G loss: 2.531933]\n",
      "714 [D loss: 0.555232, acc.: 85.16%] [G loss: 2.175391]\n",
      "715 [D loss: 0.561271, acc.: 83.59%] [G loss: 1.348143]\n",
      "716 [D loss: 0.628331, acc.: 53.12%] [G loss: 1.567471]\n",
      "717 [D loss: 0.656482, acc.: 52.34%] [G loss: 1.745435]\n",
      "718 [D loss: 0.640795, acc.: 53.91%] [G loss: 1.568611]\n",
      "719 [D loss: 0.564028, acc.: 85.16%] [G loss: 1.370618]\n",
      "720 [D loss: 0.605427, acc.: 79.69%] [G loss: 1.959367]\n",
      "721 [D loss: 0.528399, acc.: 87.50%] [G loss: 2.008921]\n",
      "722 [D loss: 0.512048, acc.: 89.06%] [G loss: 2.184471]\n",
      "723 [D loss: 0.578068, acc.: 80.47%] [G loss: 1.644987]\n",
      "724 [D loss: 0.568985, acc.: 84.38%] [G loss: 1.976724]\n",
      "725 [D loss: 0.598302, acc.: 69.53%] [G loss: 1.451231]\n",
      "726 [D loss: 0.530887, acc.: 85.94%] [G loss: 1.731566]\n",
      "727 [D loss: 0.547935, acc.: 83.59%] [G loss: 1.393191]\n",
      "728 [D loss: 0.596193, acc.: 73.44%] [G loss: 1.405089]\n",
      "729 [D loss: 0.587368, acc.: 78.12%] [G loss: 1.889717]\n",
      "730 [D loss: 0.581902, acc.: 75.00%] [G loss: 1.470660]\n",
      "731 [D loss: 0.609071, acc.: 67.19%] [G loss: 1.463179]\n",
      "732 [D loss: 0.524024, acc.: 85.16%] [G loss: 1.641279]\n",
      "733 [D loss: 0.537228, acc.: 82.03%] [G loss: 2.145744]\n",
      "734 [D loss: 0.541866, acc.: 85.16%] [G loss: 2.185592]\n",
      "735 [D loss: 0.525372, acc.: 83.59%] [G loss: 1.877897]\n",
      "736 [D loss: 0.512164, acc.: 86.72%] [G loss: 2.247244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 [D loss: 0.552804, acc.: 78.12%] [G loss: 1.798980]\n",
      "738 [D loss: 0.561622, acc.: 86.72%] [G loss: 1.493197]\n",
      "739 [D loss: 0.507454, acc.: 85.16%] [G loss: 1.606327]\n",
      "740 [D loss: 0.513519, acc.: 87.50%] [G loss: 2.084687]\n",
      "741 [D loss: 0.610825, acc.: 73.44%] [G loss: 1.465688]\n",
      "742 [D loss: 0.542819, acc.: 86.72%] [G loss: 1.467928]\n",
      "743 [D loss: 0.576616, acc.: 85.16%] [G loss: 1.962228]\n",
      "744 [D loss: 0.512198, acc.: 89.84%] [G loss: 1.970083]\n",
      "745 [D loss: 0.551693, acc.: 79.69%] [G loss: 2.023402]\n",
      "746 [D loss: 0.514349, acc.: 87.50%] [G loss: 1.584431]\n",
      "747 [D loss: 0.570279, acc.: 71.09%] [G loss: 1.636143]\n",
      "748 [D loss: 0.604683, acc.: 60.16%] [G loss: 1.808338]\n",
      "749 [D loss: 0.545712, acc.: 75.78%] [G loss: 1.763078]\n",
      "750 [D loss: 0.565366, acc.: 73.44%] [G loss: 1.879912]\n",
      "751 [D loss: 0.645987, acc.: 53.91%] [G loss: 1.618202]\n",
      "752 [D loss: 0.557818, acc.: 78.12%] [G loss: 1.979944]\n",
      "753 [D loss: 0.542235, acc.: 79.69%] [G loss: 1.982042]\n",
      "754 [D loss: 0.583265, acc.: 68.75%] [G loss: 1.925290]\n",
      "755 [D loss: 0.743642, acc.: 42.97%] [G loss: 1.968366]\n",
      "756 [D loss: 0.585618, acc.: 60.94%] [G loss: 1.885185]\n",
      "757 [D loss: 0.591804, acc.: 70.31%] [G loss: 1.876040]\n",
      "758 [D loss: 0.639715, acc.: 57.81%] [G loss: 1.899107]\n",
      "759 [D loss: 0.570966, acc.: 69.53%] [G loss: 1.852232]\n",
      "760 [D loss: 0.560197, acc.: 77.34%] [G loss: 2.113264]\n",
      "761 [D loss: 0.592621, acc.: 67.97%] [G loss: 1.670212]\n",
      "762 [D loss: 0.681333, acc.: 68.75%] [G loss: 1.799700]\n",
      "763 [D loss: 0.545000, acc.: 83.59%] [G loss: 1.858990]\n",
      "764 [D loss: 0.623594, acc.: 67.19%] [G loss: 2.014205]\n",
      "765 [D loss: 0.562144, acc.: 75.78%] [G loss: 2.438179]\n",
      "766 [D loss: 0.585994, acc.: 78.12%] [G loss: 1.974300]\n",
      "767 [D loss: 0.668128, acc.: 67.97%] [G loss: 2.365435]\n",
      "768 [D loss: 0.621685, acc.: 70.31%] [G loss: 1.708087]\n",
      "769 [D loss: 0.737733, acc.: 56.25%] [G loss: 2.025366]\n",
      "770 [D loss: 0.690641, acc.: 43.75%] [G loss: 1.760391]\n",
      "771 [D loss: 0.779178, acc.: 39.84%] [G loss: 1.603070]\n",
      "772 [D loss: 0.715903, acc.: 37.50%] [G loss: 2.306550]\n",
      "773 [D loss: 0.596986, acc.: 69.53%] [G loss: 2.315723]\n",
      "774 [D loss: 0.639036, acc.: 68.75%] [G loss: 1.855912]\n",
      "775 [D loss: 0.585944, acc.: 80.47%] [G loss: 2.033198]\n",
      "776 [D loss: 0.597693, acc.: 78.12%] [G loss: 2.096601]\n",
      "777 [D loss: 0.614355, acc.: 68.75%] [G loss: 1.592671]\n",
      "778 [D loss: 0.617778, acc.: 73.44%] [G loss: 2.048817]\n",
      "779 [D loss: 0.631197, acc.: 67.97%] [G loss: 1.933523]\n",
      "780 [D loss: 0.644334, acc.: 67.19%] [G loss: 2.132885]\n",
      "781 [D loss: 0.619343, acc.: 70.31%] [G loss: 1.664469]\n",
      "782 [D loss: 0.623672, acc.: 65.62%] [G loss: 1.423196]\n",
      "783 [D loss: 0.642421, acc.: 72.66%] [G loss: 1.647716]\n",
      "784 [D loss: 0.600635, acc.: 67.97%] [G loss: 1.714230]\n",
      "785 [D loss: 0.601227, acc.: 71.88%] [G loss: 1.750950]\n",
      "786 [D loss: 0.596424, acc.: 74.22%] [G loss: 2.274856]\n",
      "787 [D loss: 0.625259, acc.: 63.28%] [G loss: 1.505318]\n",
      "788 [D loss: 0.622860, acc.: 74.22%] [G loss: 2.560168]\n",
      "789 [D loss: 0.561897, acc.: 75.78%] [G loss: 2.401291]\n",
      "790 [D loss: 0.534991, acc.: 84.38%] [G loss: 2.136246]\n",
      "791 [D loss: 0.509614, acc.: 81.25%] [G loss: 1.822872]\n",
      "792 [D loss: 0.638411, acc.: 60.16%] [G loss: 1.737484]\n",
      "793 [D loss: 0.697097, acc.: 50.00%] [G loss: 2.327893]\n",
      "794 [D loss: 0.565717, acc.: 72.66%] [G loss: 3.178090]\n",
      "795 [D loss: 0.548290, acc.: 82.81%] [G loss: 2.919655]\n",
      "796 [D loss: 0.524956, acc.: 80.47%] [G loss: 2.386301]\n",
      "797 [D loss: 0.580528, acc.: 78.12%] [G loss: 2.147512]\n",
      "798 [D loss: 0.574643, acc.: 82.03%] [G loss: 2.241233]\n",
      "799 [D loss: 0.579100, acc.: 76.56%] [G loss: 2.239922]\n",
      "800 [D loss: 0.522018, acc.: 84.38%] [G loss: 2.054315]\n",
      "801 [D loss: 0.551628, acc.: 79.69%] [G loss: 2.161614]\n",
      "802 [D loss: 0.588541, acc.: 71.09%] [G loss: 1.549335]\n",
      "803 [D loss: 0.619980, acc.: 65.62%] [G loss: 1.318582]\n",
      "804 [D loss: 0.588668, acc.: 76.56%] [G loss: 1.575739]\n",
      "805 [D loss: 0.629804, acc.: 78.91%] [G loss: 1.588402]\n",
      "806 [D loss: 0.574376, acc.: 75.00%] [G loss: 2.107914]\n",
      "807 [D loss: 0.623814, acc.: 62.50%] [G loss: 2.350489]\n",
      "808 [D loss: 0.591760, acc.: 73.44%] [G loss: 1.957117]\n",
      "809 [D loss: 0.580273, acc.: 71.09%] [G loss: 1.845913]\n",
      "810 [D loss: 0.572347, acc.: 79.69%] [G loss: 2.196358]\n",
      "811 [D loss: 0.574114, acc.: 75.00%] [G loss: 2.888719]\n",
      "812 [D loss: 0.568903, acc.: 73.44%] [G loss: 2.594022]\n",
      "813 [D loss: 0.554143, acc.: 75.00%] [G loss: 2.057887]\n",
      "814 [D loss: 0.692000, acc.: 49.22%] [G loss: 1.845109]\n",
      "815 [D loss: 0.632236, acc.: 63.28%] [G loss: 1.948159]\n",
      "816 [D loss: 0.606409, acc.: 63.28%] [G loss: 1.874197]\n",
      "817 [D loss: 0.672573, acc.: 66.41%] [G loss: 1.832083]\n",
      "818 [D loss: 0.597748, acc.: 67.19%] [G loss: 1.820880]\n",
      "819 [D loss: 0.603014, acc.: 68.75%] [G loss: 1.898304]\n",
      "820 [D loss: 0.586787, acc.: 74.22%] [G loss: 1.576045]\n",
      "821 [D loss: 0.569642, acc.: 77.34%] [G loss: 1.519670]\n",
      "822 [D loss: 0.555246, acc.: 80.47%] [G loss: 1.536404]\n",
      "823 [D loss: 0.589951, acc.: 74.22%] [G loss: 1.729548]\n",
      "824 [D loss: 0.614528, acc.: 68.75%] [G loss: 1.876597]\n",
      "825 [D loss: 0.631060, acc.: 68.75%] [G loss: 1.498013]\n",
      "826 [D loss: 0.667557, acc.: 53.91%] [G loss: 1.769103]\n",
      "827 [D loss: 0.711387, acc.: 44.53%] [G loss: 1.718626]\n",
      "828 [D loss: 0.714516, acc.: 47.66%] [G loss: 1.133598]\n",
      "829 [D loss: 0.598492, acc.: 69.53%] [G loss: 1.887651]\n",
      "830 [D loss: 0.698227, acc.: 46.88%] [G loss: 1.580130]\n",
      "831 [D loss: 0.659403, acc.: 46.09%] [G loss: 1.637655]\n",
      "832 [D loss: 0.656518, acc.: 50.78%] [G loss: 2.133154]\n",
      "833 [D loss: 0.617074, acc.: 71.88%] [G loss: 1.795762]\n",
      "834 [D loss: 0.645124, acc.: 60.94%] [G loss: 1.867098]\n",
      "835 [D loss: 0.602714, acc.: 66.41%] [G loss: 1.932841]\n",
      "836 [D loss: 0.622528, acc.: 64.06%] [G loss: 2.080296]\n",
      "837 [D loss: 0.603779, acc.: 65.62%] [G loss: 1.995027]\n",
      "838 [D loss: 0.687958, acc.: 49.22%] [G loss: 2.031313]\n",
      "839 [D loss: 0.724407, acc.: 46.09%] [G loss: 1.335078]\n",
      "840 [D loss: 0.669163, acc.: 53.91%] [G loss: 1.518452]\n",
      "841 [D loss: 0.875303, acc.: 44.53%] [G loss: 1.608039]\n",
      "842 [D loss: 0.597545, acc.: 73.44%] [G loss: 1.942390]\n",
      "843 [D loss: 0.579157, acc.: 73.44%] [G loss: 1.542747]\n",
      "844 [D loss: 0.598401, acc.: 73.44%] [G loss: 1.372712]\n",
      "845 [D loss: 0.609772, acc.: 70.31%] [G loss: 1.520261]\n",
      "846 [D loss: 0.596345, acc.: 70.31%] [G loss: 1.697253]\n",
      "847 [D loss: 0.651159, acc.: 72.66%] [G loss: 1.871184]\n",
      "848 [D loss: 0.640702, acc.: 64.06%] [G loss: 1.639101]\n",
      "849 [D loss: 0.593848, acc.: 73.44%] [G loss: 1.385594]\n",
      "850 [D loss: 0.592848, acc.: 72.66%] [G loss: 2.104081]\n",
      "851 [D loss: 0.678890, acc.: 67.19%] [G loss: 2.287073]\n",
      "852 [D loss: 0.644562, acc.: 64.84%] [G loss: 1.289587]\n",
      "853 [D loss: 0.799139, acc.: 63.28%] [G loss: 1.637098]\n",
      "854 [D loss: 0.805995, acc.: 70.31%] [G loss: 1.854202]\n",
      "855 [D loss: 0.591612, acc.: 65.62%] [G loss: 2.520817]\n",
      "856 [D loss: 0.609992, acc.: 67.19%] [G loss: 2.020585]\n",
      "857 [D loss: 0.540603, acc.: 76.56%] [G loss: 2.421135]\n",
      "858 [D loss: 0.555064, acc.: 73.44%] [G loss: 1.802573]\n",
      "859 [D loss: 0.615633, acc.: 70.31%] [G loss: 2.141078]\n",
      "860 [D loss: 0.574205, acc.: 78.91%] [G loss: 1.792240]\n",
      "861 [D loss: 0.558994, acc.: 81.25%] [G loss: 1.444398]\n",
      "862 [D loss: 0.555643, acc.: 74.22%] [G loss: 1.621823]\n",
      "863 [D loss: 0.597967, acc.: 75.78%] [G loss: 1.606528]\n",
      "864 [D loss: 0.575464, acc.: 76.56%] [G loss: 1.969493]\n",
      "865 [D loss: 0.616284, acc.: 70.31%] [G loss: 2.495487]\n",
      "866 [D loss: 0.504364, acc.: 80.47%] [G loss: 2.629496]\n",
      "867 [D loss: 0.608868, acc.: 73.44%] [G loss: 2.174121]\n",
      "868 [D loss: 0.688597, acc.: 49.22%] [G loss: 1.981863]\n",
      "869 [D loss: 0.528746, acc.: 79.69%] [G loss: 2.101016]\n",
      "870 [D loss: 0.539447, acc.: 75.78%] [G loss: 1.986913]\n",
      "871 [D loss: 0.607115, acc.: 71.88%] [G loss: 1.285434]\n",
      "872 [D loss: 0.603454, acc.: 71.88%] [G loss: 1.304446]\n",
      "873 [D loss: 0.595154, acc.: 70.31%] [G loss: 1.398586]\n",
      "874 [D loss: 0.553520, acc.: 77.34%] [G loss: 1.575816]\n",
      "875 [D loss: 0.565748, acc.: 79.69%] [G loss: 1.996576]\n",
      "876 [D loss: 0.571875, acc.: 75.00%] [G loss: 1.487119]\n",
      "877 [D loss: 0.590094, acc.: 78.12%] [G loss: 2.208483]\n",
      "878 [D loss: 0.499409, acc.: 81.25%] [G loss: 2.049356]\n",
      "879 [D loss: 0.495004, acc.: 79.69%] [G loss: 1.873855]\n",
      "880 [D loss: 0.544106, acc.: 78.91%] [G loss: 1.965867]\n",
      "881 [D loss: 0.561957, acc.: 79.69%] [G loss: 2.359582]\n",
      "882 [D loss: 0.620383, acc.: 71.09%] [G loss: 1.756875]\n",
      "883 [D loss: 0.658913, acc.: 60.16%] [G loss: 1.676285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 [D loss: 0.636376, acc.: 54.69%] [G loss: 1.775900]\n",
      "885 [D loss: 0.572682, acc.: 75.00%] [G loss: 1.993074]\n",
      "886 [D loss: 0.536603, acc.: 77.34%] [G loss: 2.153284]\n",
      "887 [D loss: 0.553620, acc.: 74.22%] [G loss: 1.701163]\n",
      "888 [D loss: 0.620878, acc.: 67.19%] [G loss: 1.625551]\n",
      "889 [D loss: 0.652042, acc.: 61.72%] [G loss: 1.272920]\n",
      "890 [D loss: 0.793006, acc.: 57.03%] [G loss: 1.383049]\n",
      "891 [D loss: 0.644904, acc.: 55.47%] [G loss: 1.984232]\n",
      "892 [D loss: 0.597622, acc.: 69.53%] [G loss: 1.951907]\n",
      "893 [D loss: 0.542793, acc.: 78.91%] [G loss: 1.976114]\n",
      "894 [D loss: 0.689686, acc.: 55.47%] [G loss: 1.869825]\n",
      "895 [D loss: 0.635912, acc.: 54.69%] [G loss: 1.577805]\n",
      "896 [D loss: 0.597836, acc.: 69.53%] [G loss: 1.726808]\n",
      "897 [D loss: 0.637276, acc.: 64.06%] [G loss: 1.583239]\n",
      "898 [D loss: 0.672465, acc.: 62.50%] [G loss: 1.516470]\n",
      "899 [D loss: 0.655053, acc.: 74.22%] [G loss: 1.446781]\n",
      "900 [D loss: 0.549668, acc.: 79.69%] [G loss: 1.810255]\n",
      "901 [D loss: 0.623266, acc.: 70.31%] [G loss: 1.321230]\n",
      "902 [D loss: 0.632007, acc.: 64.06%] [G loss: 1.683693]\n",
      "903 [D loss: 0.571914, acc.: 77.34%] [G loss: 1.712953]\n",
      "904 [D loss: 0.665310, acc.: 68.75%] [G loss: 1.499252]\n",
      "905 [D loss: 0.608770, acc.: 72.66%] [G loss: 1.443092]\n",
      "906 [D loss: 0.633203, acc.: 65.62%] [G loss: 1.433628]\n",
      "907 [D loss: 0.642515, acc.: 56.25%] [G loss: 1.294136]\n",
      "908 [D loss: 0.577583, acc.: 81.25%] [G loss: 1.851967]\n",
      "909 [D loss: 0.638147, acc.: 62.50%] [G loss: 1.271084]\n",
      "910 [D loss: 0.876746, acc.: 67.97%] [G loss: 1.362988]\n",
      "911 [D loss: 0.596938, acc.: 66.41%] [G loss: 1.633101]\n",
      "912 [D loss: 0.679308, acc.: 50.78%] [G loss: 2.004729]\n",
      "913 [D loss: 0.575157, acc.: 75.00%] [G loss: 1.652271]\n",
      "914 [D loss: 0.531965, acc.: 79.69%] [G loss: 1.844191]\n",
      "915 [D loss: 0.563787, acc.: 77.34%] [G loss: 1.963002]\n",
      "916 [D loss: 0.579768, acc.: 72.66%] [G loss: 1.425331]\n",
      "917 [D loss: 0.645750, acc.: 77.34%] [G loss: 1.428248]\n",
      "918 [D loss: 0.628931, acc.: 66.41%] [G loss: 1.905562]\n",
      "919 [D loss: 0.589591, acc.: 64.84%] [G loss: 1.630890]\n",
      "920 [D loss: 0.632074, acc.: 67.97%] [G loss: 1.340601]\n",
      "921 [D loss: 0.650240, acc.: 59.38%] [G loss: 1.253520]\n",
      "922 [D loss: 0.672083, acc.: 49.22%] [G loss: 1.045230]\n",
      "923 [D loss: 0.604769, acc.: 72.66%] [G loss: 1.325558]\n",
      "924 [D loss: 0.552335, acc.: 77.34%] [G loss: 1.430378]\n",
      "925 [D loss: 0.624223, acc.: 68.75%] [G loss: 1.522379]\n",
      "926 [D loss: 0.676790, acc.: 53.91%] [G loss: 1.177713]\n",
      "927 [D loss: 0.671874, acc.: 66.41%] [G loss: 1.443098]\n",
      "928 [D loss: 0.617744, acc.: 70.31%] [G loss: 1.458137]\n",
      "929 [D loss: 0.645997, acc.: 60.16%] [G loss: 1.156913]\n",
      "930 [D loss: 0.692606, acc.: 55.47%] [G loss: 1.286108]\n",
      "931 [D loss: 0.589468, acc.: 68.75%] [G loss: 1.647186]\n",
      "932 [D loss: 0.650058, acc.: 57.81%] [G loss: 2.782423]\n",
      "933 [D loss: 0.605448, acc.: 72.66%] [G loss: 1.841761]\n",
      "934 [D loss: 0.569910, acc.: 69.53%] [G loss: 1.886275]\n",
      "935 [D loss: 0.720823, acc.: 48.44%] [G loss: 1.736640]\n",
      "936 [D loss: 0.579922, acc.: 71.88%] [G loss: 1.937590]\n",
      "937 [D loss: 0.501426, acc.: 75.78%] [G loss: 2.431772]\n",
      "938 [D loss: 0.590982, acc.: 68.75%] [G loss: 2.082321]\n",
      "939 [D loss: 0.496034, acc.: 83.59%] [G loss: 2.132747]\n",
      "940 [D loss: 0.608959, acc.: 66.41%] [G loss: 1.146058]\n",
      "941 [D loss: 0.645125, acc.: 59.38%] [G loss: 1.433121]\n",
      "942 [D loss: 0.653660, acc.: 57.81%] [G loss: 2.173099]\n",
      "943 [D loss: 0.626963, acc.: 65.62%] [G loss: 1.488127]\n",
      "944 [D loss: 0.644108, acc.: 61.72%] [G loss: 1.568699]\n",
      "945 [D loss: 0.664576, acc.: 60.94%] [G loss: 1.675339]\n",
      "946 [D loss: 0.603445, acc.: 75.00%] [G loss: 1.942609]\n",
      "947 [D loss: 0.555522, acc.: 79.69%] [G loss: 1.366645]\n",
      "948 [D loss: 0.552467, acc.: 79.69%] [G loss: 2.028069]\n",
      "949 [D loss: 0.564898, acc.: 74.22%] [G loss: 1.678348]\n",
      "950 [D loss: 0.599579, acc.: 64.84%] [G loss: 1.449583]\n",
      "951 [D loss: 0.632023, acc.: 67.19%] [G loss: 1.706188]\n",
      "952 [D loss: 0.572865, acc.: 78.12%] [G loss: 1.733284]\n",
      "953 [D loss: 0.574461, acc.: 76.56%] [G loss: 1.242684]\n",
      "954 [D loss: 0.653204, acc.: 57.81%] [G loss: 1.183511]\n",
      "955 [D loss: 0.637343, acc.: 62.50%] [G loss: 1.241878]\n",
      "956 [D loss: 0.644117, acc.: 64.06%] [G loss: 1.657463]\n",
      "957 [D loss: 0.730252, acc.: 67.19%] [G loss: 1.279411]\n",
      "958 [D loss: 0.613335, acc.: 69.53%] [G loss: 2.146050]\n",
      "959 [D loss: 0.571613, acc.: 76.56%] [G loss: 2.000098]\n",
      "960 [D loss: 0.555779, acc.: 75.78%] [G loss: 1.661944]\n",
      "961 [D loss: 0.602493, acc.: 69.53%] [G loss: 1.501384]\n",
      "962 [D loss: 0.603674, acc.: 68.75%] [G loss: 1.590570]\n",
      "963 [D loss: 0.561627, acc.: 75.00%] [G loss: 1.663288]\n",
      "964 [D loss: 0.573700, acc.: 71.09%] [G loss: 2.375140]\n",
      "965 [D loss: 0.548856, acc.: 75.00%] [G loss: 1.833856]\n",
      "966 [D loss: 0.563623, acc.: 76.56%] [G loss: 2.018926]\n",
      "967 [D loss: 0.682634, acc.: 61.72%] [G loss: 1.373441]\n",
      "968 [D loss: 0.615299, acc.: 71.09%] [G loss: 2.175026]\n",
      "969 [D loss: 0.525947, acc.: 81.25%] [G loss: 1.693163]\n",
      "970 [D loss: 0.574483, acc.: 73.44%] [G loss: 1.556263]\n",
      "971 [D loss: 0.604181, acc.: 71.88%] [G loss: 1.637848]\n",
      "972 [D loss: 0.561874, acc.: 81.25%] [G loss: 1.340324]\n",
      "973 [D loss: 0.525367, acc.: 79.69%] [G loss: 1.541757]\n",
      "974 [D loss: 0.541648, acc.: 79.69%] [G loss: 1.556813]\n",
      "975 [D loss: 0.591167, acc.: 74.22%] [G loss: 0.986280]\n",
      "976 [D loss: 0.599711, acc.: 67.97%] [G loss: 1.121012]\n",
      "977 [D loss: 0.618157, acc.: 71.88%] [G loss: 1.257797]\n",
      "978 [D loss: 0.615042, acc.: 74.22%] [G loss: 1.383830]\n",
      "979 [D loss: 0.652733, acc.: 59.38%] [G loss: 1.426345]\n",
      "980 [D loss: 0.606532, acc.: 68.75%] [G loss: 1.184349]\n",
      "981 [D loss: 0.537255, acc.: 80.47%] [G loss: 1.238473]\n",
      "982 [D loss: 0.588888, acc.: 77.34%] [G loss: 1.098150]\n",
      "983 [D loss: 0.657805, acc.: 71.88%] [G loss: 1.610782]\n",
      "984 [D loss: 0.618804, acc.: 73.44%] [G loss: 1.737125]\n",
      "985 [D loss: 0.632121, acc.: 75.78%] [G loss: 1.495080]\n",
      "986 [D loss: 0.543117, acc.: 79.69%] [G loss: 1.610603]\n",
      "987 [D loss: 0.614059, acc.: 68.75%] [G loss: 1.874863]\n",
      "988 [D loss: 0.598517, acc.: 65.62%] [G loss: 1.815874]\n",
      "989 [D loss: 0.544517, acc.: 78.12%] [G loss: 1.904725]\n",
      "990 [D loss: 0.502795, acc.: 85.16%] [G loss: 1.357349]\n",
      "991 [D loss: 0.771094, acc.: 77.34%] [G loss: 1.557305]\n",
      "992 [D loss: 0.693676, acc.: 68.75%] [G loss: 1.234341]\n",
      "993 [D loss: 0.642615, acc.: 63.28%] [G loss: 1.369739]\n",
      "994 [D loss: 0.627209, acc.: 65.62%] [G loss: 1.490324]\n",
      "995 [D loss: 0.652277, acc.: 69.53%] [G loss: 1.683576]\n",
      "996 [D loss: 0.609977, acc.: 66.41%] [G loss: 1.713207]\n",
      "997 [D loss: 0.583579, acc.: 75.78%] [G loss: 1.840103]\n",
      "998 [D loss: 0.597613, acc.: 77.34%] [G loss: 1.377458]\n",
      "999 [D loss: 0.543692, acc.: 75.78%] [G loss: 1.782525]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1000, data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41407, 34848)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 2. 2. ... 3. 1. 2.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[1 1 1 0 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "gen = 10\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "new_mails = generator.predict(noise)\n",
    "print(np.round(new_mails))\n",
    "\n",
    "idx = np.random.randint(0, X_train.shape[1], gen)\n",
    "imgs = X_train[idx]\n",
    "print(imgs)\n",
    "\n",
    "generated_labels = clf.predict(new_mails)\n",
    "print(generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['changing', 'feature', 'feels', 'forgot', 'future', 'hundred',\n",
       "        'include', 'like', 'logo', 'love', 'loved', 'marco', 'memory',\n",
       "        'new', 'options', 'overnight', 'pay', 'perfect', 'ported',\n",
       "        'refund', 'sleek', 'slide', 'slowest', 'still', 'straight', 'sure',\n",
       "        'unneeded', 'well', 'worked', 'working', 'works'], dtype='<U112'),\n",
       " array(['cause', 'compare', 'depending', 'end', 'exactly', 'excellent',\n",
       "        'fast', 'feature', 'feels', 'great', 'greenify', 'half', 'happy',\n",
       "        'iphone', 'know', 'love', 'memory', 'new', 'nice', 'perfect',\n",
       "        'phone', 'price', 'quality', 'samsung', 'simple', 'slowest',\n",
       "        'sure', 'using', 'version'], dtype='<U112'),\n",
       " array(['bought', 'exactly', 'feature', 'forgot', 'future', 'girl',\n",
       "        'great', 'hundred', 'marco', 'nice', 'ok', 'overnight', 'price',\n",
       "        'product', 'sd', 'stars', 'unneeded', 'works'], dtype='<U112'),\n",
       " array(['awesome', 'back', 'bad', 'based', 'blocks', 'bought', 'budget',\n",
       "        'called', 'case', 'cause', 'charger', 'come', 'compare', 'didn',\n",
       "        'disappointed', 'end', 'galaxy', 'gigs', 'great', 'greenify',\n",
       "        'haunting', 'husband', 'initially', 'isn', 'issues', 'light',\n",
       "        'moto', 'nice', 'now', 'ok', 'only', 'perfect', 'phone', 'phones',\n",
       "        'pretty', 'price', 'problems', 'purchasing', 'put', 'quality',\n",
       "        'refund', 'responded', 'return', 'returning', 'samsung', 'says',\n",
       "        'screen', 'see', 'seller', 'slowest', 'specially', 'sprint',\n",
       "        'started', 'talk', 'time', 'turn', 'using', 'volume', 'wrong'],\n",
       "       dtype='<U112'),\n",
       " array(['awesome', 'feature', 'iphone', 'like', 'little', 'love', 'new',\n",
       "        'nice', 'overnight', 'phone', 'quality', 'samsung', 'see',\n",
       "        'simple', 'slowest', 'using'], dtype='<U112'),\n",
       " array(['awesome', 'cause', 'compare', 'depending', 'end', 'fast',\n",
       "        'feature', 'feels', 'forgot', 'great', 'like', 'loved', 'memory',\n",
       "        'nice', 'perfect', 'phone', 'price', 'quality', 'sd', 'seller',\n",
       "        'settings', 'simple', 'slowest', 'standby', 'sure', 'unneeded',\n",
       "        'using'], dtype='<U112'),\n",
       " array(['awesome', 'budget', 'fast', 'feature', 'girl', 'great',\n",
       "        'greenify', 'know', 'nice', 'ok', 'price', 'quality', 'slowest',\n",
       "        'standby', 'stars', 'using', 'volume'], dtype='<U112'),\n",
       " array(['__', '___', '____', ..., '', '', ''], dtype='<U112'),\n",
       " array(['adjustments', 'agent', 'avail', 'bam', 'basic', 'bathtub',\n",
       "        'begun', 'blocks', 'boom', 'called', 'calling', 'camera', 'charge',\n",
       "        'charger', 'chassis', 'cheaper', 'cheapest', 'classic',\n",
       "        'connectivity', 'customer', 'dead', 'declared', 'description',\n",
       "        'diagnostic', 'difficulties', 'dissapointed', 'downloaded', 'edge',\n",
       "        'effective', 'electronics', 'eliminate', 'european', 'expect',\n",
       "        'fake', 'fire', 'fix', 'flush', 'fyi', 'fbrica', 'garbled',\n",
       "        'gifts', 'gripe', 'hammer', 'hang', 'impecable', 'instructions',\n",
       "        'issue', 'kinda', 'knowing', 'learn', 'leave', 'leaves', 'lofty',\n",
       "        'look', 'looks', 'mark', 'mega', 'missing', 'mobility', 'models',\n",
       "        'new', 'nonsense', 'note', 'odd', 'only', 'paid', 'passwords',\n",
       "        'people', 'person', 'phone', 'phones', 'play', 'playlists',\n",
       "        'possible', 'powered', 'problems', 'produt', 'prohibitive',\n",
       "        'promised', 'promptly', 'quit', 'realized', 'red', 'registering',\n",
       "        'reluctant', 'removable', 'reorder', 'repair', 'replace',\n",
       "        'residence', 'responded', 'rocking', 'runs', 'samsung', 'screen',\n",
       "        'sections', 'secured', 'service', 'software', 'solution',\n",
       "        'specially', 'staying', 'string', 'sweet', 'take', 'takes', 'th',\n",
       "        'town', 'truly', 'try', 'trying', 'turn', 'tv', 'upset', 'vale',\n",
       "        've', 'versin', 'virgin', 'wasn', 'whatsapp', 'windows', 'wonder',\n",
       "        'worse', 'year', 'yrs'], dtype='<U112'),\n",
       " array(['blocks', 'bought', 'broken', 'budget', 'cause', 'charger',\n",
       "        'compare', 'condition', 'excellent', 'greenify', 'happy', 'iphone',\n",
       "        'know', 'new', 'nice', 'perfect', 'phone', 'price', 'quality',\n",
       "        'reassembled', 'slowest', 'stars', 'thrown', 'volume'],\n",
       "       dtype='<U112')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(new_mails) # See the generated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
