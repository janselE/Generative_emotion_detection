{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example from https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')\n",
    "df_dataset = pd.read_json('clothing_dataset/renttherunway_final_data.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Schema:\\n\", df_idf.dtypes)\n",
    "# print(\"Shape of database =\", df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "df_dataset['text'] = df_dataset['review_summary'] + \" \" + df_dataset['review_text']\n",
    "df_dataset['text'] = df_dataset['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "sub_dataset = df_dataset[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192544\n"
     ]
    }
   ],
   "source": [
    "# print(df_idf['text'][2])\n",
    "# print()\n",
    "# print(sub_dataset['text'][0])\n",
    "\n",
    "# print(df_idf['text'])\n",
    "\n",
    "\n",
    "all_data = df_idf['text'].append(sub_dataset['text'])\n",
    "print(len(sub_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stop_words('stopwords.txt')\n",
    "docs = all_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df = .85, stop_words=stopwords)\n",
    "wordCountVec = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'best',\n",
       " 'worst',\n",
       " 'samsung',\n",
       " 'awhile',\n",
       " 'absolute',\n",
       " 'doo',\n",
       " 'read',\n",
       " 'review',\n",
       " 'detect']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "# tfidf.fit(wordCountVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_coo(coo_matrix):\n",
    "#     tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "#     return sorted(tuples, key=lambda x: (x[1] , x[0]), reverse=True)\n",
    "\n",
    "# def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "#     sorted_items = sorted_items[:topn]\n",
    "    \n",
    "#     score_vals = []\n",
    "#     feature_vals = []\n",
    "    \n",
    "#     for idx, score in sorted_items:\n",
    "#         score_vals.append(round(score, 3))\n",
    "#         feature_vals.append(feature_names[idx])\n",
    "        \n",
    "#         results = {}\n",
    "#         for idx in range(len(feature_vals)):\n",
    "#             results[feature_vals[idx]]=score_vals[idx]\n",
    "            \n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = cv.get_feature_names()\n",
    "\n",
    "# doc = docs[1]\n",
    "\n",
    "# tf_idf_vector = tfidf.transform(cv.transform([doc]))\n",
    "\n",
    "# sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# keywords = extract_topn_from_vector(feature_names, sorted_items, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192544\n"
     ]
    }
   ],
   "source": [
    "# test = cv.transform([doc])\n",
    "\n",
    "# print(keywords)\n",
    "# for idx in range(len(sorted_items)):\n",
    "#     print(feature_names[sorted_items[idx][0]], sorted_items[idx][1])\n",
    "print(len(df_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dataset['rating']\n",
    "# fixing the labels, if > 3.5 is going to be 1 which is positive, else 0\n",
    "y = y.apply(lambda x: 1 if x > 5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()\n",
    "x = wordCountVec.toarray()[:len(df_dataset)]\n",
    "print(x.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[:, :], y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a binary file\n",
    "import pickle\n",
    "filename = 'model2.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from the binary file\n",
    "import pickle\n",
    "filename = 'model.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[:, 0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_p = clf.predict(X_train)\n",
    "# y_test_p = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y_test)):\n",
    "#     print(y_test_p[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# print(\"Accuracy in testing set:\", accuracy_score(y_test, y_test_p))\n",
    "# print(\"Accuracy in training set:\", accuracy_score(y_train, y_train_p))\n",
    "# print(confusion_matrix(y_test, y_test_p))\n",
    "# print(confusion_matrix(y_train, y_train_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = cv.transform([\"Hate\", \"Good\", \"Awful\", \"Best\"]).toarray()\n",
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(110, 120):\n",
    "#     test = cv.transform([docs[i]]).toarray()\n",
    "#     p = clf.predict(test)\n",
    "#     print(docs[i], p, y[i])\n",
    "    \n",
    "#     print(type(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we start building the GANs, this model takes the word embedding and generate new embeddings that are similar to the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape):\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               17842688  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 17,974,273\n",
      "Trainable params: 17,974,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 34848)             35719200  \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 34848)             0         \n",
      "=================================================================\n",
      "Total params: 36,409,120\n",
      "Trainable params: 36,405,536\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "\n",
    "img_rows = 1\n",
    "img_cols = X_train[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(self, pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.694669, acc.: 49.22%] [G loss: 0.963271]\n",
      "1 [D loss: 0.781345, acc.: 68.75%] [G loss: 0.987514]\n",
      "2 [D loss: 0.836146, acc.: 69.53%] [G loss: 1.056294]\n",
      "3 [D loss: 1.161055, acc.: 58.59%] [G loss: 1.251272]\n",
      "4 [D loss: 0.899115, acc.: 71.09%] [G loss: 1.401319]\n",
      "5 [D loss: 0.882163, acc.: 65.62%] [G loss: 1.593820]\n",
      "6 [D loss: 0.876900, acc.: 68.75%] [G loss: 1.877352]\n",
      "7 [D loss: 0.729376, acc.: 71.88%] [G loss: 1.988602]\n",
      "8 [D loss: 0.688198, acc.: 66.41%] [G loss: 2.270581]\n",
      "9 [D loss: 0.700677, acc.: 62.50%] [G loss: 2.547071]\n",
      "10 [D loss: 0.658148, acc.: 71.09%] [G loss: 2.873407]\n",
      "11 [D loss: 0.478531, acc.: 75.00%] [G loss: 3.039404]\n",
      "12 [D loss: 0.632129, acc.: 71.09%] [G loss: 3.033529]\n",
      "13 [D loss: 0.485821, acc.: 76.56%] [G loss: 3.237605]\n",
      "14 [D loss: 0.550982, acc.: 68.75%] [G loss: 3.539235]\n",
      "15 [D loss: 0.498697, acc.: 77.34%] [G loss: 3.619682]\n",
      "16 [D loss: 0.436810, acc.: 85.16%] [G loss: 3.434534]\n",
      "17 [D loss: 0.494989, acc.: 85.16%] [G loss: 3.520747]\n",
      "18 [D loss: 0.455619, acc.: 84.38%] [G loss: 3.711877]\n",
      "19 [D loss: 0.478387, acc.: 89.06%] [G loss: 4.013745]\n",
      "20 [D loss: 0.465241, acc.: 82.81%] [G loss: 4.172201]\n",
      "21 [D loss: 0.533588, acc.: 80.47%] [G loss: 4.300178]\n",
      "22 [D loss: 0.378685, acc.: 91.41%] [G loss: 4.204320]\n",
      "23 [D loss: 0.438533, acc.: 85.94%] [G loss: 4.278266]\n",
      "24 [D loss: 0.429822, acc.: 88.28%] [G loss: 4.291743]\n",
      "25 [D loss: 0.367801, acc.: 91.41%] [G loss: 4.211768]\n",
      "26 [D loss: 0.367982, acc.: 94.53%] [G loss: 4.289474]\n",
      "27 [D loss: 0.363621, acc.: 92.97%] [G loss: 4.223760]\n",
      "28 [D loss: 0.396008, acc.: 90.62%] [G loss: 4.304004]\n",
      "29 [D loss: 0.394397, acc.: 92.19%] [G loss: 4.449445]\n",
      "30 [D loss: 0.444053, acc.: 88.28%] [G loss: 4.789124]\n",
      "31 [D loss: 0.330051, acc.: 93.75%] [G loss: 4.775753]\n",
      "32 [D loss: 0.319623, acc.: 94.53%] [G loss: 4.606793]\n",
      "33 [D loss: 0.323574, acc.: 95.31%] [G loss: 4.591719]\n",
      "34 [D loss: 0.319236, acc.: 94.53%] [G loss: 4.481606]\n",
      "35 [D loss: 0.421904, acc.: 91.41%] [G loss: 4.598547]\n",
      "36 [D loss: 0.358521, acc.: 93.75%] [G loss: 4.716866]\n",
      "37 [D loss: 0.312749, acc.: 93.75%] [G loss: 4.681842]\n",
      "38 [D loss: 0.311063, acc.: 97.66%] [G loss: 4.679089]\n",
      "39 [D loss: 0.283794, acc.: 98.44%] [G loss: 4.737048]\n",
      "40 [D loss: 0.339244, acc.: 95.31%] [G loss: 4.678870]\n",
      "41 [D loss: 0.311491, acc.: 95.31%] [G loss: 4.941708]\n",
      "42 [D loss: 0.403575, acc.: 90.62%] [G loss: 5.373185]\n",
      "43 [D loss: 0.292919, acc.: 99.22%] [G loss: 5.746503]\n",
      "44 [D loss: 0.274750, acc.: 98.44%] [G loss: 5.310981]\n",
      "45 [D loss: 0.316688, acc.: 97.66%] [G loss: 5.150929]\n",
      "46 [D loss: 0.369814, acc.: 89.84%] [G loss: 5.358916]\n",
      "47 [D loss: 0.322142, acc.: 93.75%] [G loss: 5.724769]\n",
      "48 [D loss: 0.296351, acc.: 96.88%] [G loss: 5.597439]\n",
      "49 [D loss: 0.271674, acc.: 98.44%] [G loss: 5.664950]\n",
      "50 [D loss: 0.294697, acc.: 96.88%] [G loss: 5.704259]\n",
      "51 [D loss: 0.328013, acc.: 94.53%] [G loss: 6.069795]\n",
      "52 [D loss: 0.316483, acc.: 94.53%] [G loss: 6.843237]\n",
      "53 [D loss: 0.305754, acc.: 94.53%] [G loss: 7.142498]\n",
      "54 [D loss: 0.305424, acc.: 95.31%] [G loss: 7.539225]\n",
      "55 [D loss: 0.309107, acc.: 96.09%] [G loss: 7.832669]\n",
      "56 [D loss: 0.592343, acc.: 86.72%] [G loss: 8.500966]\n",
      "57 [D loss: 0.455570, acc.: 90.62%] [G loss: 8.991825]\n",
      "58 [D loss: 0.600986, acc.: 87.50%] [G loss: 9.387657]\n",
      "59 [D loss: 0.727001, acc.: 82.81%] [G loss: 10.491570]\n",
      "60 [D loss: 0.534343, acc.: 90.62%] [G loss: 10.714561]\n",
      "61 [D loss: 0.813267, acc.: 85.16%] [G loss: 10.424639]\n",
      "62 [D loss: 0.461410, acc.: 91.41%] [G loss: 11.297602]\n",
      "63 [D loss: 0.381146, acc.: 92.97%] [G loss: 11.741505]\n",
      "64 [D loss: 0.305083, acc.: 94.53%] [G loss: 10.349751]\n",
      "65 [D loss: 1.036529, acc.: 82.03%] [G loss: 11.251897]\n",
      "66 [D loss: 0.255204, acc.: 99.22%] [G loss: 10.175239]\n",
      "67 [D loss: 0.537445, acc.: 92.19%] [G loss: 10.644522]\n",
      "68 [D loss: 0.243374, acc.: 99.22%] [G loss: 10.276964]\n",
      "69 [D loss: 0.447733, acc.: 93.75%] [G loss: 9.737749]\n",
      "70 [D loss: 0.300198, acc.: 93.75%] [G loss: 9.356200]\n",
      "71 [D loss: 0.284133, acc.: 98.44%] [G loss: 8.327574]\n",
      "72 [D loss: 0.298353, acc.: 96.88%] [G loss: 8.442282]\n",
      "73 [D loss: 0.279190, acc.: 96.88%] [G loss: 7.909426]\n",
      "74 [D loss: 0.223115, acc.: 100.00%] [G loss: 7.952270]\n",
      "75 [D loss: 0.243170, acc.: 99.22%] [G loss: 7.797355]\n",
      "76 [D loss: 0.230369, acc.: 100.00%] [G loss: 7.115590]\n",
      "77 [D loss: 0.238589, acc.: 100.00%] [G loss: 6.468756]\n",
      "78 [D loss: 0.216037, acc.: 100.00%] [G loss: 6.618860]\n",
      "79 [D loss: 0.213029, acc.: 100.00%] [G loss: 6.623537]\n",
      "80 [D loss: 0.220542, acc.: 100.00%] [G loss: 6.722982]\n",
      "81 [D loss: 0.233617, acc.: 100.00%] [G loss: 7.096951]\n",
      "82 [D loss: 0.210662, acc.: 100.00%] [G loss: 7.356941]\n",
      "83 [D loss: 0.218177, acc.: 100.00%] [G loss: 7.475101]\n",
      "84 [D loss: 0.217398, acc.: 99.22%] [G loss: 7.835541]\n",
      "85 [D loss: 0.208507, acc.: 99.22%] [G loss: 6.763243]\n",
      "86 [D loss: 0.210525, acc.: 100.00%] [G loss: 6.773044]\n",
      "87 [D loss: 0.225078, acc.: 99.22%] [G loss: 7.298141]\n",
      "88 [D loss: 0.265733, acc.: 96.09%] [G loss: 8.061543]\n",
      "89 [D loss: 0.225485, acc.: 99.22%] [G loss: 8.074430]\n",
      "90 [D loss: 0.220481, acc.: 98.44%] [G loss: 8.044104]\n",
      "91 [D loss: 0.203370, acc.: 100.00%] [G loss: 7.642558]\n",
      "92 [D loss: 0.333047, acc.: 93.75%] [G loss: 8.888519]\n",
      "93 [D loss: 0.257892, acc.: 96.09%] [G loss: 9.196590]\n",
      "94 [D loss: 0.215046, acc.: 98.44%] [G loss: 9.607217]\n",
      "95 [D loss: 0.217985, acc.: 99.22%] [G loss: 8.661257]\n",
      "96 [D loss: 0.245723, acc.: 96.09%] [G loss: 8.011389]\n",
      "97 [D loss: 0.313453, acc.: 94.53%] [G loss: 10.101372]\n",
      "98 [D loss: 0.245195, acc.: 97.66%] [G loss: 10.256739]\n",
      "99 [D loss: 0.283977, acc.: 96.88%] [G loss: 9.958885]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=100, data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41407, 34848)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \t[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Real \t\t[[0 0 0 ... 0 0 0]]\n",
      "prediction \t[0]\n"
     ]
    }
   ],
   "source": [
    "gen = 1\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "new_mails = np.absolute(np.round(generator.predict(noise)))\n",
    "\n",
    "idx = np.random.randint(0, X_train.shape[1], gen)\n",
    "imgs = X_train[idx]\n",
    "\n",
    "prediction = clf.predict(new_mails)\n",
    "\n",
    "print(\"Predicted \\t{}\".format(new_mails))\n",
    "print(\"Real \\t\\t{}\".format(imgs))\n",
    "print(\"prediction \\t{}\".format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv.transform([\"Hate\", \"Good\", \"Awful\", \"Best\"]).toarray()\n",
    "cv.inverse_transform(test) # See the generated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
