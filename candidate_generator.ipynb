{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM, SpatialDropout1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example from https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')\n",
    "df_dataset = pd.read_json('clothing_dataset/renttherunway_final_data.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "df_dataset['text'] = df_dataset['review_summary'] + \" \" + df_dataset['review_text']\n",
    "df_dataset['text'] = df_dataset['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "sub_dataset = df_dataset[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275359\n",
      "275359\n"
     ]
    }
   ],
   "source": [
    "all_data = df_idf['text'].append(sub_dataset['text'])\n",
    "all_rating = df_idf['rating'].append(sub_dataset['rating'])\n",
    "\n",
    "print(len(all_data))\n",
    "print(len(all_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stop_words('stopwords.txt')\n",
    "docs = all_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df = .85, stop_words=stopwords)\n",
    "wordCountVec = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'best',\n",
       " 'worst',\n",
       " 'samsung',\n",
       " 'awhile',\n",
       " 'absolute',\n",
       " 'doo',\n",
       " 'read',\n",
       " 'review',\n",
       " 'detect']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_rating\n",
    "# fixing the labels, if > 3.5 is going to be 1 which is positive, else 0\n",
    "y[:len(df_idf)] = y[:len(df_idf)].apply(lambda x: 1 if x > 3.5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y[len(df_idf):] = y[len(df_idf):].apply(lambda x: 1 if x > 5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()\n",
    "x = wordCountVec.toarray()\n",
    "\n",
    "X_train = x[len(df_idf):]\n",
    "y_train = y[len(df_idf):]\n",
    "\n",
    "X_test = x[:len(df_idf)]\n",
    "y_test = y[:len(df_idf)]\n",
    "# X_train, _test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[0].shape)\n",
    "# X_train = X_train.reshape(-1, 1, X_train[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192544, 49781)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               25488384  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 26,016,769\n",
      "Trainable params: 26,015,745\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train[0].shape)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(1024))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "192544/192544 [==============================] - 435s 2ms/step - loss: 0.0705 - acc: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x107f4af98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, verbose=1, batch_size=512 , epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 49781)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999387]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(x.shape, y.shape)\n",
    "# print(y[:len(df_idf)])\n",
    "# rat = all_rating.to_numpy()\n",
    "# for i in range(0, len(df_idf)):\n",
    "#     if y[i] == 1:\n",
    "#         print(df_idf['text'][i], y_test[i], y[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train[0:1].shape)\n",
    "model.predict(X_train[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# print(\"training with scikit\")\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a binary file\n",
    "# import pickle\n",
    "# filename = 'model2.sav'\n",
    "# pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from the binary file\n",
    "# import pickle\n",
    "# filename = 'model.sav'\n",
    "# clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cv.transform([\"Hate\", \"Good\", \"Awful\", \"Best\"]).toarray()\n",
    "# clf.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we start building the GANs, this model takes the word embedding and generate new embeddings that are similar to the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(shape):\n",
    "    img_shape = shape\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               25488384  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 25,619,969\n",
      "Trainable params: 25,619,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 49781)             51025525  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 49781)             0         \n",
      "=================================================================\n",
      "Total params: 51,715,445\n",
      "Trainable params: 51,711,861\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows = 1\n",
    "img_cols = X_train[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(self, pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.699525, acc.: 77.34%] [G loss: 1.250275]\n",
      "1 [D loss: 0.802384, acc.: 75.00%] [G loss: 1.401539]\n",
      "2 [D loss: 0.755390, acc.: 78.91%] [G loss: 1.494471]\n",
      "3 [D loss: 0.774179, acc.: 73.44%] [G loss: 1.986894]\n",
      "4 [D loss: 0.827418, acc.: 78.12%] [G loss: 2.402992]\n",
      "5 [D loss: 0.793542, acc.: 79.69%] [G loss: 2.589801]\n",
      "6 [D loss: 0.581426, acc.: 83.59%] [G loss: 2.748578]\n",
      "7 [D loss: 0.603625, acc.: 80.47%] [G loss: 2.994431]\n",
      "8 [D loss: 0.516021, acc.: 86.72%] [G loss: 3.435513]\n",
      "9 [D loss: 0.507325, acc.: 89.84%] [G loss: 3.333429]\n",
      "10 [D loss: 0.493324, acc.: 88.28%] [G loss: 3.645289]\n",
      "11 [D loss: 0.430483, acc.: 90.62%] [G loss: 3.989146]\n",
      "12 [D loss: 0.381665, acc.: 93.75%] [G loss: 4.107628]\n",
      "13 [D loss: 0.427821, acc.: 91.41%] [G loss: 4.361995]\n",
      "14 [D loss: 0.441416, acc.: 89.06%] [G loss: 4.661269]\n",
      "15 [D loss: 0.406706, acc.: 92.97%] [G loss: 4.967968]\n",
      "16 [D loss: 0.367875, acc.: 94.53%] [G loss: 5.217135]\n",
      "17 [D loss: 0.421368, acc.: 91.41%] [G loss: 5.367411]\n",
      "18 [D loss: 0.340237, acc.: 96.09%] [G loss: 5.502360]\n",
      "19 [D loss: 0.331017, acc.: 96.88%] [G loss: 5.364221]\n",
      "20 [D loss: 0.362189, acc.: 94.53%] [G loss: 5.361533]\n",
      "21 [D loss: 0.342254, acc.: 93.75%] [G loss: 5.979020]\n",
      "22 [D loss: 0.341892, acc.: 94.53%] [G loss: 6.080132]\n",
      "23 [D loss: 0.295014, acc.: 96.88%] [G loss: 6.225562]\n",
      "24 [D loss: 0.342086, acc.: 93.75%] [G loss: 6.402130]\n",
      "25 [D loss: 0.321719, acc.: 96.88%] [G loss: 6.318735]\n",
      "26 [D loss: 0.303357, acc.: 95.31%] [G loss: 6.205137]\n",
      "27 [D loss: 0.275561, acc.: 98.44%] [G loss: 6.560819]\n",
      "28 [D loss: 0.275749, acc.: 96.09%] [G loss: 6.804233]\n",
      "29 [D loss: 0.250692, acc.: 97.66%] [G loss: 7.161464]\n",
      "30 [D loss: 0.350463, acc.: 92.97%] [G loss: 7.289444]\n",
      "31 [D loss: 0.283419, acc.: 97.66%] [G loss: 7.493433]\n",
      "32 [D loss: 0.370113, acc.: 94.53%] [G loss: 7.813758]\n",
      "33 [D loss: 0.461334, acc.: 90.62%] [G loss: 8.137669]\n",
      "34 [D loss: 0.329523, acc.: 93.75%] [G loss: 8.217198]\n",
      "35 [D loss: 0.494231, acc.: 88.28%] [G loss: 8.721859]\n",
      "36 [D loss: 0.224167, acc.: 99.22%] [G loss: 9.065676]\n",
      "37 [D loss: 0.266260, acc.: 96.09%] [G loss: 8.708410]\n",
      "38 [D loss: 0.423633, acc.: 90.62%] [G loss: 8.667885]\n",
      "39 [D loss: 0.228146, acc.: 98.44%] [G loss: 9.630638]\n",
      "40 [D loss: 0.281025, acc.: 95.31%] [G loss: 9.355533]\n",
      "41 [D loss: 0.276228, acc.: 95.31%] [G loss: 10.231517]\n",
      "42 [D loss: 0.280579, acc.: 96.09%] [G loss: 11.206388]\n",
      "43 [D loss: 0.375000, acc.: 93.75%] [G loss: 10.911350]\n",
      "44 [D loss: 0.230870, acc.: 97.66%] [G loss: 11.108147]\n",
      "45 [D loss: 0.217464, acc.: 98.44%] [G loss: 11.142561]\n",
      "46 [D loss: 0.392644, acc.: 92.97%] [G loss: 11.517916]\n",
      "47 [D loss: 0.298224, acc.: 94.53%] [G loss: 11.609096]\n",
      "48 [D loss: 0.182095, acc.: 98.44%] [G loss: 11.957350]\n",
      "49 [D loss: 0.379288, acc.: 95.31%] [G loss: 11.737606]\n",
      "50 [D loss: 0.212303, acc.: 98.44%] [G loss: 11.397533]\n",
      "51 [D loss: 0.281780, acc.: 96.88%] [G loss: 10.317160]\n",
      "52 [D loss: 0.226494, acc.: 98.44%] [G loss: 11.805500]\n",
      "53 [D loss: 0.186554, acc.: 98.44%] [G loss: 11.321135]\n",
      "54 [D loss: 0.306594, acc.: 94.53%] [G loss: 12.267179]\n",
      "55 [D loss: 0.262392, acc.: 94.53%] [G loss: 12.521270]\n",
      "56 [D loss: 0.282399, acc.: 95.31%] [G loss: 12.217272]\n",
      "57 [D loss: 0.245606, acc.: 96.88%] [G loss: 12.803669]\n",
      "58 [D loss: 0.215242, acc.: 96.88%] [G loss: 13.352566]\n",
      "59 [D loss: 0.303168, acc.: 96.09%] [G loss: 13.283407]\n",
      "60 [D loss: 0.164788, acc.: 99.22%] [G loss: 13.409719]\n",
      "61 [D loss: 0.173168, acc.: 98.44%] [G loss: 12.576046]\n",
      "62 [D loss: 0.220672, acc.: 96.09%] [G loss: 12.176796]\n",
      "63 [D loss: 0.242551, acc.: 96.09%] [G loss: 13.185822]\n",
      "64 [D loss: 0.156530, acc.: 100.00%] [G loss: 13.774046]\n",
      "65 [D loss: 0.272175, acc.: 96.88%] [G loss: 12.508629]\n",
      "66 [D loss: 0.181908, acc.: 98.44%] [G loss: 13.346957]\n",
      "67 [D loss: 0.203685, acc.: 98.44%] [G loss: 13.715824]\n",
      "68 [D loss: 0.138730, acc.: 100.00%] [G loss: 13.233706]\n",
      "69 [D loss: 0.159071, acc.: 99.22%] [G loss: 13.534486]\n",
      "70 [D loss: 0.214403, acc.: 97.66%] [G loss: 14.127602]\n",
      "71 [D loss: 0.134436, acc.: 100.00%] [G loss: 13.711954]\n",
      "72 [D loss: 0.130963, acc.: 100.00%] [G loss: 13.711592]\n",
      "73 [D loss: 0.240273, acc.: 96.88%] [G loss: 13.713524]\n",
      "74 [D loss: 0.161609, acc.: 98.44%] [G loss: 13.839923]\n",
      "75 [D loss: 0.127140, acc.: 100.00%] [G loss: 14.463579]\n",
      "76 [D loss: 0.153252, acc.: 98.44%] [G loss: 14.348145]\n",
      "77 [D loss: 0.152931, acc.: 97.66%] [G loss: 14.310795]\n",
      "78 [D loss: 0.206948, acc.: 96.88%] [G loss: 14.148500]\n",
      "79 [D loss: 0.252633, acc.: 96.88%] [G loss: 14.601650]\n",
      "80 [D loss: 0.125547, acc.: 100.00%] [G loss: 15.208308]\n",
      "81 [D loss: 0.112440, acc.: 100.00%] [G loss: 14.664688]\n",
      "82 [D loss: 0.134438, acc.: 99.22%] [G loss: 14.072483]\n",
      "83 [D loss: 0.345271, acc.: 94.53%] [G loss: 14.820339]\n",
      "84 [D loss: 0.133815, acc.: 98.44%] [G loss: 15.140263]\n",
      "85 [D loss: 0.120082, acc.: 100.00%] [G loss: 14.554541]\n",
      "86 [D loss: 0.130181, acc.: 100.00%] [G loss: 15.101406]\n",
      "87 [D loss: 0.124940, acc.: 100.00%] [G loss: 15.109449]\n",
      "88 [D loss: 0.116475, acc.: 99.22%] [G loss: 14.867397]\n",
      "89 [D loss: 0.141298, acc.: 98.44%] [G loss: 14.863567]\n",
      "90 [D loss: 0.200898, acc.: 97.66%] [G loss: 14.966154]\n",
      "91 [D loss: 0.116828, acc.: 98.44%] [G loss: 15.288765]\n",
      "92 [D loss: 0.101637, acc.: 100.00%] [G loss: 15.063131]\n",
      "93 [D loss: 0.123850, acc.: 97.66%] [G loss: 15.118847]\n",
      "94 [D loss: 0.104771, acc.: 99.22%] [G loss: 14.947108]\n",
      "95 [D loss: 0.097953, acc.: 100.00%] [G loss: 15.284419]\n",
      "96 [D loss: 0.213017, acc.: 97.66%] [G loss: 15.386079]\n",
      "97 [D loss: 0.173588, acc.: 98.44%] [G loss: 15.186529]\n",
      "98 [D loss: 0.102002, acc.: 99.22%] [G loss: 15.573436]\n",
      "99 [D loss: 0.091202, acc.: 100.00%] [G loss: 15.716404]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=100, data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 49781)\n",
      "[[0.75550365]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1].shape)\n",
    "print(discriminator.predict(X_train[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \t[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Real \t\t[[0 0 0 ... 0 0 0]]\n",
      "prediction \t[[1.]]\n",
      "Real? \t\t[[0.98631835]]\n"
     ]
    }
   ],
   "source": [
    "gen = 1\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "new_mails = np.absolute(np.round(generator.predict(noise)))\n",
    "\n",
    "idx = np.random.randint(0, X_train.shape[1], gen)\n",
    "imgs = X_train[idx]\n",
    "\n",
    "prediction = model.predict(new_mails)\n",
    "\n",
    "print(\"Predicted \\t{}\".format(new_mails))\n",
    "print(\"Real \\t\\t{}\".format(imgs))\n",
    "print(\"prediction \\t{}\".format(prediction))\n",
    "print(\"Real? \\t\\t{}\".format(discriminator.predict(new_mails[0:1]))) # add a normalization somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 49781)\n"
     ]
    }
   ],
   "source": [
    "test = cv.transform([\"Hate\", \"Good\", \"Awful\", \"Best\"]).toarray()\n",
    "cv.inverse_transform(test) # See the generated words\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
