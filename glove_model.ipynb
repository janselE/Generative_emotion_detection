{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM, Embedding, SpatialDropout1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')\n",
    "df_dataset = pd.read_json('clothing_dataset/renttherunway_final_data.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "df_dataset['text'] = df_dataset['review_summary'] + \" \" + df_dataset['review_text']\n",
    "df_dataset['text'] = df_dataset['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "sub_dataset = df_dataset[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df_idf['text'].append(sub_dataset['text'])\n",
    "all_rating = df_idf['rating'].append(sub_dataset['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_rating\n",
    "y[:len(df_idf)] = y[:len(df_idf)].apply(lambda x: 1 if x > 3.5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y[len(df_idf):] = y[len(df_idf):].apply(lambda x: 1 if x > 5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to use both datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.1, random_state=37)\n",
    "# X_train = df_dataset['text']\n",
    "# y_train = y[len(df_idf):]\n",
    "\n",
    "# X_test = df_idf['text'].to_numpy()\n",
    "# y_test = y[:len(df_idf)]\n",
    "\n",
    "# print('# Train data samples:', X_train.shape)\n",
    "# print('# Test data samples:', X_test.shape)\n",
    "\n",
    "# print('Sample train', X_train[0])\n",
    "# print('\\nSample test', X_test[0])\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# assert X_train.shape[0] == y_train.shape[0]\n",
    "# assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 5260\n",
    "GLOVE_DIM = 300\n",
    "NB_WORDS = 49781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(all_data)\n",
    "\n",
    "all_data_seq = tk.texts_to_sequences(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = all_data_seq[len(df_idf):]\n",
    "X_test_seq = all_data_seq[:len(df_idf)]\n",
    "\n",
    "y_train = y[len(df_idf):]\n",
    "y_test = y[:len(df_idf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    275359.000000\n",
      "mean         63.517557\n",
      "std          69.966379\n",
      "min           1.000000\n",
      "25%          26.000000\n",
      "50%          50.000000\n",
      "75%          82.000000\n",
      "max        5260.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = all_data.apply(lambda x: len(x.split(' ')))\n",
    "print(seq_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...    84   204   520]\n",
      " [    0     0     0 ...     1   792 23123]\n",
      " [    0     0     0 ...  1027    16     5]\n",
      " ...\n",
      " [    0     0     0 ...     6    26   525]\n",
      " [    0     0     0 ...   177     1   982]\n",
      " [    0     0     0 ...    43    20   708]]\n",
      "[1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[1316 2351 3203 3428  438 4732 1238 3755 4516 3807 1202 2107 2453  211\n",
      " 5116  491 2304 4664 2475  203 2261 1930  933 2988 1155  286  881  818\n",
      " 2526 3964 4557 1685  874 3797 4419  323  410 2009 4719 3970 3222 1220\n",
      " 2900 5233 3726 2635 1843 5048 1681 5110 1290  925 5257 1959 1338 3823\n",
      " 2905 3582 2938  349  209 3179 2263 3545]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1316    perfect for all my daily activities i loved th...\n",
       "2351    not a good emergency only phone i activated th...\n",
       "3203    flawless coming from another smartphone that w...\n",
       "3428    i actually never imagined a phone could be thi...\n",
       "438     excellent service responses excellent service ...\n",
       "4732    absolutely hate it this phone could not be wor...\n",
       "1238    so far so good have only had it for days but s...\n",
       "3755    it happens every day now and it s annoying bec...\n",
       "4516    five stars i already wrote a review they are g...\n",
       "3807    think twice before buying there are definitely...\n",
       "1202    had exactly what i needed when i needed it tha...\n",
       "2107    no external memory can t save items this was m...\n",
       "2453    i like this phone and i give it four stars out...\n",
       "211     it s ok except for the browser button seems li...\n",
       "5116    excellent bought the phone for my daughter rec...\n",
       "491     make sure your aware that it can only be used ...\n",
       "2304              i love it just what i needed i love it \n",
       "4664    one star it is too sad to sale a product that ...\n",
       "2475    handles the basics replacement for the same ty...\n",
       "203     how to fix browser problem as some of you have...\n",
       "2261    five stars snow drift samsung a gophone it wor...\n",
       "1930    one star first week having it stayed having is...\n",
       "933     good phone price and features make it a winnin...\n",
       "2988    don t walk run away from this model update may...\n",
       "1155    great phone and os not so great app selection ...\n",
       "286                           one star would not turn on \n",
       "881                             five stars so far so good\n",
       "818     it was a terrible experience and i still have ...\n",
       "2526       five stars it was great and price even better \n",
       "3964    i order international unlocked model and what ...\n",
       "                              ...                        \n",
       "4419    five stars love the phone refurbished but like...\n",
       "323     an obvious rip off phones lasted only a few da...\n",
       "410                            five stars i love my phone\n",
       "2009    i hate this phone i got this phone to replace ...\n",
       "4719    best phone i ve ever had beautiful design good...\n",
       "3970    very disgusted with the quality of a nokia pro...\n",
       "3222    one great phone beats the iphone by a mile i l...\n",
       "1220    doesn t fully function i received this i last ...\n",
       "2900    great great came as good as description even b...\n",
       "5233    terrible phone the phone was working amazing f...\n",
       "3726    one year using this phone and it s good enough...\n",
       "2635            two stars no minutes just phone a rip off\n",
       "1843                         five stars good basic phone \n",
       "5048                                  one star fine phone\n",
       "1681    cell phone purchase this is a very simple phon...\n",
       "5110                                     one star no good\n",
       "1290    great phone it s a great device faster and lon...\n",
       "925     received as expected is working with net used ...\n",
       "5257    excellent phone you do not need the latest ang...\n",
       "1959    convoy u great phone hardware wise rugged and ...\n",
       "1338                                          five stars \n",
       "3823           three stars it came with the wrong charger\n",
       "2905                     five stars exactly what i wanted\n",
       "3582    pretty good well the shipping took longer than...\n",
       "2938    five stars very good nice condicion fast shipping\n",
       "349     do not order cell phones from this seller the ...\n",
       "209     excellent phone exactly the kind of mobile pho...\n",
       "3179    horrid this phone was slow and glitchy i was p...\n",
       "2263    gophone although i am a smart phone kind of pe...\n",
       "3545    not unlocked only works with at t network this...\n",
       "Name: text, Length: 64, dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # This is to split the X_test_seq_trunc in two normalize dataset for GANs\n",
    "# print(X_test_seq_trunc.shape)\n",
    "\n",
    "# positives = X_test_seq_trunc[0:1]\n",
    "# negatives = X_test_seq_trunc[0:1]\n",
    "\n",
    "# normalized_data = X_test_seq_trunc[0:1]\n",
    "\n",
    "# print(positives.shape)\n",
    "\n",
    "# for i in range(0, y_test.shape[0]):\n",
    "#     if y_test[i] == 1:\n",
    "#         positives = np.append(positives, X_test_seq_trunc[i:(i+1)], axis=0)\n",
    "#     else:\n",
    "#         negatives = np.append(negatives, X_test_seq_trunc[i:(i+1)], axis=0)\n",
    "        \n",
    "# iter_ = min(positives.shape[0], negatives.shape[0])\n",
    "# for i in range(0, iter_):\n",
    "#     normalized_data = np.append(normalized_data, positives[i:(i+1)], axis=0)\n",
    "#     normalized_data = np.append(normalized_data, negatives[i:(i+1)], axis=0)\n",
    "    \n",
    "        \n",
    "# print(positives.shape, negatives.shape, normalized_data.shape)\n",
    "\n",
    "# for i in range(0, 5000):\n",
    "#     if(positives[0:1, i] != 0):\n",
    "#         print(positives[0:1, i], X_test_seq_trunc[0:1, i], i)\n",
    "\n",
    "half_batch = int(128/2)\n",
    "\n",
    "idx = np.random.randint(0, X_test_seq_trunc.shape[1], half_batch)\n",
    "samp = np.array([])\n",
    "count = 0\n",
    "for i in range(0, half_batch):\n",
    "    if count != half_batch/2:\n",
    "        idx = np.random.randint(0, X_test_seq_trunc.shape[1], 1)\n",
    "        while(y_test[idx] == 0):\n",
    "            idx = np.random.randint(0, X_test_seq_trunc.shape[1], 1)\n",
    "        samp = np.append(samp, idx, axis=0)\n",
    "        \n",
    "        while(y_test[idx] == 1):\n",
    "            idx = np.random.randint(0, X_test_seq_trunc.shape[1], 1)\n",
    "        samp = np.append(samp, idx, axis=0)\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "\n",
    "# # samp = X_test_seq_trunc[idx]\n",
    "samp = samp.astype(int)\n",
    "\n",
    "print(X_test_seq_trunc[samp])\n",
    "print(y_test[samp])\n",
    "print(samp)\n",
    "df_idf['text'][samp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0 ...  61  14 316]\n",
      "(192544, 5260)\n",
      "(82815, 5260)\n",
      "(192544,)\n",
      "(82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq_trunc[10])  # Example of padded sequence\n",
    "print(X_train_seq_trunc.shape)\n",
    "print(X_test_seq_trunc.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (19255, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "\n",
    "print('Shape of validation set:', X_valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This read the embeddings\n",
    "# glove_file = 'glove.42B.' + str(GLOVE_DIM) + 'd.txt'\n",
    "# emb_dict = {}\n",
    "# glove = open(glove_file)\n",
    "# for line in glove:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     vector = np.asarray(values[1:], dtype='float32')\n",
    "#     emb_dict[word] = vector\n",
    "# glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word car in the dictionary\n",
      "Found the word nice in the dictionary\n",
      "Found the word flight in the dictionary\n",
      "Found the word luggage in the dictionary\n"
     ]
    }
   ],
   "source": [
    "airline_words = ['car', 'nice', 'flight', 'luggage']\n",
    "for w in airline_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary'.format(w))\n",
    "# print(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a matrix that represent words and it corresponding emdg\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model.add(Flatten())\n",
    "glove_model.add(Dense(1, activation='sigmoid'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 173289 samples, validate on 19255 samples\n",
      "Epoch 1/5\n",
      "173289/173289 [==============================] - 637s 4ms/step - loss: 0.1121 - acc: 0.9751 - val_loss: 0.1034 - val_acc: 0.9747\n",
      "Epoch 2/5\n",
      "173289/173289 [==============================] - 680s 4ms/step - loss: 0.0806 - acc: 0.9791 - val_loss: 0.1231 - val_acc: 0.9727\n",
      "Epoch 3/5\n",
      "173289/173289 [==============================] - 628s 4ms/step - loss: 0.0675 - acc: 0.9819 - val_loss: 0.1317 - val_acc: 0.9774\n",
      "Epoch 4/5\n",
      "173289/173289 [==============================] - 658s 4ms/step - loss: 0.0589 - acc: 0.9838 - val_loss: 0.1307 - val_acc: 0.9760\n",
      "Epoch 5/5\n",
      "173289/173289 [==============================] - 585s 3ms/step - loss: 0.0524 - acc: 0.9851 - val_loss: 0.1415 - val_acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "history = glove_model.fit(X_train_emb\n",
    "                       , y_train_emb\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb, y_valid_emb)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999]]\n",
      "(5260,)\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.predict(X_train_emb[0:1]))\n",
    "print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we start building the GANs, this model takes the word embedding and generate new embeddings that are similar to the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(shape):\n",
    "    img_shape = shape\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def results(pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               2693632   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,825,217\n",
      "Trainable params: 2,825,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5260)              5391500   \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 5260)              0         \n",
      "=================================================================\n",
      "Total params: 6,081,420\n",
      "Trainable params: 6,077,836\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows = 1\n",
    "img_cols = X_train_emb[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "#             idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "#             imgs = X_train[idx]\n",
    "\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            samp = np.array([])\n",
    "            count = 0\n",
    "            for i in range(0, half_batch):\n",
    "                if count != half_batch/2:\n",
    "                    idx = np.random.randint(0, X_train.shape[1], 1)\n",
    "                    while(y_test[idx] == 0):\n",
    "                        idx = np.random.randint(0, X_train.shape[1], 1)\n",
    "                    samp = np.append(samp, idx, axis=0)\n",
    "\n",
    "                    while(y_test[idx] == 1):\n",
    "                        idx = np.random.randint(0, X_train.shape[1], 1)\n",
    "                    samp = np.append(samp, idx, axis=0)\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "            # samp = X_test_seq_trunc[idx]\n",
    "            samp = samp.astype(int)\n",
    "            imgs = X_train[samp]\n",
    "#             print(np.sum(y_test[samp]))\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = np.round(generator.predict(noise))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.810067, acc.: 36.72%] [G loss: 0.641288]\n",
      "1 [D loss: 0.822508, acc.: 55.47%] [G loss: 0.539981]\n",
      "2 [D loss: 0.704065, acc.: 53.91%] [G loss: 0.440695]\n",
      "3 [D loss: 0.705125, acc.: 54.69%] [G loss: 0.373934]\n",
      "4 [D loss: 0.812922, acc.: 51.56%] [G loss: 0.345146]\n",
      "5 [D loss: 1.087976, acc.: 50.00%] [G loss: 0.308388]\n",
      "6 [D loss: 0.958924, acc.: 50.78%] [G loss: 0.310668]\n",
      "7 [D loss: 1.603963, acc.: 47.66%] [G loss: 0.300471]\n",
      "8 [D loss: 1.038210, acc.: 49.22%] [G loss: 0.290471]\n",
      "9 [D loss: 1.533416, acc.: 46.88%] [G loss: 0.290719]\n",
      "10 [D loss: 1.701197, acc.: 46.88%] [G loss: 0.221435]\n",
      "11 [D loss: 1.214371, acc.: 50.78%] [G loss: 0.238506]\n",
      "12 [D loss: 1.186348, acc.: 50.78%] [G loss: 0.224354]\n",
      "13 [D loss: 1.349773, acc.: 49.22%] [G loss: 0.267747]\n",
      "14 [D loss: 0.933843, acc.: 50.78%] [G loss: 0.281795]\n",
      "15 [D loss: 0.869056, acc.: 56.25%] [G loss: 0.306148]\n",
      "16 [D loss: 1.054489, acc.: 53.12%] [G loss: 0.311180]\n",
      "17 [D loss: 1.115238, acc.: 53.91%] [G loss: 0.314307]\n",
      "18 [D loss: 0.981008, acc.: 52.34%] [G loss: 0.329690]\n",
      "19 [D loss: 0.982672, acc.: 53.12%] [G loss: 0.313394]\n",
      "20 [D loss: 1.427156, acc.: 48.44%] [G loss: 0.340575]\n",
      "21 [D loss: 1.660590, acc.: 47.66%] [G loss: 0.209658]\n",
      "22 [D loss: 1.090575, acc.: 51.56%] [G loss: 0.228700]\n",
      "23 [D loss: 0.933478, acc.: 52.34%] [G loss: 0.250460]\n",
      "24 [D loss: 0.941246, acc.: 53.12%] [G loss: 0.304868]\n",
      "25 [D loss: 0.961755, acc.: 51.56%] [G loss: 0.305491]\n",
      "26 [D loss: 0.990225, acc.: 53.91%] [G loss: 0.328536]\n",
      "27 [D loss: 1.001027, acc.: 57.03%] [G loss: 0.342973]\n",
      "28 [D loss: 0.802637, acc.: 53.91%] [G loss: 0.371801]\n",
      "29 [D loss: 1.015235, acc.: 60.16%] [G loss: 0.409123]\n",
      "30 [D loss: 0.775030, acc.: 61.72%] [G loss: 0.409165]\n",
      "31 [D loss: 0.878654, acc.: 57.81%] [G loss: 0.349401]\n",
      "32 [D loss: 0.922274, acc.: 54.69%] [G loss: 0.348880]\n",
      "33 [D loss: 1.000246, acc.: 53.91%] [G loss: 0.355164]\n",
      "34 [D loss: 1.108411, acc.: 53.12%] [G loss: 0.281234]\n",
      "35 [D loss: 0.986758, acc.: 53.91%] [G loss: 0.306504]\n",
      "36 [D loss: 0.953700, acc.: 53.12%] [G loss: 0.317789]\n",
      "37 [D loss: 0.783348, acc.: 60.94%] [G loss: 0.361250]\n",
      "38 [D loss: 0.956353, acc.: 55.47%] [G loss: 0.367289]\n",
      "39 [D loss: 0.997107, acc.: 55.47%] [G loss: 0.371809]\n",
      "40 [D loss: 0.745352, acc.: 60.94%] [G loss: 0.368139]\n",
      "41 [D loss: 0.677337, acc.: 63.28%] [G loss: 0.454694]\n",
      "42 [D loss: 0.786424, acc.: 61.72%] [G loss: 0.433055]\n",
      "43 [D loss: 0.723245, acc.: 59.38%] [G loss: 0.472537]\n",
      "44 [D loss: 0.833084, acc.: 57.81%] [G loss: 0.425849]\n",
      "45 [D loss: 0.938996, acc.: 56.25%] [G loss: 0.456299]\n",
      "46 [D loss: 0.908929, acc.: 61.72%] [G loss: 0.424929]\n",
      "47 [D loss: 0.658237, acc.: 63.28%] [G loss: 0.472454]\n",
      "48 [D loss: 0.802799, acc.: 57.03%] [G loss: 0.464743]\n",
      "49 [D loss: 0.808735, acc.: 61.72%] [G loss: 0.472193]\n",
      "50 [D loss: 0.656040, acc.: 62.50%] [G loss: 0.465824]\n",
      "51 [D loss: 0.779333, acc.: 63.28%] [G loss: 0.502574]\n",
      "52 [D loss: 0.907231, acc.: 60.94%] [G loss: 0.484860]\n",
      "53 [D loss: 0.642528, acc.: 62.50%] [G loss: 0.534609]\n",
      "54 [D loss: 0.723254, acc.: 63.28%] [G loss: 0.499878]\n",
      "55 [D loss: 0.918359, acc.: 64.84%] [G loss: 0.516809]\n",
      "56 [D loss: 1.758085, acc.: 50.00%] [G loss: 0.388121]\n",
      "57 [D loss: 1.260460, acc.: 56.25%] [G loss: 0.338077]\n",
      "58 [D loss: 0.856883, acc.: 57.81%] [G loss: 0.327245]\n",
      "59 [D loss: 0.864610, acc.: 57.03%] [G loss: 0.369645]\n",
      "60 [D loss: 0.752203, acc.: 57.03%] [G loss: 0.408099]\n",
      "61 [D loss: 0.782518, acc.: 57.03%] [G loss: 0.504625]\n",
      "62 [D loss: 0.668835, acc.: 61.72%] [G loss: 0.499738]\n",
      "63 [D loss: 0.812024, acc.: 60.94%] [G loss: 0.485113]\n",
      "64 [D loss: 0.726500, acc.: 60.94%] [G loss: 0.474691]\n",
      "65 [D loss: 0.560619, acc.: 70.31%] [G loss: 0.502753]\n",
      "66 [D loss: 0.727395, acc.: 63.28%] [G loss: 0.524131]\n",
      "67 [D loss: 0.647270, acc.: 61.72%] [G loss: 0.516040]\n",
      "68 [D loss: 0.664848, acc.: 65.62%] [G loss: 0.508257]\n",
      "69 [D loss: 0.609460, acc.: 64.06%] [G loss: 0.484674]\n",
      "70 [D loss: 0.559138, acc.: 60.94%] [G loss: 0.551808]\n",
      "71 [D loss: 0.712882, acc.: 61.72%] [G loss: 0.563919]\n",
      "72 [D loss: 0.781362, acc.: 69.53%] [G loss: 0.554133]\n",
      "73 [D loss: 0.771281, acc.: 63.28%] [G loss: 0.622692]\n",
      "74 [D loss: 0.795789, acc.: 66.41%] [G loss: 0.589161]\n",
      "75 [D loss: 0.782434, acc.: 67.97%] [G loss: 0.643384]\n",
      "76 [D loss: 0.651741, acc.: 71.09%] [G loss: 0.625396]\n",
      "77 [D loss: 0.696268, acc.: 69.53%] [G loss: 0.641988]\n",
      "78 [D loss: 0.588379, acc.: 71.88%] [G loss: 0.701950]\n",
      "79 [D loss: 0.770210, acc.: 70.31%] [G loss: 0.623922]\n",
      "80 [D loss: 0.607141, acc.: 71.88%] [G loss: 0.661853]\n",
      "81 [D loss: 0.717027, acc.: 65.62%] [G loss: 0.677283]\n",
      "82 [D loss: 0.589288, acc.: 66.41%] [G loss: 0.662940]\n",
      "83 [D loss: 0.654695, acc.: 67.97%] [G loss: 0.685374]\n",
      "84 [D loss: 0.758142, acc.: 69.53%] [G loss: 0.637605]\n",
      "85 [D loss: 0.988774, acc.: 67.19%] [G loss: 0.572933]\n",
      "86 [D loss: 0.595137, acc.: 65.62%] [G loss: 0.539339]\n",
      "87 [D loss: 0.648281, acc.: 70.31%] [G loss: 0.563287]\n",
      "88 [D loss: 0.612124, acc.: 72.66%] [G loss: 0.591936]\n",
      "89 [D loss: 0.588955, acc.: 73.44%] [G loss: 0.602221]\n",
      "90 [D loss: 0.645963, acc.: 67.19%] [G loss: 0.649583]\n",
      "91 [D loss: 0.550049, acc.: 70.31%] [G loss: 0.681890]\n",
      "92 [D loss: 0.543018, acc.: 74.22%] [G loss: 0.713036]\n",
      "93 [D loss: 0.618665, acc.: 70.31%] [G loss: 0.751125]\n",
      "94 [D loss: 0.636654, acc.: 67.97%] [G loss: 0.683668]\n",
      "95 [D loss: 0.826772, acc.: 67.19%] [G loss: 0.663032]\n",
      "96 [D loss: 0.653174, acc.: 78.12%] [G loss: 0.757150]\n",
      "97 [D loss: 0.686556, acc.: 78.91%] [G loss: 0.759468]\n",
      "98 [D loss: 0.781185, acc.: 71.09%] [G loss: 0.700941]\n",
      "99 [D loss: 0.767213, acc.: 75.78%] [G loss: 0.673757]\n",
      "100 [D loss: 0.790002, acc.: 77.34%] [G loss: 0.727735]\n",
      "101 [D loss: 0.671586, acc.: 74.22%] [G loss: 0.775038]\n",
      "102 [D loss: 0.477494, acc.: 71.09%] [G loss: 0.744109]\n",
      "103 [D loss: 0.655271, acc.: 76.56%] [G loss: 0.763805]\n",
      "104 [D loss: 0.998750, acc.: 65.62%] [G loss: 0.500241]\n",
      "105 [D loss: 0.714182, acc.: 63.28%] [G loss: 0.510495]\n",
      "106 [D loss: 0.606318, acc.: 70.31%] [G loss: 0.693647]\n",
      "107 [D loss: 0.559796, acc.: 71.09%] [G loss: 0.794879]\n",
      "108 [D loss: 0.745869, acc.: 68.75%] [G loss: 0.767114]\n",
      "109 [D loss: 0.424870, acc.: 76.56%] [G loss: 0.771095]\n",
      "110 [D loss: 0.712623, acc.: 70.31%] [G loss: 0.778549]\n",
      "111 [D loss: 0.545867, acc.: 76.56%] [G loss: 0.739378]\n",
      "112 [D loss: 0.464574, acc.: 74.22%] [G loss: 0.766067]\n",
      "113 [D loss: 0.465556, acc.: 73.44%] [G loss: 0.778889]\n",
      "114 [D loss: 0.630791, acc.: 75.78%] [G loss: 0.726738]\n",
      "115 [D loss: 0.806576, acc.: 68.75%] [G loss: 0.762595]\n",
      "116 [D loss: 0.672963, acc.: 71.09%] [G loss: 0.802953]\n",
      "117 [D loss: 0.776996, acc.: 71.09%] [G loss: 0.838095]\n",
      "118 [D loss: 0.967801, acc.: 72.66%] [G loss: 0.820552]\n",
      "119 [D loss: 1.272944, acc.: 67.97%] [G loss: 0.880343]\n",
      "120 [D loss: 1.624023, acc.: 66.41%] [G loss: 0.894584]\n",
      "121 [D loss: 0.564912, acc.: 73.44%] [G loss: 0.882853]\n",
      "122 [D loss: 1.805236, acc.: 61.72%] [G loss: 0.908013]\n",
      "123 [D loss: 0.745520, acc.: 78.12%] [G loss: 0.914996]\n",
      "124 [D loss: 0.989874, acc.: 67.97%] [G loss: 0.866731]\n",
      "125 [D loss: 0.658717, acc.: 72.66%] [G loss: 0.881453]\n",
      "126 [D loss: 0.994865, acc.: 69.53%] [G loss: 0.873976]\n",
      "127 [D loss: 1.262298, acc.: 67.97%] [G loss: 0.655521]\n",
      "128 [D loss: 1.183029, acc.: 63.28%] [G loss: 0.652619]\n",
      "129 [D loss: 0.951595, acc.: 68.75%] [G loss: 0.706722]\n",
      "130 [D loss: 1.263514, acc.: 68.75%] [G loss: 0.782272]\n",
      "131 [D loss: 1.718261, acc.: 64.06%] [G loss: 0.655353]\n",
      "132 [D loss: 1.006429, acc.: 64.06%] [G loss: 0.708086]\n",
      "133 [D loss: 1.111385, acc.: 67.19%] [G loss: 0.841685]\n",
      "134 [D loss: 1.111657, acc.: 71.88%] [G loss: 0.806621]\n",
      "135 [D loss: 0.936250, acc.: 75.00%] [G loss: 0.822543]\n",
      "136 [D loss: 1.243461, acc.: 65.62%] [G loss: 0.807329]\n",
      "137 [D loss: 1.306399, acc.: 64.06%] [G loss: 0.754414]\n",
      "138 [D loss: 1.260435, acc.: 68.75%] [G loss: 0.817576]\n",
      "139 [D loss: 1.178020, acc.: 66.41%] [G loss: 0.777626]\n",
      "140 [D loss: 1.285573, acc.: 75.78%] [G loss: 0.856702]\n",
      "141 [D loss: 1.450465, acc.: 66.41%] [G loss: 0.719451]\n",
      "142 [D loss: 1.903772, acc.: 65.62%] [G loss: 0.759464]\n",
      "143 [D loss: 1.896099, acc.: 57.03%] [G loss: 0.612842]\n",
      "144 [D loss: 0.813011, acc.: 64.06%] [G loss: 0.742171]\n",
      "145 [D loss: 0.848020, acc.: 65.62%] [G loss: 0.834323]\n",
      "146 [D loss: 1.306804, acc.: 66.41%] [G loss: 0.874909]\n",
      "147 [D loss: 0.771803, acc.: 67.97%] [G loss: 0.900457]\n",
      "148 [D loss: 1.593958, acc.: 62.50%] [G loss: 0.990271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 1.709611, acc.: 70.31%] [G loss: 0.947220]\n",
      "150 [D loss: 0.672634, acc.: 71.09%] [G loss: 0.922792]\n",
      "151 [D loss: 1.439794, acc.: 69.53%] [G loss: 0.871985]\n",
      "152 [D loss: 1.623712, acc.: 56.25%] [G loss: 0.951732]\n",
      "153 [D loss: 1.828401, acc.: 64.84%] [G loss: 0.968485]\n",
      "154 [D loss: 2.022963, acc.: 64.84%] [G loss: 0.956028]\n",
      "155 [D loss: 1.515982, acc.: 64.84%] [G loss: 0.916122]\n",
      "156 [D loss: 1.453091, acc.: 71.88%] [G loss: 0.906584]\n",
      "157 [D loss: 2.502814, acc.: 57.81%] [G loss: 0.657890]\n",
      "158 [D loss: 1.270637, acc.: 59.38%] [G loss: 0.802096]\n",
      "159 [D loss: 1.695183, acc.: 62.50%] [G loss: 0.919700]\n",
      "160 [D loss: 1.172889, acc.: 67.19%] [G loss: 0.973992]\n",
      "161 [D loss: 1.459300, acc.: 70.31%] [G loss: 0.979617]\n",
      "162 [D loss: 1.608061, acc.: 64.06%] [G loss: 1.025523]\n",
      "163 [D loss: 1.777737, acc.: 64.84%] [G loss: 0.963184]\n",
      "164 [D loss: 1.943427, acc.: 65.62%] [G loss: 0.967020]\n",
      "165 [D loss: 1.523937, acc.: 59.38%] [G loss: 0.947238]\n",
      "166 [D loss: 1.295017, acc.: 63.28%] [G loss: 0.999987]\n",
      "167 [D loss: 2.344001, acc.: 60.94%] [G loss: 0.991139]\n",
      "168 [D loss: 2.285305, acc.: 58.59%] [G loss: 0.799345]\n",
      "169 [D loss: 1.724372, acc.: 66.41%] [G loss: 0.827650]\n",
      "170 [D loss: 1.249963, acc.: 57.81%] [G loss: 0.960913]\n",
      "171 [D loss: 1.773853, acc.: 64.06%] [G loss: 1.117813]\n",
      "172 [D loss: 1.519615, acc.: 68.75%] [G loss: 1.096737]\n",
      "173 [D loss: 1.329986, acc.: 69.53%] [G loss: 1.107634]\n",
      "174 [D loss: 1.522508, acc.: 63.28%] [G loss: 1.082551]\n",
      "175 [D loss: 1.890366, acc.: 60.94%] [G loss: 1.075002]\n",
      "176 [D loss: 1.974438, acc.: 64.06%] [G loss: 1.091833]\n",
      "177 [D loss: 2.013816, acc.: 63.28%] [G loss: 1.045130]\n",
      "178 [D loss: 1.592956, acc.: 57.03%] [G loss: 1.181609]\n",
      "179 [D loss: 1.884283, acc.: 66.41%] [G loss: 1.207166]\n",
      "180 [D loss: 1.084404, acc.: 68.75%] [G loss: 1.212401]\n",
      "181 [D loss: 2.447597, acc.: 65.62%] [G loss: 1.257524]\n",
      "182 [D loss: 2.018054, acc.: 60.94%] [G loss: 1.333411]\n",
      "183 [D loss: 2.171252, acc.: 57.03%] [G loss: 0.825734]\n",
      "184 [D loss: 1.509205, acc.: 50.78%] [G loss: 0.810617]\n",
      "185 [D loss: 0.937459, acc.: 61.72%] [G loss: 1.158497]\n",
      "186 [D loss: 0.536893, acc.: 69.53%] [G loss: 1.297141]\n",
      "187 [D loss: 0.469023, acc.: 74.22%] [G loss: 1.282345]\n",
      "188 [D loss: 0.385774, acc.: 76.56%] [G loss: 1.147545]\n",
      "189 [D loss: 1.240232, acc.: 66.41%] [G loss: 1.095351]\n",
      "190 [D loss: 1.254405, acc.: 66.41%] [G loss: 1.108036]\n",
      "191 [D loss: 0.921942, acc.: 64.06%] [G loss: 1.213051]\n",
      "192 [D loss: 0.921983, acc.: 70.31%] [G loss: 1.160434]\n",
      "193 [D loss: 0.840939, acc.: 67.19%] [G loss: 1.194510]\n",
      "194 [D loss: 0.969890, acc.: 68.75%] [G loss: 1.205929]\n",
      "195 [D loss: 0.767680, acc.: 76.56%] [G loss: 1.201200]\n",
      "196 [D loss: 0.702998, acc.: 73.44%] [G loss: 1.108241]\n",
      "197 [D loss: 1.135917, acc.: 66.41%] [G loss: 1.049568]\n",
      "198 [D loss: 1.372664, acc.: 64.06%] [G loss: 1.202284]\n",
      "199 [D loss: 1.583657, acc.: 67.19%] [G loss: 1.204392]\n",
      "200 [D loss: 1.025403, acc.: 70.31%] [G loss: 1.248470]\n",
      "201 [D loss: 1.797551, acc.: 67.19%] [G loss: 1.132529]\n",
      "202 [D loss: 1.544415, acc.: 69.53%] [G loss: 1.133344]\n",
      "203 [D loss: 1.244296, acc.: 61.72%] [G loss: 1.175740]\n",
      "204 [D loss: 1.657985, acc.: 61.72%] [G loss: 1.380652]\n",
      "205 [D loss: 1.374337, acc.: 78.12%] [G loss: 1.398305]\n",
      "206 [D loss: 1.299016, acc.: 67.19%] [G loss: 1.433814]\n",
      "207 [D loss: 2.546635, acc.: 47.66%] [G loss: 0.746578]\n",
      "208 [D loss: 0.833258, acc.: 64.84%] [G loss: 0.997430]\n",
      "209 [D loss: 0.720954, acc.: 65.62%] [G loss: 1.388031]\n",
      "210 [D loss: 0.465848, acc.: 83.59%] [G loss: 1.537744]\n",
      "211 [D loss: 0.547855, acc.: 70.31%] [G loss: 1.448785]\n",
      "212 [D loss: 0.835577, acc.: 72.66%] [G loss: 1.311322]\n",
      "213 [D loss: 0.782552, acc.: 70.31%] [G loss: 1.358716]\n",
      "214 [D loss: 0.478965, acc.: 75.78%] [G loss: 1.110599]\n",
      "215 [D loss: 0.922542, acc.: 66.41%] [G loss: 1.072873]\n",
      "216 [D loss: 0.753580, acc.: 64.84%] [G loss: 1.225794]\n",
      "217 [D loss: 0.707395, acc.: 73.44%] [G loss: 1.297747]\n",
      "218 [D loss: 0.546418, acc.: 71.09%] [G loss: 1.304383]\n",
      "219 [D loss: 0.636525, acc.: 71.88%] [G loss: 1.268215]\n",
      "220 [D loss: 1.243235, acc.: 68.75%] [G loss: 1.236825]\n",
      "221 [D loss: 0.833996, acc.: 72.66%] [G loss: 1.305675]\n",
      "222 [D loss: 0.633082, acc.: 77.34%] [G loss: 1.346636]\n",
      "223 [D loss: 1.230769, acc.: 71.09%] [G loss: 1.253279]\n",
      "224 [D loss: 1.189873, acc.: 62.50%] [G loss: 1.318256]\n",
      "225 [D loss: 1.036681, acc.: 70.31%] [G loss: 1.291492]\n",
      "226 [D loss: 1.074423, acc.: 68.75%] [G loss: 1.272267]\n",
      "227 [D loss: 0.860206, acc.: 71.88%] [G loss: 1.340451]\n",
      "228 [D loss: 1.009357, acc.: 72.66%] [G loss: 1.276477]\n",
      "229 [D loss: 1.093810, acc.: 69.53%] [G loss: 1.199206]\n",
      "230 [D loss: 1.471936, acc.: 67.19%] [G loss: 1.062847]\n",
      "231 [D loss: 1.374236, acc.: 64.84%] [G loss: 1.143024]\n",
      "232 [D loss: 1.157728, acc.: 72.66%] [G loss: 1.251774]\n",
      "233 [D loss: 0.892198, acc.: 74.22%] [G loss: 1.384334]\n",
      "234 [D loss: 0.812253, acc.: 81.25%] [G loss: 1.335675]\n",
      "235 [D loss: 1.435055, acc.: 67.19%] [G loss: 1.188774]\n",
      "236 [D loss: 0.759429, acc.: 72.66%] [G loss: 1.320607]\n",
      "237 [D loss: 1.158786, acc.: 74.22%] [G loss: 1.377962]\n",
      "238 [D loss: 0.912653, acc.: 68.75%] [G loss: 1.456290]\n",
      "239 [D loss: 1.758132, acc.: 70.31%] [G loss: 1.475859]\n",
      "240 [D loss: 1.835653, acc.: 64.06%] [G loss: 1.450459]\n",
      "241 [D loss: 2.075480, acc.: 71.09%] [G loss: 1.488376]\n",
      "242 [D loss: 1.737855, acc.: 71.09%] [G loss: 1.507444]\n",
      "243 [D loss: 1.785188, acc.: 77.34%] [G loss: 1.320201]\n",
      "244 [D loss: 3.736213, acc.: 53.12%] [G loss: 1.073230]\n",
      "245 [D loss: 2.005682, acc.: 66.41%] [G loss: 1.183180]\n",
      "246 [D loss: 1.859399, acc.: 67.19%] [G loss: 1.314056]\n",
      "247 [D loss: 1.914457, acc.: 64.06%] [G loss: 1.498335]\n",
      "248 [D loss: 1.363051, acc.: 83.59%] [G loss: 1.607662]\n",
      "249 [D loss: 1.702192, acc.: 75.78%] [G loss: 1.617017]\n",
      "250 [D loss: 1.872533, acc.: 80.47%] [G loss: 1.531024]\n",
      "251 [D loss: 1.439209, acc.: 74.22%] [G loss: 1.493654]\n",
      "252 [D loss: 2.111682, acc.: 66.41%] [G loss: 1.200473]\n",
      "253 [D loss: 1.273126, acc.: 72.66%] [G loss: 1.332864]\n",
      "254 [D loss: 1.610954, acc.: 82.03%] [G loss: 1.466540]\n",
      "255 [D loss: 1.656865, acc.: 75.78%] [G loss: 1.494760]\n",
      "256 [D loss: 2.123620, acc.: 78.12%] [G loss: 1.404537]\n",
      "257 [D loss: 2.097308, acc.: 79.69%] [G loss: 1.436115]\n",
      "258 [D loss: 2.029190, acc.: 75.78%] [G loss: 1.434336]\n",
      "259 [D loss: 2.150701, acc.: 74.22%] [G loss: 1.444522]\n",
      "260 [D loss: 2.006425, acc.: 79.69%] [G loss: 1.220186]\n",
      "261 [D loss: 1.108697, acc.: 74.22%] [G loss: 1.326223]\n",
      "262 [D loss: 2.099821, acc.: 59.38%] [G loss: 0.917249]\n",
      "263 [D loss: 1.226790, acc.: 71.88%] [G loss: 1.160189]\n",
      "264 [D loss: 0.687087, acc.: 78.12%] [G loss: 1.418028]\n",
      "265 [D loss: 1.635253, acc.: 79.69%] [G loss: 1.611300]\n",
      "266 [D loss: 1.559395, acc.: 76.56%] [G loss: 1.594378]\n",
      "267 [D loss: 1.131585, acc.: 82.03%] [G loss: 1.512717]\n",
      "268 [D loss: 0.762629, acc.: 85.16%] [G loss: 1.398316]\n",
      "269 [D loss: 1.685833, acc.: 75.78%] [G loss: 1.295536]\n",
      "270 [D loss: 0.913526, acc.: 79.69%] [G loss: 1.353832]\n",
      "271 [D loss: 1.538783, acc.: 77.34%] [G loss: 1.380633]\n",
      "272 [D loss: 1.880993, acc.: 76.56%] [G loss: 1.291171]\n",
      "273 [D loss: 2.217442, acc.: 71.88%] [G loss: 1.361906]\n",
      "274 [D loss: 0.741244, acc.: 85.94%] [G loss: 1.378938]\n",
      "275 [D loss: 1.630100, acc.: 80.47%] [G loss: 1.278561]\n",
      "276 [D loss: 1.549556, acc.: 77.34%] [G loss: 1.313637]\n",
      "277 [D loss: 2.399473, acc.: 73.44%] [G loss: 1.284402]\n",
      "278 [D loss: 0.517050, acc.: 85.16%] [G loss: 1.425609]\n",
      "279 [D loss: 1.563330, acc.: 70.31%] [G loss: 1.007239]\n",
      "280 [D loss: 0.848974, acc.: 75.00%] [G loss: 1.118730]\n",
      "281 [D loss: 1.114253, acc.: 75.78%] [G loss: 1.253558]\n",
      "282 [D loss: 0.915774, acc.: 80.47%] [G loss: 1.481664]\n",
      "283 [D loss: 0.965631, acc.: 86.72%] [G loss: 1.368075]\n",
      "284 [D loss: 1.272808, acc.: 83.59%] [G loss: 1.289725]\n",
      "285 [D loss: 1.260376, acc.: 82.81%] [G loss: 1.218898]\n",
      "286 [D loss: 1.086448, acc.: 74.22%] [G loss: 1.233681]\n",
      "287 [D loss: 1.298882, acc.: 78.12%] [G loss: 1.282100]\n",
      "288 [D loss: 1.290287, acc.: 78.91%] [G loss: 1.295545]\n",
      "289 [D loss: 0.496983, acc.: 84.38%] [G loss: 1.387429]\n",
      "290 [D loss: 0.886159, acc.: 82.81%] [G loss: 1.286029]\n",
      "291 [D loss: 1.085554, acc.: 79.69%] [G loss: 1.310158]\n",
      "292 [D loss: 1.743623, acc.: 69.53%] [G loss: 1.042837]\n",
      "293 [D loss: 0.996901, acc.: 75.00%] [G loss: 1.137819]\n",
      "294 [D loss: 0.394665, acc.: 86.72%] [G loss: 1.250359]\n",
      "295 [D loss: 1.255512, acc.: 84.38%] [G loss: 1.293511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.714596, acc.: 89.06%] [G loss: 1.195927]\n",
      "297 [D loss: 1.293511, acc.: 78.12%] [G loss: 1.234422]\n",
      "298 [D loss: 1.510658, acc.: 80.47%] [G loss: 1.265035]\n",
      "299 [D loss: 0.834029, acc.: 79.69%] [G loss: 1.246167]\n",
      "300 [D loss: 0.905931, acc.: 82.81%] [G loss: 1.321465]\n",
      "301 [D loss: 0.521523, acc.: 86.72%] [G loss: 1.316330]\n",
      "302 [D loss: 1.369074, acc.: 81.25%] [G loss: 1.340075]\n",
      "303 [D loss: 1.119299, acc.: 83.59%] [G loss: 1.299991]\n",
      "304 [D loss: 1.496243, acc.: 82.81%] [G loss: 1.254269]\n",
      "305 [D loss: 1.473108, acc.: 85.16%] [G loss: 1.294131]\n",
      "306 [D loss: 1.363028, acc.: 85.16%] [G loss: 1.247320]\n",
      "307 [D loss: 1.118316, acc.: 85.94%] [G loss: 1.321675]\n",
      "308 [D loss: 1.407594, acc.: 75.00%] [G loss: 1.346804]\n",
      "309 [D loss: 2.245696, acc.: 75.00%] [G loss: 1.397185]\n",
      "310 [D loss: 0.618976, acc.: 86.72%] [G loss: 1.304172]\n",
      "311 [D loss: 2.029838, acc.: 75.78%] [G loss: 1.323601]\n",
      "312 [D loss: 1.461155, acc.: 86.72%] [G loss: 1.371235]\n",
      "313 [D loss: 1.732625, acc.: 82.03%] [G loss: 1.399932]\n",
      "314 [D loss: 1.300275, acc.: 89.84%] [G loss: 1.451452]\n",
      "315 [D loss: 1.059982, acc.: 90.62%] [G loss: 1.406756]\n",
      "316 [D loss: 1.926958, acc.: 84.38%] [G loss: 1.336893]\n",
      "317 [D loss: 1.432822, acc.: 86.72%] [G loss: 1.336622]\n",
      "318 [D loss: 1.137636, acc.: 85.94%] [G loss: 1.271422]\n",
      "319 [D loss: 1.341503, acc.: 83.59%] [G loss: 1.382517]\n",
      "320 [D loss: 1.844496, acc.: 79.69%] [G loss: 1.498026]\n",
      "321 [D loss: 1.448618, acc.: 83.59%] [G loss: 1.552145]\n",
      "322 [D loss: 1.704066, acc.: 82.81%] [G loss: 1.619117]\n",
      "323 [D loss: 1.536784, acc.: 85.94%] [G loss: 1.499295]\n",
      "324 [D loss: 2.280315, acc.: 84.38%] [G loss: 1.408513]\n",
      "325 [D loss: 1.826781, acc.: 84.38%] [G loss: 1.389200]\n",
      "326 [D loss: 1.318320, acc.: 89.84%] [G loss: 1.455148]\n",
      "327 [D loss: 2.597507, acc.: 59.38%] [G loss: 0.680868]\n",
      "328 [D loss: 1.191294, acc.: 64.84%] [G loss: 0.993920]\n",
      "329 [D loss: 1.496059, acc.: 82.03%] [G loss: 1.449548]\n",
      "330 [D loss: 1.144355, acc.: 89.84%] [G loss: 1.564106]\n",
      "331 [D loss: 1.283555, acc.: 91.41%] [G loss: 1.509279]\n",
      "332 [D loss: 1.205052, acc.: 74.22%] [G loss: 1.097313]\n",
      "333 [D loss: 0.276358, acc.: 86.72%] [G loss: 1.204120]\n",
      "334 [D loss: 0.516437, acc.: 85.16%] [G loss: 1.340364]\n",
      "335 [D loss: 0.491794, acc.: 84.38%] [G loss: 1.479062]\n",
      "336 [D loss: 0.195485, acc.: 93.75%] [G loss: 1.357685]\n",
      "337 [D loss: 0.353830, acc.: 92.19%] [G loss: 1.324887]\n",
      "338 [D loss: 0.735619, acc.: 88.28%] [G loss: 1.265442]\n",
      "339 [D loss: 0.775680, acc.: 82.81%] [G loss: 1.225626]\n",
      "340 [D loss: 0.340649, acc.: 87.50%] [G loss: 1.248204]\n",
      "341 [D loss: 0.516383, acc.: 83.59%] [G loss: 1.182764]\n",
      "342 [D loss: 0.990930, acc.: 84.38%] [G loss: 1.196883]\n",
      "343 [D loss: 0.407660, acc.: 85.16%] [G loss: 1.213590]\n",
      "344 [D loss: 0.832640, acc.: 87.50%] [G loss: 1.193570]\n",
      "345 [D loss: 0.761891, acc.: 80.47%] [G loss: 1.186255]\n",
      "346 [D loss: 0.850662, acc.: 85.94%] [G loss: 1.247295]\n",
      "347 [D loss: 0.602498, acc.: 85.16%] [G loss: 1.212225]\n",
      "348 [D loss: 1.229997, acc.: 86.72%] [G loss: 1.211674]\n",
      "349 [D loss: 1.195524, acc.: 77.34%] [G loss: 1.147215]\n",
      "350 [D loss: 0.845523, acc.: 74.22%] [G loss: 1.240581]\n",
      "351 [D loss: 0.603424, acc.: 86.72%] [G loss: 1.266638]\n",
      "352 [D loss: 0.744038, acc.: 88.28%] [G loss: 1.196294]\n",
      "353 [D loss: 0.360347, acc.: 85.94%] [G loss: 1.180197]\n",
      "354 [D loss: 0.791149, acc.: 82.03%] [G loss: 1.275705]\n",
      "355 [D loss: 0.980551, acc.: 84.38%] [G loss: 1.227531]\n",
      "356 [D loss: 0.711918, acc.: 89.06%] [G loss: 1.236623]\n",
      "357 [D loss: 1.159755, acc.: 82.81%] [G loss: 1.217143]\n",
      "358 [D loss: 1.116254, acc.: 83.59%] [G loss: 1.311545]\n",
      "359 [D loss: 0.471182, acc.: 89.84%] [G loss: 1.339367]\n",
      "360 [D loss: 0.772493, acc.: 83.59%] [G loss: 1.296682]\n",
      "361 [D loss: 0.843425, acc.: 85.94%] [G loss: 1.377536]\n",
      "362 [D loss: 0.878053, acc.: 82.81%] [G loss: 1.292615]\n",
      "363 [D loss: 1.608038, acc.: 81.25%] [G loss: 1.249174]\n",
      "364 [D loss: 0.871898, acc.: 82.03%] [G loss: 1.272609]\n",
      "365 [D loss: 0.904230, acc.: 82.81%] [G loss: 1.353628]\n",
      "366 [D loss: 1.622186, acc.: 81.25%] [G loss: 1.373284]\n",
      "367 [D loss: 1.321893, acc.: 87.50%] [G loss: 1.318791]\n",
      "368 [D loss: 1.442123, acc.: 87.50%] [G loss: 1.242821]\n",
      "369 [D loss: 1.345243, acc.: 85.16%] [G loss: 1.203398]\n",
      "370 [D loss: 1.718642, acc.: 85.94%] [G loss: 1.239673]\n",
      "371 [D loss: 1.370915, acc.: 84.38%] [G loss: 1.307616]\n",
      "372 [D loss: 1.442750, acc.: 74.22%] [G loss: 1.269355]\n",
      "373 [D loss: 1.317338, acc.: 75.78%] [G loss: 1.424792]\n",
      "374 [D loss: 1.279537, acc.: 87.50%] [G loss: 1.481333]\n",
      "375 [D loss: 1.201923, acc.: 89.06%] [G loss: 1.380799]\n",
      "376 [D loss: 1.906538, acc.: 76.56%] [G loss: 1.273422]\n",
      "377 [D loss: 1.322028, acc.: 85.94%] [G loss: 1.362729]\n",
      "378 [D loss: 1.328459, acc.: 85.94%] [G loss: 1.354234]\n",
      "379 [D loss: 2.239844, acc.: 76.56%] [G loss: 1.470314]\n",
      "380 [D loss: 1.685874, acc.: 78.91%] [G loss: 1.080056]\n",
      "381 [D loss: 1.263911, acc.: 84.38%] [G loss: 1.211649]\n",
      "382 [D loss: 1.074342, acc.: 88.28%] [G loss: 1.303585]\n",
      "383 [D loss: 1.564726, acc.: 85.94%] [G loss: 1.365867]\n",
      "384 [D loss: 1.072930, acc.: 88.28%] [G loss: 1.387269]\n",
      "385 [D loss: 1.715977, acc.: 82.81%] [G loss: 1.471705]\n",
      "386 [D loss: 1.155176, acc.: 89.84%] [G loss: 1.463553]\n",
      "387 [D loss: 0.932005, acc.: 88.28%] [G loss: 1.460451]\n",
      "388 [D loss: 1.055535, acc.: 89.84%] [G loss: 1.393442]\n",
      "389 [D loss: 1.068220, acc.: 85.16%] [G loss: 1.427022]\n",
      "390 [D loss: 1.562473, acc.: 85.94%] [G loss: 1.447651]\n",
      "391 [D loss: 0.787792, acc.: 90.62%] [G loss: 1.499070]\n",
      "392 [D loss: 1.418938, acc.: 89.06%] [G loss: 1.480391]\n",
      "393 [D loss: 1.309674, acc.: 85.16%] [G loss: 1.473376]\n",
      "394 [D loss: 2.804506, acc.: 78.12%] [G loss: 1.484817]\n",
      "395 [D loss: 1.919650, acc.: 86.72%] [G loss: 1.421361]\n",
      "396 [D loss: 1.064585, acc.: 82.81%] [G loss: 1.216481]\n",
      "397 [D loss: 1.366820, acc.: 55.47%] [G loss: 0.554752]\n",
      "398 [D loss: 0.607099, acc.: 61.72%] [G loss: 1.131562]\n",
      "399 [D loss: 0.193346, acc.: 90.62%] [G loss: 1.664716]\n",
      "400 [D loss: 0.187460, acc.: 90.62%] [G loss: 1.663152]\n",
      "401 [D loss: 0.225059, acc.: 86.72%] [G loss: 1.593562]\n",
      "402 [D loss: 0.394423, acc.: 87.50%] [G loss: 1.431775]\n",
      "403 [D loss: 0.293901, acc.: 85.94%] [G loss: 1.413507]\n",
      "404 [D loss: 0.273403, acc.: 84.38%] [G loss: 1.290147]\n",
      "405 [D loss: 0.317458, acc.: 82.03%] [G loss: 1.252619]\n",
      "406 [D loss: 0.502333, acc.: 79.69%] [G loss: 1.391723]\n",
      "407 [D loss: 0.318771, acc.: 85.16%] [G loss: 1.279519]\n",
      "408 [D loss: 0.354861, acc.: 80.47%] [G loss: 1.304149]\n",
      "409 [D loss: 0.315224, acc.: 82.03%] [G loss: 1.130917]\n",
      "410 [D loss: 0.617930, acc.: 78.12%] [G loss: 1.193778]\n",
      "411 [D loss: 0.537939, acc.: 73.44%] [G loss: 1.173642]\n",
      "412 [D loss: 0.387098, acc.: 78.91%] [G loss: 1.219110]\n",
      "413 [D loss: 0.532224, acc.: 74.22%] [G loss: 1.247310]\n",
      "414 [D loss: 0.483466, acc.: 79.69%] [G loss: 1.196940]\n",
      "415 [D loss: 0.772147, acc.: 75.78%] [G loss: 1.178806]\n",
      "416 [D loss: 0.360257, acc.: 79.69%] [G loss: 1.205649]\n",
      "417 [D loss: 0.430969, acc.: 72.66%] [G loss: 1.106299]\n",
      "418 [D loss: 0.494018, acc.: 68.75%] [G loss: 1.266764]\n",
      "419 [D loss: 0.963335, acc.: 80.47%] [G loss: 1.295896]\n",
      "420 [D loss: 0.968007, acc.: 74.22%] [G loss: 1.296945]\n",
      "421 [D loss: 0.340456, acc.: 80.47%] [G loss: 1.239695]\n",
      "422 [D loss: 0.515226, acc.: 79.69%] [G loss: 1.153063]\n",
      "423 [D loss: 0.468488, acc.: 78.12%] [G loss: 1.213026]\n",
      "424 [D loss: 1.077527, acc.: 71.09%] [G loss: 1.081427]\n",
      "425 [D loss: 0.820480, acc.: 73.44%] [G loss: 1.123822]\n",
      "426 [D loss: 0.993412, acc.: 76.56%] [G loss: 1.265608]\n",
      "427 [D loss: 1.198253, acc.: 78.12%] [G loss: 1.294159]\n",
      "428 [D loss: 0.635070, acc.: 76.56%] [G loss: 1.300320]\n",
      "429 [D loss: 1.670454, acc.: 71.09%] [G loss: 1.309240]\n",
      "430 [D loss: 0.923368, acc.: 78.91%] [G loss: 1.406008]\n",
      "431 [D loss: 1.147780, acc.: 75.78%] [G loss: 1.335540]\n",
      "432 [D loss: 1.532457, acc.: 69.53%] [G loss: 1.354347]\n",
      "433 [D loss: 0.687050, acc.: 78.12%] [G loss: 1.397480]\n",
      "434 [D loss: 0.954186, acc.: 78.91%] [G loss: 1.462581]\n",
      "435 [D loss: 2.007731, acc.: 71.88%] [G loss: 1.524246]\n",
      "436 [D loss: 1.165843, acc.: 83.59%] [G loss: 1.562499]\n",
      "437 [D loss: 1.649828, acc.: 80.47%] [G loss: 1.465140]\n",
      "438 [D loss: 1.285774, acc.: 80.47%] [G loss: 1.524468]\n",
      "439 [D loss: 1.793195, acc.: 74.22%] [G loss: 1.569811]\n",
      "440 [D loss: 1.765592, acc.: 77.34%] [G loss: 1.671876]\n",
      "441 [D loss: 1.112823, acc.: 85.16%] [G loss: 1.619161]\n",
      "442 [D loss: 2.359254, acc.: 78.12%] [G loss: 1.626081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 2.003702, acc.: 76.56%] [G loss: 1.568848]\n",
      "444 [D loss: 1.629616, acc.: 76.56%] [G loss: 1.642401]\n",
      "445 [D loss: 1.702112, acc.: 50.00%] [G loss: 0.364613]\n",
      "446 [D loss: 1.462091, acc.: 50.78%] [G loss: 0.903962]\n",
      "447 [D loss: 0.520578, acc.: 74.22%] [G loss: 1.923959]\n",
      "448 [D loss: 0.290172, acc.: 90.62%] [G loss: 2.300140]\n",
      "449 [D loss: 0.383202, acc.: 88.28%] [G loss: 1.984142]\n",
      "450 [D loss: 0.265164, acc.: 89.06%] [G loss: 1.728876]\n",
      "451 [D loss: 0.400966, acc.: 86.72%] [G loss: 1.469148]\n",
      "452 [D loss: 0.474052, acc.: 82.81%] [G loss: 1.483725]\n",
      "453 [D loss: 0.502260, acc.: 77.34%] [G loss: 1.540681]\n",
      "454 [D loss: 0.325179, acc.: 85.94%] [G loss: 1.571933]\n",
      "455 [D loss: 0.512734, acc.: 77.34%] [G loss: 1.402898]\n",
      "456 [D loss: 0.580449, acc.: 75.00%] [G loss: 1.454484]\n",
      "457 [D loss: 0.689765, acc.: 82.81%] [G loss: 1.508670]\n",
      "458 [D loss: 1.205544, acc.: 80.47%] [G loss: 1.456156]\n",
      "459 [D loss: 0.707895, acc.: 79.69%] [G loss: 1.387047]\n",
      "460 [D loss: 0.694075, acc.: 78.12%] [G loss: 1.458135]\n",
      "461 [D loss: 0.781109, acc.: 84.38%] [G loss: 1.342824]\n",
      "462 [D loss: 0.694617, acc.: 84.38%] [G loss: 1.280200]\n",
      "463 [D loss: 0.920758, acc.: 75.00%] [G loss: 1.424765]\n",
      "464 [D loss: 0.556195, acc.: 82.81%] [G loss: 1.367704]\n",
      "465 [D loss: 0.797174, acc.: 82.81%] [G loss: 1.366900]\n",
      "466 [D loss: 1.049152, acc.: 78.12%] [G loss: 1.466088]\n",
      "467 [D loss: 0.916720, acc.: 78.91%] [G loss: 1.484274]\n",
      "468 [D loss: 1.063334, acc.: 78.12%] [G loss: 1.319885]\n",
      "469 [D loss: 1.078138, acc.: 71.88%] [G loss: 1.185988]\n",
      "470 [D loss: 0.797413, acc.: 75.78%] [G loss: 1.405310]\n",
      "471 [D loss: 0.651390, acc.: 82.81%] [G loss: 1.495066]\n",
      "472 [D loss: 1.345674, acc.: 77.34%] [G loss: 1.555674]\n",
      "473 [D loss: 0.890357, acc.: 82.03%] [G loss: 1.549031]\n",
      "474 [D loss: 1.733072, acc.: 71.09%] [G loss: 1.597811]\n",
      "475 [D loss: 1.354207, acc.: 79.69%] [G loss: 1.473323]\n",
      "476 [D loss: 1.158859, acc.: 71.09%] [G loss: 1.062915]\n",
      "477 [D loss: 0.981029, acc.: 68.75%] [G loss: 1.268125]\n",
      "478 [D loss: 0.662556, acc.: 83.59%] [G loss: 1.567413]\n",
      "479 [D loss: 0.909721, acc.: 81.25%] [G loss: 1.655008]\n",
      "480 [D loss: 0.384996, acc.: 86.72%] [G loss: 1.597473]\n",
      "481 [D loss: 1.061021, acc.: 78.91%] [G loss: 1.427725]\n",
      "482 [D loss: 0.937481, acc.: 68.75%] [G loss: 1.429276]\n",
      "483 [D loss: 1.205905, acc.: 78.12%] [G loss: 1.549374]\n",
      "484 [D loss: 1.663893, acc.: 73.44%] [G loss: 1.687408]\n",
      "485 [D loss: 1.616666, acc.: 53.91%] [G loss: 0.354179]\n",
      "486 [D loss: 1.569524, acc.: 55.47%] [G loss: 0.612878]\n",
      "487 [D loss: 1.059281, acc.: 62.50%] [G loss: 1.639785]\n",
      "488 [D loss: 0.456222, acc.: 78.91%] [G loss: 1.945224]\n",
      "489 [D loss: 0.421050, acc.: 82.81%] [G loss: 1.828457]\n",
      "490 [D loss: 0.538881, acc.: 78.91%] [G loss: 1.387585]\n",
      "491 [D loss: 0.924293, acc.: 71.88%] [G loss: 1.251771]\n",
      "492 [D loss: 0.838758, acc.: 70.31%] [G loss: 1.332560]\n",
      "493 [D loss: 0.811475, acc.: 68.75%] [G loss: 1.141606]\n",
      "494 [D loss: 0.635774, acc.: 73.44%] [G loss: 1.289056]\n",
      "495 [D loss: 0.583776, acc.: 73.44%] [G loss: 1.192743]\n",
      "496 [D loss: 0.863419, acc.: 67.97%] [G loss: 1.284786]\n",
      "497 [D loss: 0.747912, acc.: 68.75%] [G loss: 1.298916]\n",
      "498 [D loss: 0.640287, acc.: 71.88%] [G loss: 1.240855]\n",
      "499 [D loss: 0.917879, acc.: 68.75%] [G loss: 1.350061]\n",
      "500 [D loss: 0.702208, acc.: 76.56%] [G loss: 1.301227]\n",
      "501 [D loss: 0.857373, acc.: 69.53%] [G loss: 1.220032]\n",
      "502 [D loss: 0.870211, acc.: 71.88%] [G loss: 1.108067]\n",
      "503 [D loss: 0.969257, acc.: 71.88%] [G loss: 1.169397]\n",
      "504 [D loss: 1.229201, acc.: 62.50%] [G loss: 1.295381]\n",
      "505 [D loss: 0.958141, acc.: 68.75%] [G loss: 1.471434]\n",
      "506 [D loss: 1.065043, acc.: 66.41%] [G loss: 1.564864]\n",
      "507 [D loss: 0.776002, acc.: 73.44%] [G loss: 1.582557]\n",
      "508 [D loss: 1.004079, acc.: 71.09%] [G loss: 1.554895]\n",
      "509 [D loss: 1.100345, acc.: 71.09%] [G loss: 1.698653]\n",
      "510 [D loss: 0.836421, acc.: 74.22%] [G loss: 1.353481]\n",
      "511 [D loss: 1.043524, acc.: 72.66%] [G loss: 1.555723]\n",
      "512 [D loss: 0.702206, acc.: 71.09%] [G loss: 1.451372]\n",
      "513 [D loss: 1.066955, acc.: 71.88%] [G loss: 1.539144]\n",
      "514 [D loss: 1.446611, acc.: 69.53%] [G loss: 1.758404]\n",
      "515 [D loss: 1.583178, acc.: 67.19%] [G loss: 1.667830]\n",
      "516 [D loss: 1.304705, acc.: 64.84%] [G loss: 1.761074]\n",
      "517 [D loss: 0.883189, acc.: 67.97%] [G loss: 1.971515]\n",
      "518 [D loss: 1.260367, acc.: 67.19%] [G loss: 1.987633]\n",
      "519 [D loss: 1.464359, acc.: 64.84%] [G loss: 2.023426]\n",
      "520 [D loss: 1.358602, acc.: 73.44%] [G loss: 1.925329]\n",
      "521 [D loss: 2.611481, acc.: 57.81%] [G loss: 2.227307]\n",
      "522 [D loss: 1.160852, acc.: 71.09%] [G loss: 2.416729]\n",
      "523 [D loss: 1.387137, acc.: 72.66%] [G loss: 2.432370]\n",
      "524 [D loss: 1.771785, acc.: 71.88%] [G loss: 2.548415]\n",
      "525 [D loss: 2.637057, acc.: 59.38%] [G loss: 2.648671]\n",
      "526 [D loss: 1.871473, acc.: 62.50%] [G loss: 3.364492]\n",
      "527 [D loss: 2.246977, acc.: 68.75%] [G loss: 3.975144]\n",
      "528 [D loss: 1.938315, acc.: 71.88%] [G loss: 3.884326]\n",
      "529 [D loss: 1.981406, acc.: 70.31%] [G loss: 3.957861]\n",
      "530 [D loss: 1.690473, acc.: 78.12%] [G loss: 3.272514]\n",
      "531 [D loss: 2.133443, acc.: 66.41%] [G loss: 3.465809]\n",
      "532 [D loss: 2.803180, acc.: 70.31%] [G loss: 3.708040]\n",
      "533 [D loss: 2.038207, acc.: 75.78%] [G loss: 3.732782]\n",
      "534 [D loss: 2.742802, acc.: 68.75%] [G loss: 3.936435]\n",
      "535 [D loss: 2.346789, acc.: 75.78%] [G loss: 3.676469]\n",
      "536 [D loss: 2.978338, acc.: 73.44%] [G loss: 3.108407]\n",
      "537 [D loss: 1.681389, acc.: 79.69%] [G loss: 3.076200]\n",
      "538 [D loss: 2.072112, acc.: 79.69%] [G loss: 3.110443]\n",
      "539 [D loss: 1.733132, acc.: 80.47%] [G loss: 3.276579]\n",
      "540 [D loss: 2.424580, acc.: 79.69%] [G loss: 3.210460]\n",
      "541 [D loss: 2.446417, acc.: 75.00%] [G loss: 3.171008]\n",
      "542 [D loss: 2.895856, acc.: 78.12%] [G loss: 2.981437]\n",
      "543 [D loss: 2.844299, acc.: 75.78%] [G loss: 2.941195]\n",
      "544 [D loss: 2.869024, acc.: 74.22%] [G loss: 3.114793]\n",
      "545 [D loss: 4.095325, acc.: 60.16%] [G loss: 3.066383]\n",
      "546 [D loss: 2.263875, acc.: 80.47%] [G loss: 3.001764]\n",
      "547 [D loss: 2.255008, acc.: 81.25%] [G loss: 2.648974]\n",
      "548 [D loss: 2.585586, acc.: 75.78%] [G loss: 3.004249]\n",
      "549 [D loss: 2.016749, acc.: 81.25%] [G loss: 3.012532]\n",
      "550 [D loss: 2.237692, acc.: 83.59%] [G loss: 2.796006]\n",
      "551 [D loss: 1.523799, acc.: 85.94%] [G loss: 2.706173]\n",
      "552 [D loss: 2.229478, acc.: 78.12%] [G loss: 2.546664]\n",
      "553 [D loss: 1.711458, acc.: 89.06%] [G loss: 2.580593]\n",
      "554 [D loss: 1.519671, acc.: 85.16%] [G loss: 2.459621]\n",
      "555 [D loss: 1.885700, acc.: 74.22%] [G loss: 1.858472]\n",
      "556 [D loss: 1.388674, acc.: 82.03%] [G loss: 2.301224]\n",
      "557 [D loss: 1.128818, acc.: 89.06%] [G loss: 2.524015]\n",
      "558 [D loss: 1.008572, acc.: 90.62%] [G loss: 2.362965]\n",
      "559 [D loss: 1.524109, acc.: 87.50%] [G loss: 2.839705]\n",
      "560 [D loss: 1.609048, acc.: 87.50%] [G loss: 2.558669]\n",
      "561 [D loss: 1.160724, acc.: 86.72%] [G loss: 2.280553]\n",
      "562 [D loss: 1.021528, acc.: 90.62%] [G loss: 2.392760]\n",
      "563 [D loss: 1.167211, acc.: 86.72%] [G loss: 2.494890]\n",
      "564 [D loss: 1.652243, acc.: 84.38%] [G loss: 3.106454]\n",
      "565 [D loss: 2.443922, acc.: 66.41%] [G loss: 5.039402]\n",
      "566 [D loss: 0.923007, acc.: 87.50%] [G loss: 4.525260]\n",
      "567 [D loss: 1.562575, acc.: 83.59%] [G loss: 3.957044]\n",
      "568 [D loss: 0.661546, acc.: 89.84%] [G loss: 3.147893]\n",
      "569 [D loss: 0.996684, acc.: 85.16%] [G loss: 3.125725]\n",
      "570 [D loss: 2.125338, acc.: 77.34%] [G loss: 2.309286]\n",
      "571 [D loss: 1.383911, acc.: 80.47%] [G loss: 2.147485]\n",
      "572 [D loss: 1.286101, acc.: 81.25%] [G loss: 2.539054]\n",
      "573 [D loss: 1.070847, acc.: 78.91%] [G loss: 4.139387]\n",
      "574 [D loss: 0.751830, acc.: 87.50%] [G loss: 3.426713]\n",
      "575 [D loss: 1.247524, acc.: 72.66%] [G loss: 4.347943]\n",
      "576 [D loss: 1.182691, acc.: 83.59%] [G loss: 4.777670]\n",
      "577 [D loss: 1.232722, acc.: 84.38%] [G loss: 4.530668]\n",
      "578 [D loss: 1.233212, acc.: 83.59%] [G loss: 3.634900]\n",
      "579 [D loss: 0.943189, acc.: 88.28%] [G loss: 3.057441]\n",
      "580 [D loss: 1.134722, acc.: 82.81%] [G loss: 3.230005]\n",
      "581 [D loss: 1.428061, acc.: 79.69%] [G loss: 2.766370]\n",
      "582 [D loss: 1.210936, acc.: 77.34%] [G loss: 3.200137]\n",
      "583 [D loss: 0.807276, acc.: 84.38%] [G loss: 3.324931]\n",
      "584 [D loss: 0.711630, acc.: 87.50%] [G loss: 3.556377]\n",
      "585 [D loss: 0.921546, acc.: 80.47%] [G loss: 3.492659]\n",
      "586 [D loss: 0.953574, acc.: 88.28%] [G loss: 3.428897]\n",
      "587 [D loss: 0.935868, acc.: 84.38%] [G loss: 2.586266]\n",
      "588 [D loss: 0.760609, acc.: 77.34%] [G loss: 3.417641]\n",
      "589 [D loss: 1.256616, acc.: 86.72%] [G loss: 2.952684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 1.363073, acc.: 83.59%] [G loss: 2.512846]\n",
      "591 [D loss: 1.633005, acc.: 82.03%] [G loss: 4.290456]\n",
      "592 [D loss: 1.405654, acc.: 76.56%] [G loss: 3.692740]\n",
      "593 [D loss: 1.304805, acc.: 78.91%] [G loss: 3.398654]\n",
      "594 [D loss: 0.629678, acc.: 82.81%] [G loss: 3.718849]\n",
      "595 [D loss: 1.413895, acc.: 85.16%] [G loss: 3.452236]\n",
      "596 [D loss: 0.989781, acc.: 82.03%] [G loss: 3.335234]\n",
      "597 [D loss: 1.358843, acc.: 82.81%] [G loss: 3.503377]\n",
      "598 [D loss: 1.442609, acc.: 74.22%] [G loss: 3.383338]\n",
      "599 [D loss: 1.737278, acc.: 72.66%] [G loss: 3.826262]\n",
      "600 [D loss: 0.908668, acc.: 86.72%] [G loss: 3.643501]\n",
      "601 [D loss: 0.749052, acc.: 91.41%] [G loss: 2.876334]\n",
      "602 [D loss: 0.782199, acc.: 91.41%] [G loss: 2.437479]\n",
      "603 [D loss: 1.762811, acc.: 71.09%] [G loss: 3.670056]\n",
      "604 [D loss: 1.488532, acc.: 71.88%] [G loss: 3.267539]\n",
      "605 [D loss: 1.051528, acc.: 78.12%] [G loss: 2.906778]\n",
      "606 [D loss: 0.493251, acc.: 82.03%] [G loss: 3.014779]\n",
      "607 [D loss: 0.485221, acc.: 80.47%] [G loss: 3.174373]\n",
      "608 [D loss: 0.201543, acc.: 89.84%] [G loss: 3.317326]\n",
      "609 [D loss: 0.248403, acc.: 96.09%] [G loss: 2.764624]\n",
      "610 [D loss: 0.600867, acc.: 85.16%] [G loss: 2.130972]\n",
      "611 [D loss: 0.591782, acc.: 78.91%] [G loss: 2.120009]\n",
      "612 [D loss: 0.251219, acc.: 86.72%] [G loss: 2.541823]\n",
      "613 [D loss: 0.408114, acc.: 92.19%] [G loss: 2.460418]\n",
      "614 [D loss: 1.030455, acc.: 86.72%] [G loss: 2.285060]\n",
      "615 [D loss: 0.324047, acc.: 92.97%] [G loss: 2.287544]\n",
      "616 [D loss: 0.575871, acc.: 89.06%] [G loss: 2.132951]\n",
      "617 [D loss: 0.592025, acc.: 82.81%] [G loss: 3.032170]\n",
      "618 [D loss: 0.393145, acc.: 88.28%] [G loss: 2.746269]\n",
      "619 [D loss: 0.418345, acc.: 89.84%] [G loss: 2.582329]\n",
      "620 [D loss: 0.162050, acc.: 93.75%] [G loss: 2.409131]\n",
      "621 [D loss: 0.571607, acc.: 89.06%] [G loss: 2.173055]\n",
      "622 [D loss: 0.899307, acc.: 82.03%] [G loss: 2.231939]\n",
      "623 [D loss: 0.601508, acc.: 88.28%] [G loss: 2.257226]\n",
      "624 [D loss: 0.493924, acc.: 85.16%] [G loss: 2.606612]\n",
      "625 [D loss: 0.188618, acc.: 89.84%] [G loss: 2.471979]\n",
      "626 [D loss: 0.811205, acc.: 88.28%] [G loss: 2.402067]\n",
      "627 [D loss: 0.710599, acc.: 85.94%] [G loss: 2.608166]\n",
      "628 [D loss: 0.282058, acc.: 92.19%] [G loss: 2.322033]\n",
      "629 [D loss: 0.257989, acc.: 95.31%] [G loss: 2.104515]\n",
      "630 [D loss: 0.476593, acc.: 87.50%] [G loss: 2.138141]\n",
      "631 [D loss: 0.780321, acc.: 79.69%] [G loss: 2.212162]\n",
      "632 [D loss: 0.731359, acc.: 75.00%] [G loss: 3.739953]\n",
      "633 [D loss: 0.639700, acc.: 85.16%] [G loss: 3.639400]\n",
      "634 [D loss: 1.059072, acc.: 81.25%] [G loss: 3.493699]\n",
      "635 [D loss: 0.345727, acc.: 87.50%] [G loss: 3.397521]\n",
      "636 [D loss: 0.683881, acc.: 89.06%] [G loss: 2.789320]\n",
      "637 [D loss: 0.760631, acc.: 85.16%] [G loss: 3.110368]\n",
      "638 [D loss: 0.433028, acc.: 90.62%] [G loss: 2.511575]\n",
      "639 [D loss: 0.563082, acc.: 82.81%] [G loss: 2.858033]\n",
      "640 [D loss: 0.197034, acc.: 91.41%] [G loss: 2.663090]\n",
      "641 [D loss: 0.502148, acc.: 86.72%] [G loss: 2.612860]\n",
      "642 [D loss: 1.054311, acc.: 84.38%] [G loss: 2.462784]\n",
      "643 [D loss: 0.734417, acc.: 85.16%] [G loss: 2.514076]\n",
      "644 [D loss: 0.682494, acc.: 90.62%] [G loss: 2.673589]\n",
      "645 [D loss: 1.081343, acc.: 87.50%] [G loss: 2.673469]\n",
      "646 [D loss: 1.342921, acc.: 81.25%] [G loss: 2.577912]\n",
      "647 [D loss: 0.902066, acc.: 84.38%] [G loss: 3.020656]\n",
      "648 [D loss: 0.562954, acc.: 83.59%] [G loss: 3.114953]\n",
      "649 [D loss: 0.861215, acc.: 90.62%] [G loss: 2.907277]\n",
      "650 [D loss: 0.915582, acc.: 88.28%] [G loss: 2.555159]\n",
      "651 [D loss: 0.593697, acc.: 89.06%] [G loss: 2.487371]\n",
      "652 [D loss: 0.512371, acc.: 89.84%] [G loss: 2.898818]\n",
      "653 [D loss: 1.327504, acc.: 75.00%] [G loss: 4.416291]\n",
      "654 [D loss: 1.097838, acc.: 75.00%] [G loss: 4.972326]\n",
      "655 [D loss: 1.009955, acc.: 89.06%] [G loss: 5.252805]\n",
      "656 [D loss: 0.543049, acc.: 89.84%] [G loss: 3.980091]\n",
      "657 [D loss: 0.695108, acc.: 95.31%] [G loss: 3.164335]\n",
      "658 [D loss: 0.337856, acc.: 95.31%] [G loss: 2.680134]\n",
      "659 [D loss: 0.813443, acc.: 88.28%] [G loss: 2.257665]\n",
      "660 [D loss: 0.949060, acc.: 87.50%] [G loss: 2.413117]\n",
      "661 [D loss: 1.022009, acc.: 78.12%] [G loss: 1.554811]\n",
      "662 [D loss: 0.930156, acc.: 67.19%] [G loss: 2.788375]\n",
      "663 [D loss: 0.826924, acc.: 76.56%] [G loss: 4.215620]\n",
      "664 [D loss: 0.431828, acc.: 83.59%] [G loss: 4.854479]\n",
      "665 [D loss: 0.231690, acc.: 96.88%] [G loss: 4.396211]\n",
      "666 [D loss: 0.396726, acc.: 92.19%] [G loss: 3.774040]\n",
      "667 [D loss: 0.225846, acc.: 95.31%] [G loss: 2.977805]\n",
      "668 [D loss: 0.144673, acc.: 93.75%] [G loss: 2.524714]\n",
      "669 [D loss: 0.315051, acc.: 89.84%] [G loss: 2.443367]\n",
      "670 [D loss: 0.297994, acc.: 92.97%] [G loss: 2.223101]\n",
      "671 [D loss: 0.410554, acc.: 92.19%] [G loss: 2.146161]\n",
      "672 [D loss: 0.255182, acc.: 93.75%] [G loss: 2.054470]\n",
      "673 [D loss: 0.570026, acc.: 89.06%] [G loss: 2.081944]\n",
      "674 [D loss: 0.571465, acc.: 85.16%] [G loss: 2.780663]\n",
      "675 [D loss: 0.380338, acc.: 87.50%] [G loss: 3.199919]\n",
      "676 [D loss: 0.714110, acc.: 88.28%] [G loss: 3.271298]\n",
      "677 [D loss: 0.431337, acc.: 96.88%] [G loss: 2.770137]\n",
      "678 [D loss: 0.683602, acc.: 88.28%] [G loss: 2.582685]\n",
      "679 [D loss: 0.847608, acc.: 85.16%] [G loss: 2.651674]\n",
      "680 [D loss: 0.703039, acc.: 82.03%] [G loss: 2.889667]\n",
      "681 [D loss: 1.122950, acc.: 78.12%] [G loss: 3.164645]\n",
      "682 [D loss: 0.355491, acc.: 94.53%] [G loss: 3.214855]\n",
      "683 [D loss: 0.492401, acc.: 92.97%] [G loss: 3.112386]\n",
      "684 [D loss: 0.740298, acc.: 92.19%] [G loss: 2.549097]\n",
      "685 [D loss: 0.651333, acc.: 89.84%] [G loss: 2.311804]\n",
      "686 [D loss: 0.577171, acc.: 87.50%] [G loss: 2.301728]\n",
      "687 [D loss: 0.937440, acc.: 89.84%] [G loss: 2.309305]\n",
      "688 [D loss: 0.512665, acc.: 83.59%] [G loss: 2.659707]\n",
      "689 [D loss: 0.392782, acc.: 91.41%] [G loss: 2.575938]\n",
      "690 [D loss: 0.682350, acc.: 88.28%] [G loss: 2.776717]\n",
      "691 [D loss: 0.974146, acc.: 86.72%] [G loss: 2.802823]\n",
      "692 [D loss: 0.593413, acc.: 92.97%] [G loss: 2.737598]\n",
      "693 [D loss: 0.627665, acc.: 94.53%] [G loss: 2.402974]\n",
      "694 [D loss: 0.655306, acc.: 92.19%] [G loss: 2.463533]\n",
      "695 [D loss: 0.595583, acc.: 89.06%] [G loss: 2.416591]\n",
      "696 [D loss: 0.837697, acc.: 92.97%] [G loss: 2.424964]\n",
      "697 [D loss: 1.162828, acc.: 87.50%] [G loss: 2.412429]\n",
      "698 [D loss: 0.772793, acc.: 90.62%] [G loss: 2.474296]\n",
      "699 [D loss: 0.733664, acc.: 93.75%] [G loss: 2.324158]\n",
      "700 [D loss: 1.003547, acc.: 92.19%] [G loss: 2.208802]\n",
      "701 [D loss: 0.827873, acc.: 88.28%] [G loss: 2.470310]\n",
      "702 [D loss: 0.846732, acc.: 83.59%] [G loss: 3.398732]\n",
      "703 [D loss: 1.228010, acc.: 76.56%] [G loss: 3.861442]\n",
      "704 [D loss: 1.134375, acc.: 82.03%] [G loss: 4.161078]\n",
      "705 [D loss: 0.946713, acc.: 90.62%] [G loss: 3.810351]\n",
      "706 [D loss: 1.547079, acc.: 90.62%] [G loss: 2.919496]\n",
      "707 [D loss: 0.875616, acc.: 90.62%] [G loss: 2.463632]\n",
      "708 [D loss: 0.505225, acc.: 91.41%] [G loss: 2.478571]\n",
      "709 [D loss: 0.370852, acc.: 96.09%] [G loss: 2.176618]\n",
      "710 [D loss: 1.256023, acc.: 86.72%] [G loss: 2.139514]\n",
      "711 [D loss: 1.309304, acc.: 85.16%] [G loss: 2.402516]\n",
      "712 [D loss: 0.725077, acc.: 93.75%] [G loss: 2.651208]\n",
      "713 [D loss: 0.870999, acc.: 91.41%] [G loss: 2.689560]\n",
      "714 [D loss: 1.000101, acc.: 90.62%] [G loss: 2.717820]\n",
      "715 [D loss: 0.827030, acc.: 85.16%] [G loss: 2.770418]\n",
      "716 [D loss: 1.125566, acc.: 76.56%] [G loss: 4.175524]\n",
      "717 [D loss: 1.212840, acc.: 72.66%] [G loss: 5.112671]\n",
      "718 [D loss: 1.755984, acc.: 72.66%] [G loss: 6.815166]\n",
      "719 [D loss: 0.523473, acc.: 96.88%] [G loss: 6.130745]\n",
      "720 [D loss: 0.652113, acc.: 95.31%] [G loss: 5.379746]\n",
      "721 [D loss: 0.816859, acc.: 92.19%] [G loss: 3.991295]\n",
      "722 [D loss: 0.465746, acc.: 94.53%] [G loss: 3.538275]\n",
      "723 [D loss: 1.682922, acc.: 89.06%] [G loss: 2.775711]\n",
      "724 [D loss: 1.065378, acc.: 93.75%] [G loss: 2.359021]\n",
      "725 [D loss: 1.043420, acc.: 87.50%] [G loss: 2.319270]\n",
      "726 [D loss: 1.346222, acc.: 89.06%] [G loss: 2.293406]\n",
      "727 [D loss: 0.638174, acc.: 89.84%] [G loss: 2.541650]\n",
      "728 [D loss: 0.684004, acc.: 95.31%] [G loss: 2.514364]\n",
      "729 [D loss: 0.984735, acc.: 91.41%] [G loss: 2.404705]\n",
      "730 [D loss: 0.709861, acc.: 95.31%] [G loss: 2.517696]\n",
      "731 [D loss: 1.099002, acc.: 91.41%] [G loss: 2.451537]\n",
      "732 [D loss: 0.578370, acc.: 95.31%] [G loss: 2.440716]\n",
      "733 [D loss: 1.231919, acc.: 89.84%] [G loss: 2.498018]\n",
      "734 [D loss: 1.204375, acc.: 92.19%] [G loss: 2.565049]\n",
      "735 [D loss: 0.586288, acc.: 94.53%] [G loss: 2.472022]\n",
      "736 [D loss: 1.330089, acc.: 90.62%] [G loss: 2.424798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 [D loss: 0.580509, acc.: 94.53%] [G loss: 2.531780]\n",
      "738 [D loss: 0.824377, acc.: 95.31%] [G loss: 2.230260]\n",
      "739 [D loss: 0.566771, acc.: 96.09%] [G loss: 2.214003]\n",
      "740 [D loss: 1.341032, acc.: 91.41%] [G loss: 2.352428]\n",
      "741 [D loss: 0.707575, acc.: 93.75%] [G loss: 2.536540]\n",
      "742 [D loss: 0.571456, acc.: 96.09%] [G loss: 2.715281]\n",
      "743 [D loss: 1.316275, acc.: 92.19%] [G loss: 2.460939]\n",
      "744 [D loss: 0.314392, acc.: 97.66%] [G loss: 2.358516]\n",
      "745 [D loss: 0.960277, acc.: 93.75%] [G loss: 1.973122]\n",
      "746 [D loss: 0.969865, acc.: 92.97%] [G loss: 2.231240]\n",
      "747 [D loss: 0.732484, acc.: 95.31%] [G loss: 2.521386]\n",
      "748 [D loss: 0.980284, acc.: 92.19%] [G loss: 2.956102]\n",
      "749 [D loss: 1.039448, acc.: 87.50%] [G loss: 3.466633]\n",
      "750 [D loss: 0.863772, acc.: 92.19%] [G loss: 3.260241]\n",
      "751 [D loss: 0.695952, acc.: 95.31%] [G loss: 2.957492]\n",
      "752 [D loss: 0.406425, acc.: 96.88%] [G loss: 2.645305]\n",
      "753 [D loss: 1.049145, acc.: 93.75%] [G loss: 2.373531]\n",
      "754 [D loss: 0.685589, acc.: 96.09%] [G loss: 2.414248]\n",
      "755 [D loss: 0.941398, acc.: 93.75%] [G loss: 2.772823]\n",
      "756 [D loss: 0.418301, acc.: 97.66%] [G loss: 2.512658]\n",
      "757 [D loss: 0.552643, acc.: 90.62%] [G loss: 1.608382]\n",
      "758 [D loss: 0.248366, acc.: 89.06%] [G loss: 1.950663]\n",
      "759 [D loss: 0.642672, acc.: 82.03%] [G loss: 2.530621]\n",
      "760 [D loss: 0.959319, acc.: 75.78%] [G loss: 4.403262]\n",
      "761 [D loss: 1.624124, acc.: 76.56%] [G loss: 5.465668]\n",
      "762 [D loss: 0.010504, acc.: 100.00%] [G loss: 5.891767]\n",
      "763 [D loss: 0.061232, acc.: 98.44%] [G loss: 4.166311]\n",
      "764 [D loss: 0.059390, acc.: 98.44%] [G loss: 2.772027]\n",
      "765 [D loss: 0.131885, acc.: 94.53%] [G loss: 2.387306]\n",
      "766 [D loss: 0.206784, acc.: 89.06%] [G loss: 2.278973]\n",
      "767 [D loss: 0.120805, acc.: 96.88%] [G loss: 2.299061]\n",
      "768 [D loss: 0.164956, acc.: 93.75%] [G loss: 1.979110]\n",
      "769 [D loss: 0.200307, acc.: 90.62%] [G loss: 2.146914]\n",
      "770 [D loss: 0.252994, acc.: 94.53%] [G loss: 2.028900]\n",
      "771 [D loss: 0.161841, acc.: 93.75%] [G loss: 1.902178]\n",
      "772 [D loss: 0.162282, acc.: 94.53%] [G loss: 1.834379]\n",
      "773 [D loss: 0.244737, acc.: 89.84%] [G loss: 2.221155]\n",
      "774 [D loss: 0.437164, acc.: 90.62%] [G loss: 2.020418]\n",
      "775 [D loss: 0.251396, acc.: 87.50%] [G loss: 2.160677]\n",
      "776 [D loss: 0.311974, acc.: 81.25%] [G loss: 2.441483]\n",
      "777 [D loss: 0.387419, acc.: 80.47%] [G loss: 2.713954]\n",
      "778 [D loss: 0.112470, acc.: 94.53%] [G loss: 2.450015]\n",
      "779 [D loss: 0.467870, acc.: 89.06%] [G loss: 2.595754]\n",
      "780 [D loss: 0.366196, acc.: 87.50%] [G loss: 2.248925]\n",
      "781 [D loss: 0.413441, acc.: 83.59%] [G loss: 2.226732]\n",
      "782 [D loss: 0.478547, acc.: 78.91%] [G loss: 2.761294]\n",
      "783 [D loss: 0.402368, acc.: 90.62%] [G loss: 2.710265]\n",
      "784 [D loss: 0.288599, acc.: 91.41%] [G loss: 2.621466]\n",
      "785 [D loss: 0.321552, acc.: 90.62%] [G loss: 2.238225]\n",
      "786 [D loss: 0.431148, acc.: 91.41%] [G loss: 1.944973]\n",
      "787 [D loss: 0.365614, acc.: 86.72%] [G loss: 1.965762]\n",
      "788 [D loss: 0.590607, acc.: 90.62%] [G loss: 2.054415]\n",
      "789 [D loss: 0.464153, acc.: 88.28%] [G loss: 1.883885]\n",
      "790 [D loss: 0.457362, acc.: 87.50%] [G loss: 2.075983]\n",
      "791 [D loss: 0.803868, acc.: 88.28%] [G loss: 1.888120]\n",
      "792 [D loss: 0.412599, acc.: 84.38%] [G loss: 1.966195]\n",
      "793 [D loss: 0.315405, acc.: 92.97%] [G loss: 2.015377]\n",
      "794 [D loss: 0.662789, acc.: 83.59%] [G loss: 2.460497]\n",
      "795 [D loss: 0.252329, acc.: 85.16%] [G loss: 2.594352]\n",
      "796 [D loss: 0.187690, acc.: 89.84%] [G loss: 2.837237]\n",
      "797 [D loss: 0.470473, acc.: 96.09%] [G loss: 2.435512]\n",
      "798 [D loss: 0.288188, acc.: 91.41%] [G loss: 2.286969]\n",
      "799 [D loss: 0.479200, acc.: 89.06%] [G loss: 2.216557]\n",
      "800 [D loss: 0.418590, acc.: 83.59%] [G loss: 3.253413]\n",
      "801 [D loss: 0.589648, acc.: 90.62%] [G loss: 3.049818]\n",
      "802 [D loss: 0.462347, acc.: 89.06%] [G loss: 2.852109]\n",
      "803 [D loss: 0.539280, acc.: 85.16%] [G loss: 3.193924]\n",
      "804 [D loss: 0.365914, acc.: 85.16%] [G loss: 2.310553]\n",
      "805 [D loss: 0.627970, acc.: 79.69%] [G loss: 2.790104]\n",
      "806 [D loss: 0.539848, acc.: 82.03%] [G loss: 3.478137]\n",
      "807 [D loss: 0.628728, acc.: 84.38%] [G loss: 3.457408]\n",
      "808 [D loss: 0.786637, acc.: 87.50%] [G loss: 2.874012]\n",
      "809 [D loss: 0.465285, acc.: 95.31%] [G loss: 2.589602]\n",
      "810 [D loss: 0.518039, acc.: 91.41%] [G loss: 2.140162]\n",
      "811 [D loss: 0.264496, acc.: 94.53%] [G loss: 1.876114]\n",
      "812 [D loss: 1.366687, acc.: 82.03%] [G loss: 2.020551]\n",
      "813 [D loss: 0.259957, acc.: 94.53%] [G loss: 1.967021]\n",
      "814 [D loss: 0.881588, acc.: 84.38%] [G loss: 1.634694]\n",
      "815 [D loss: 0.531144, acc.: 93.75%] [G loss: 1.729668]\n",
      "816 [D loss: 0.294472, acc.: 92.19%] [G loss: 1.851990]\n",
      "817 [D loss: 0.127283, acc.: 97.66%] [G loss: 1.878435]\n",
      "818 [D loss: 0.260187, acc.: 95.31%] [G loss: 1.782969]\n",
      "819 [D loss: 0.645947, acc.: 85.16%] [G loss: 2.340804]\n",
      "820 [D loss: 0.218569, acc.: 97.66%] [G loss: 2.339135]\n",
      "821 [D loss: 0.482775, acc.: 94.53%] [G loss: 2.066158]\n",
      "822 [D loss: 0.256285, acc.: 93.75%] [G loss: 2.172098]\n",
      "823 [D loss: 0.097535, acc.: 97.66%] [G loss: 2.673769]\n",
      "824 [D loss: 0.270951, acc.: 94.53%] [G loss: 2.606779]\n",
      "825 [D loss: 0.414985, acc.: 90.62%] [G loss: 2.026659]\n",
      "826 [D loss: 0.659606, acc.: 88.28%] [G loss: 1.858856]\n",
      "827 [D loss: 0.299017, acc.: 90.62%] [G loss: 2.549093]\n",
      "828 [D loss: 0.113173, acc.: 96.09%] [G loss: 2.568990]\n",
      "829 [D loss: 0.252039, acc.: 95.31%] [G loss: 2.629623]\n",
      "830 [D loss: 0.403995, acc.: 90.62%] [G loss: 2.379586]\n",
      "831 [D loss: 0.405765, acc.: 91.41%] [G loss: 2.551841]\n",
      "832 [D loss: 0.286011, acc.: 95.31%] [G loss: 2.787660]\n",
      "833 [D loss: 0.211656, acc.: 95.31%] [G loss: 2.382786]\n",
      "834 [D loss: 0.376979, acc.: 93.75%] [G loss: 2.092324]\n",
      "835 [D loss: 0.189713, acc.: 92.19%] [G loss: 2.924063]\n",
      "836 [D loss: 0.353565, acc.: 95.31%] [G loss: 2.835515]\n",
      "837 [D loss: 0.367515, acc.: 95.31%] [G loss: 2.649512]\n",
      "838 [D loss: 0.737059, acc.: 79.69%] [G loss: 4.281961]\n",
      "839 [D loss: 1.003598, acc.: 75.78%] [G loss: 4.981239]\n",
      "840 [D loss: 1.302201, acc.: 75.78%] [G loss: 7.187420]\n",
      "841 [D loss: 1.006293, acc.: 89.06%] [G loss: 6.141071]\n",
      "842 [D loss: 0.772854, acc.: 91.41%] [G loss: 5.460485]\n",
      "843 [D loss: 0.956816, acc.: 90.62%] [G loss: 4.166294]\n",
      "844 [D loss: 0.343177, acc.: 94.53%] [G loss: 3.403573]\n",
      "845 [D loss: 1.110594, acc.: 90.62%] [G loss: 2.969349]\n",
      "846 [D loss: 0.822443, acc.: 93.75%] [G loss: 2.302156]\n",
      "847 [D loss: 0.657282, acc.: 89.84%] [G loss: 2.275718]\n",
      "848 [D loss: 0.590255, acc.: 96.09%] [G loss: 2.116501]\n",
      "849 [D loss: 0.365459, acc.: 93.75%] [G loss: 2.262804]\n",
      "850 [D loss: 0.605073, acc.: 95.31%] [G loss: 2.010756]\n",
      "851 [D loss: 1.002638, acc.: 89.84%] [G loss: 1.925612]\n",
      "852 [D loss: 1.043274, acc.: 86.72%] [G loss: 2.796808]\n",
      "853 [D loss: 0.636392, acc.: 89.06%] [G loss: 2.719033]\n",
      "854 [D loss: 0.886873, acc.: 89.84%] [G loss: 2.866418]\n",
      "855 [D loss: 1.233527, acc.: 89.84%] [G loss: 2.827012]\n",
      "856 [D loss: 1.028967, acc.: 87.50%] [G loss: 2.143012]\n",
      "857 [D loss: 0.661536, acc.: 90.62%] [G loss: 2.163355]\n",
      "858 [D loss: 0.282483, acc.: 93.75%] [G loss: 2.650386]\n",
      "859 [D loss: 0.544266, acc.: 89.84%] [G loss: 2.836495]\n",
      "860 [D loss: 0.222946, acc.: 88.28%] [G loss: 3.071521]\n",
      "861 [D loss: 0.673536, acc.: 79.69%] [G loss: 4.155852]\n",
      "862 [D loss: 0.635347, acc.: 96.09%] [G loss: 3.689838]\n",
      "863 [D loss: 0.569684, acc.: 95.31%] [G loss: 2.829335]\n",
      "864 [D loss: 0.599795, acc.: 93.75%] [G loss: 2.187725]\n",
      "865 [D loss: 0.225173, acc.: 96.09%] [G loss: 2.021331]\n",
      "866 [D loss: 0.588491, acc.: 95.31%] [G loss: 1.948641]\n",
      "867 [D loss: 0.288092, acc.: 93.75%] [G loss: 2.123054]\n",
      "868 [D loss: 0.490230, acc.: 95.31%] [G loss: 2.005224]\n",
      "869 [D loss: 0.545750, acc.: 89.84%] [G loss: 2.607932]\n",
      "870 [D loss: 0.435609, acc.: 89.84%] [G loss: 3.073000]\n",
      "871 [D loss: 0.156028, acc.: 98.44%] [G loss: 3.277734]\n",
      "872 [D loss: 0.177774, acc.: 98.44%] [G loss: 2.585924]\n",
      "873 [D loss: 0.780361, acc.: 90.62%] [G loss: 2.630425]\n",
      "874 [D loss: 0.445282, acc.: 88.28%] [G loss: 2.398799]\n",
      "875 [D loss: 0.711478, acc.: 94.53%] [G loss: 2.544606]\n",
      "876 [D loss: 0.717398, acc.: 92.97%] [G loss: 2.489434]\n",
      "877 [D loss: 0.325779, acc.: 98.44%] [G loss: 2.344294]\n",
      "878 [D loss: 0.620728, acc.: 93.75%] [G loss: 2.166442]\n",
      "879 [D loss: 0.524164, acc.: 93.75%] [G loss: 2.714195]\n",
      "880 [D loss: 0.523269, acc.: 91.41%] [G loss: 3.102790]\n",
      "881 [D loss: 0.685354, acc.: 78.12%] [G loss: 3.903411]\n",
      "882 [D loss: 1.096174, acc.: 75.00%] [G loss: 5.152894]\n",
      "883 [D loss: 0.211563, acc.: 96.09%] [G loss: 5.446729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 [D loss: 0.052818, acc.: 98.44%] [G loss: 4.491965]\n",
      "885 [D loss: 0.428790, acc.: 96.09%] [G loss: 3.820748]\n",
      "886 [D loss: 0.811947, acc.: 94.53%] [G loss: 3.299686]\n",
      "887 [D loss: 0.699215, acc.: 94.53%] [G loss: 3.265159]\n",
      "888 [D loss: 1.220610, acc.: 88.28%] [G loss: 2.915794]\n",
      "889 [D loss: 0.462040, acc.: 95.31%] [G loss: 2.942919]\n",
      "890 [D loss: 0.441209, acc.: 96.09%] [G loss: 2.783367]\n",
      "891 [D loss: 0.854607, acc.: 92.19%] [G loss: 2.403064]\n",
      "892 [D loss: 0.194256, acc.: 97.66%] [G loss: 2.081568]\n",
      "893 [D loss: 0.765372, acc.: 91.41%] [G loss: 1.921470]\n",
      "894 [D loss: 0.227647, acc.: 98.44%] [G loss: 1.941106]\n",
      "895 [D loss: 0.575588, acc.: 96.88%] [G loss: 1.910451]\n",
      "896 [D loss: 0.475513, acc.: 96.09%] [G loss: 1.877444]\n",
      "897 [D loss: 1.017362, acc.: 89.84%] [G loss: 1.939756]\n",
      "898 [D loss: 0.737863, acc.: 91.41%] [G loss: 2.213669]\n",
      "899 [D loss: 0.286982, acc.: 92.97%] [G loss: 2.828068]\n",
      "900 [D loss: 0.782244, acc.: 78.91%] [G loss: 3.857322]\n",
      "901 [D loss: 0.809404, acc.: 78.12%] [G loss: 4.072615]\n",
      "902 [D loss: 0.539056, acc.: 95.31%] [G loss: 4.450109]\n",
      "903 [D loss: 0.552413, acc.: 96.09%] [G loss: 3.474620]\n",
      "904 [D loss: 0.794503, acc.: 95.31%] [G loss: 2.934372]\n",
      "905 [D loss: 0.335053, acc.: 96.09%] [G loss: 2.606520]\n",
      "906 [D loss: 0.369350, acc.: 94.53%] [G loss: 2.528365]\n",
      "907 [D loss: 0.434442, acc.: 96.88%] [G loss: 2.416307]\n",
      "908 [D loss: 0.482413, acc.: 95.31%] [G loss: 2.868304]\n",
      "909 [D loss: 1.000078, acc.: 89.84%] [G loss: 2.684804]\n",
      "910 [D loss: 0.548369, acc.: 96.88%] [G loss: 2.443832]\n",
      "911 [D loss: 0.443877, acc.: 97.66%] [G loss: 2.132085]\n",
      "912 [D loss: 0.605102, acc.: 94.53%] [G loss: 1.931394]\n",
      "913 [D loss: 0.498205, acc.: 92.97%] [G loss: 2.060928]\n",
      "914 [D loss: 0.848763, acc.: 93.75%] [G loss: 2.284687]\n",
      "915 [D loss: 0.558692, acc.: 96.09%] [G loss: 2.834048]\n",
      "916 [D loss: 0.157502, acc.: 99.22%] [G loss: 3.380456]\n",
      "917 [D loss: 0.457580, acc.: 94.53%] [G loss: 3.258745]\n",
      "918 [D loss: 1.097232, acc.: 88.28%] [G loss: 2.754187]\n",
      "919 [D loss: 0.585420, acc.: 85.94%] [G loss: 2.303812]\n",
      "920 [D loss: 0.760696, acc.: 91.41%] [G loss: 3.185982]\n",
      "921 [D loss: 0.751665, acc.: 89.84%] [G loss: 3.152851]\n",
      "922 [D loss: 0.527805, acc.: 96.88%] [G loss: 2.760686]\n",
      "923 [D loss: 0.552233, acc.: 96.88%] [G loss: 2.433651]\n",
      "924 [D loss: 0.684873, acc.: 96.09%] [G loss: 2.458932]\n",
      "925 [D loss: 0.791687, acc.: 95.31%] [G loss: 2.322796]\n",
      "926 [D loss: 0.333743, acc.: 97.66%] [G loss: 2.142603]\n",
      "927 [D loss: 0.867778, acc.: 91.41%] [G loss: 2.181865]\n",
      "928 [D loss: 0.782957, acc.: 90.62%] [G loss: 2.588858]\n",
      "929 [D loss: 0.976794, acc.: 82.81%] [G loss: 3.335865]\n",
      "930 [D loss: 0.794151, acc.: 78.91%] [G loss: 4.328390]\n",
      "931 [D loss: 1.062084, acc.: 78.12%] [G loss: 6.102698]\n",
      "932 [D loss: 1.029245, acc.: 93.75%] [G loss: 6.269231]\n",
      "933 [D loss: 0.528688, acc.: 96.88%] [G loss: 4.703855]\n",
      "934 [D loss: 1.028327, acc.: 93.75%] [G loss: 3.774210]\n",
      "935 [D loss: 0.813818, acc.: 92.97%] [G loss: 3.490169]\n",
      "936 [D loss: 0.345432, acc.: 96.88%] [G loss: 3.757205]\n",
      "937 [D loss: 1.024197, acc.: 93.75%] [G loss: 3.241208]\n",
      "938 [D loss: 1.552412, acc.: 90.62%] [G loss: 2.422441]\n",
      "939 [D loss: 0.857884, acc.: 92.19%] [G loss: 2.177649]\n",
      "940 [D loss: 0.597580, acc.: 95.31%] [G loss: 1.928065]\n",
      "941 [D loss: 0.578313, acc.: 95.31%] [G loss: 1.929690]\n",
      "942 [D loss: 0.703145, acc.: 95.31%] [G loss: 2.035011]\n",
      "943 [D loss: 0.188474, acc.: 98.44%] [G loss: 2.152773]\n",
      "944 [D loss: 0.687455, acc.: 96.09%] [G loss: 2.487067]\n",
      "945 [D loss: 0.549833, acc.: 96.09%] [G loss: 2.757983]\n",
      "946 [D loss: 0.809966, acc.: 94.53%] [G loss: 2.477692]\n",
      "947 [D loss: 0.318498, acc.: 97.66%] [G loss: 2.343980]\n",
      "948 [D loss: 0.691519, acc.: 96.09%] [G loss: 2.311014]\n",
      "949 [D loss: 0.803587, acc.: 95.31%] [G loss: 2.435262]\n",
      "950 [D loss: 0.417809, acc.: 97.66%] [G loss: 2.764685]\n",
      "951 [D loss: 0.568137, acc.: 96.09%] [G loss: 2.755890]\n",
      "952 [D loss: 0.681792, acc.: 94.53%] [G loss: 2.477899]\n",
      "953 [D loss: 0.680981, acc.: 96.09%] [G loss: 2.590050]\n",
      "954 [D loss: 0.573667, acc.: 96.09%] [G loss: 2.419165]\n",
      "955 [D loss: 0.834906, acc.: 92.19%] [G loss: 2.392250]\n",
      "956 [D loss: 0.715560, acc.: 92.97%] [G loss: 2.665564]\n",
      "957 [D loss: 1.001817, acc.: 90.62%] [G loss: 2.907119]\n",
      "958 [D loss: 1.079104, acc.: 91.41%] [G loss: 3.264955]\n",
      "959 [D loss: 0.415958, acc.: 96.88%] [G loss: 3.075594]\n",
      "960 [D loss: 0.670830, acc.: 96.09%] [G loss: 2.688372]\n",
      "961 [D loss: 0.541550, acc.: 96.88%] [G loss: 2.502332]\n",
      "962 [D loss: 0.785058, acc.: 95.31%] [G loss: 2.516225]\n",
      "963 [D loss: 0.913829, acc.: 94.53%] [G loss: 2.318029]\n",
      "964 [D loss: 0.938502, acc.: 94.53%] [G loss: 2.557199]\n",
      "965 [D loss: 0.679227, acc.: 96.09%] [G loss: 2.432277]\n",
      "966 [D loss: 0.432688, acc.: 97.66%] [G loss: 2.347558]\n",
      "967 [D loss: 0.307359, acc.: 97.66%] [G loss: 2.204145]\n",
      "968 [D loss: 1.452890, acc.: 89.84%] [G loss: 2.259842]\n",
      "969 [D loss: 0.819494, acc.: 94.53%] [G loss: 2.408193]\n",
      "970 [D loss: 0.288883, acc.: 98.44%] [G loss: 2.701716]\n",
      "971 [D loss: 0.657763, acc.: 96.09%] [G loss: 3.037010]\n",
      "972 [D loss: 0.781345, acc.: 95.31%] [G loss: 3.418546]\n",
      "973 [D loss: 0.546129, acc.: 95.31%] [G loss: 2.863359]\n",
      "974 [D loss: 0.854088, acc.: 92.97%] [G loss: 2.252480]\n",
      "975 [D loss: 0.666830, acc.: 90.62%] [G loss: 3.044549]\n",
      "976 [D loss: 1.690083, acc.: 77.34%] [G loss: 4.108111]\n",
      "977 [D loss: 1.201563, acc.: 78.12%] [G loss: 5.101376]\n",
      "978 [D loss: 0.466981, acc.: 93.75%] [G loss: 5.154957]\n",
      "979 [D loss: 0.270334, acc.: 98.44%] [G loss: 4.648998]\n",
      "980 [D loss: 0.390300, acc.: 97.66%] [G loss: 3.836703]\n",
      "981 [D loss: 0.397389, acc.: 97.66%] [G loss: 3.228662]\n",
      "982 [D loss: 0.807694, acc.: 92.97%] [G loss: 2.372663]\n",
      "983 [D loss: 0.684728, acc.: 96.09%] [G loss: 2.304139]\n",
      "984 [D loss: 0.184926, acc.: 99.22%] [G loss: 2.413394]\n",
      "985 [D loss: 0.801166, acc.: 94.53%] [G loss: 2.529338]\n",
      "986 [D loss: 0.673881, acc.: 96.09%] [G loss: 2.208912]\n",
      "987 [D loss: 0.580951, acc.: 96.09%] [G loss: 2.269546]\n",
      "988 [D loss: 0.555159, acc.: 96.88%] [G loss: 2.200822]\n",
      "989 [D loss: 0.177873, acc.: 99.22%] [G loss: 2.324192]\n",
      "990 [D loss: 0.956877, acc.: 92.97%] [G loss: 2.357399]\n",
      "991 [D loss: 0.419399, acc.: 97.66%] [G loss: 2.533503]\n",
      "992 [D loss: 0.541092, acc.: 96.88%] [G loss: 2.124171]\n",
      "993 [D loss: 0.304152, acc.: 98.44%] [G loss: 2.000142]\n",
      "994 [D loss: 0.557334, acc.: 96.09%] [G loss: 2.065949]\n",
      "995 [D loss: 0.564241, acc.: 96.88%] [G loss: 2.551481]\n",
      "996 [D loss: 0.678186, acc.: 95.31%] [G loss: 2.931001]\n",
      "997 [D loss: 0.290960, acc.: 98.44%] [G loss: 2.708683]\n",
      "998 [D loss: 0.169866, acc.: 99.22%] [G loss: 2.251429]\n",
      "999 [D loss: 0.462876, acc.: 97.66%] [G loss: 1.898142]\n",
      "1000 [D loss: 0.575774, acc.: 95.31%] [G loss: 2.237793]\n",
      "1001 [D loss: 0.692396, acc.: 94.53%] [G loss: 2.632753]\n",
      "1002 [D loss: 0.428593, acc.: 97.66%] [G loss: 2.499163]\n",
      "1003 [D loss: 0.288012, acc.: 98.44%] [G loss: 2.296175]\n",
      "1004 [D loss: 0.541314, acc.: 96.88%] [G loss: 2.122507]\n",
      "1005 [D loss: 0.430756, acc.: 97.66%] [G loss: 2.070334]\n",
      "1006 [D loss: 0.807180, acc.: 95.31%] [G loss: 2.012109]\n",
      "1007 [D loss: 0.420605, acc.: 97.66%] [G loss: 2.318095]\n",
      "1008 [D loss: 0.677847, acc.: 95.31%] [G loss: 2.801641]\n",
      "1009 [D loss: 0.913267, acc.: 94.53%] [G loss: 2.497509]\n",
      "1010 [D loss: 0.571071, acc.: 93.75%] [G loss: 1.605418]\n",
      "1011 [D loss: 0.294456, acc.: 90.62%] [G loss: 2.164969]\n",
      "1012 [D loss: 0.700118, acc.: 77.34%] [G loss: 4.621648]\n",
      "1013 [D loss: 1.472091, acc.: 80.47%] [G loss: 5.298376]\n",
      "1014 [D loss: 0.250803, acc.: 93.75%] [G loss: 5.491382]\n",
      "1015 [D loss: 0.177547, acc.: 97.66%] [G loss: 4.549550]\n",
      "1016 [D loss: 0.157948, acc.: 97.66%] [G loss: 3.926685]\n",
      "1017 [D loss: 0.341787, acc.: 94.53%] [G loss: 3.317150]\n",
      "1018 [D loss: 0.050334, acc.: 96.88%] [G loss: 3.225220]\n",
      "1019 [D loss: 0.106077, acc.: 96.88%] [G loss: 3.076772]\n",
      "1020 [D loss: 0.435243, acc.: 97.66%] [G loss: 2.552113]\n",
      "1021 [D loss: 0.135399, acc.: 93.75%] [G loss: 2.761894]\n",
      "1022 [D loss: 0.164156, acc.: 99.22%] [G loss: 2.245847]\n",
      "1023 [D loss: 0.106136, acc.: 96.09%] [G loss: 2.203079]\n",
      "1024 [D loss: 0.234892, acc.: 92.97%] [G loss: 2.199333]\n",
      "1025 [D loss: 0.090538, acc.: 97.66%] [G loss: 2.445170]\n",
      "1026 [D loss: 0.193138, acc.: 99.22%] [G loss: 2.093875]\n",
      "1027 [D loss: 0.379690, acc.: 92.19%] [G loss: 2.176097]\n",
      "1028 [D loss: 0.072974, acc.: 99.22%] [G loss: 1.955027]\n",
      "1029 [D loss: 0.373477, acc.: 93.75%] [G loss: 1.990785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 [D loss: 0.254147, acc.: 94.53%] [G loss: 2.034827]\n",
      "1031 [D loss: 0.242433, acc.: 94.53%] [G loss: 1.947265]\n",
      "1032 [D loss: 0.217875, acc.: 97.66%] [G loss: 1.959361]\n",
      "1033 [D loss: 0.358638, acc.: 95.31%] [G loss: 1.975897]\n",
      "1034 [D loss: 0.211355, acc.: 98.44%] [G loss: 2.007083]\n",
      "1035 [D loss: 0.187584, acc.: 98.44%] [G loss: 2.108546]\n",
      "1036 [D loss: 0.832736, acc.: 94.53%] [G loss: 2.410025]\n",
      "1037 [D loss: 0.461738, acc.: 95.31%] [G loss: 2.254279]\n",
      "1038 [D loss: 0.351242, acc.: 95.31%] [G loss: 1.786477]\n",
      "1039 [D loss: 0.498617, acc.: 94.53%] [G loss: 1.994265]\n",
      "1040 [D loss: 0.587747, acc.: 95.31%] [G loss: 2.063732]\n",
      "1041 [D loss: 0.701250, acc.: 95.31%] [G loss: 2.162156]\n",
      "1042 [D loss: 0.312589, acc.: 98.44%] [G loss: 2.188044]\n",
      "1043 [D loss: 0.318148, acc.: 96.88%] [G loss: 2.330760]\n",
      "1044 [D loss: 0.569779, acc.: 95.31%] [G loss: 2.177872]\n",
      "1045 [D loss: 0.435308, acc.: 96.88%] [G loss: 2.312885]\n",
      "1046 [D loss: 0.293216, acc.: 98.44%] [G loss: 2.743413]\n",
      "1047 [D loss: 0.429672, acc.: 96.88%] [G loss: 2.739817]\n",
      "1048 [D loss: 0.176247, acc.: 99.22%] [G loss: 2.634624]\n",
      "1049 [D loss: 0.443171, acc.: 96.09%] [G loss: 2.168103]\n",
      "1050 [D loss: 0.934838, acc.: 89.84%] [G loss: 2.825167]\n",
      "1051 [D loss: 0.708896, acc.: 78.12%] [G loss: 4.752759]\n",
      "1052 [D loss: 2.162267, acc.: 71.88%] [G loss: 6.230992]\n",
      "1053 [D loss: 0.812213, acc.: 91.41%] [G loss: 6.945775]\n",
      "1054 [D loss: 0.529476, acc.: 96.09%] [G loss: 6.190546]\n",
      "1055 [D loss: 0.417449, acc.: 95.31%] [G loss: 5.022918]\n",
      "1056 [D loss: 0.895786, acc.: 94.53%] [G loss: 4.437855]\n",
      "1057 [D loss: 0.502881, acc.: 96.09%] [G loss: 3.660578]\n",
      "1058 [D loss: 0.065737, acc.: 96.09%] [G loss: 3.404783]\n",
      "1059 [D loss: 0.535146, acc.: 96.88%] [G loss: 2.890774]\n",
      "1060 [D loss: 0.288063, acc.: 98.44%] [G loss: 2.555506]\n",
      "1061 [D loss: 0.438116, acc.: 97.66%] [G loss: 2.567338]\n",
      "1062 [D loss: 0.444694, acc.: 96.88%] [G loss: 2.603889]\n",
      "1063 [D loss: 0.804393, acc.: 95.31%] [G loss: 2.492213]\n",
      "1064 [D loss: 0.817753, acc.: 95.31%] [G loss: 2.692751]\n",
      "1065 [D loss: 0.558128, acc.: 96.88%] [G loss: 2.811044]\n",
      "1066 [D loss: 0.820422, acc.: 95.31%] [G loss: 2.319038]\n",
      "1067 [D loss: 0.969632, acc.: 92.19%] [G loss: 2.006595]\n",
      "1068 [D loss: 0.942827, acc.: 94.53%] [G loss: 2.252950]\n",
      "1069 [D loss: 0.548308, acc.: 96.88%] [G loss: 2.391996]\n",
      "1070 [D loss: 0.615068, acc.: 87.50%] [G loss: 1.827178]\n",
      "1071 [D loss: 0.229175, acc.: 90.62%] [G loss: 2.845311]\n",
      "1072 [D loss: 0.388829, acc.: 82.81%] [G loss: 3.742329]\n",
      "1073 [D loss: 0.173319, acc.: 91.41%] [G loss: 5.093400]\n",
      "1074 [D loss: 0.038163, acc.: 99.22%] [G loss: 3.297992]\n",
      "1075 [D loss: 0.083614, acc.: 96.88%] [G loss: 2.644162]\n",
      "1076 [D loss: 0.267797, acc.: 93.75%] [G loss: 3.049294]\n",
      "1077 [D loss: 0.330448, acc.: 85.94%] [G loss: 3.160202]\n",
      "1078 [D loss: 0.630441, acc.: 75.78%] [G loss: 5.979754]\n",
      "1079 [D loss: 1.377093, acc.: 72.66%] [G loss: 7.474217]\n",
      "1080 [D loss: 0.118666, acc.: 92.97%] [G loss: 7.003138]\n",
      "1081 [D loss: 0.455831, acc.: 95.31%] [G loss: 5.928068]\n",
      "1082 [D loss: 0.231786, acc.: 95.31%] [G loss: 4.357000]\n",
      "1083 [D loss: 0.217635, acc.: 91.41%] [G loss: 4.238103]\n",
      "1084 [D loss: 0.438824, acc.: 94.53%] [G loss: 3.760935]\n",
      "1085 [D loss: 0.384299, acc.: 91.41%] [G loss: 3.026210]\n",
      "1086 [D loss: 0.285464, acc.: 92.19%] [G loss: 2.638603]\n",
      "1087 [D loss: 0.255074, acc.: 92.97%] [G loss: 2.686005]\n",
      "1088 [D loss: 0.235789, acc.: 96.09%] [G loss: 2.399876]\n",
      "1089 [D loss: 0.353195, acc.: 95.31%] [G loss: 2.094355]\n",
      "1090 [D loss: 0.289787, acc.: 93.75%] [G loss: 2.041610]\n",
      "1091 [D loss: 0.334870, acc.: 96.88%] [G loss: 1.799845]\n",
      "1092 [D loss: 0.559795, acc.: 90.62%] [G loss: 2.179176]\n",
      "1093 [D loss: 0.230636, acc.: 95.31%] [G loss: 2.015507]\n",
      "1094 [D loss: 0.290047, acc.: 92.97%] [G loss: 2.047868]\n",
      "1095 [D loss: 0.404699, acc.: 85.16%] [G loss: 1.385087]\n",
      "1096 [D loss: 0.338658, acc.: 84.38%] [G loss: 2.051394]\n",
      "1097 [D loss: 0.612532, acc.: 79.69%] [G loss: 2.619302]\n",
      "1098 [D loss: 0.378646, acc.: 93.75%] [G loss: 2.458135]\n",
      "1099 [D loss: 0.178872, acc.: 90.62%] [G loss: 2.146378]\n",
      "1100 [D loss: 0.355235, acc.: 93.75%] [G loss: 2.117764]\n",
      "1101 [D loss: 0.250943, acc.: 96.09%] [G loss: 1.836548]\n",
      "1102 [D loss: 0.414067, acc.: 89.84%] [G loss: 1.864291]\n",
      "1103 [D loss: 0.422431, acc.: 92.97%] [G loss: 1.932973]\n",
      "1104 [D loss: 0.439285, acc.: 89.84%] [G loss: 2.136455]\n",
      "1105 [D loss: 0.110242, acc.: 96.09%] [G loss: 2.090288]\n",
      "1106 [D loss: 0.409648, acc.: 92.19%] [G loss: 1.948951]\n",
      "1107 [D loss: 0.166669, acc.: 94.53%] [G loss: 1.957137]\n",
      "1108 [D loss: 0.222481, acc.: 98.44%] [G loss: 1.938024]\n",
      "1109 [D loss: 0.386808, acc.: 94.53%] [G loss: 1.902512]\n",
      "1110 [D loss: 0.324349, acc.: 88.28%] [G loss: 2.211872]\n",
      "1111 [D loss: 0.244553, acc.: 94.53%] [G loss: 2.291483]\n",
      "1112 [D loss: 0.332723, acc.: 96.88%] [G loss: 2.220181]\n",
      "1113 [D loss: 0.084544, acc.: 96.88%] [G loss: 2.456037]\n",
      "1114 [D loss: 0.106406, acc.: 96.09%] [G loss: 2.239481]\n",
      "1115 [D loss: 0.545331, acc.: 91.41%] [G loss: 2.066154]\n",
      "1116 [D loss: 0.415647, acc.: 91.41%] [G loss: 2.206774]\n",
      "1117 [D loss: 0.636253, acc.: 92.97%] [G loss: 2.875422]\n",
      "1118 [D loss: 0.353303, acc.: 93.75%] [G loss: 2.562022]\n",
      "1119 [D loss: 0.432152, acc.: 91.41%] [G loss: 2.828961]\n",
      "1120 [D loss: 0.340644, acc.: 95.31%] [G loss: 3.186529]\n",
      "1121 [D loss: 0.201473, acc.: 96.09%] [G loss: 3.742934]\n",
      "1122 [D loss: 0.441463, acc.: 96.09%] [G loss: 3.385383]\n",
      "1123 [D loss: 0.722534, acc.: 93.75%] [G loss: 2.631385]\n",
      "1124 [D loss: 0.447277, acc.: 90.62%] [G loss: 2.738745]\n",
      "1125 [D loss: 0.323362, acc.: 95.31%] [G loss: 3.163172]\n",
      "1126 [D loss: 0.467682, acc.: 96.09%] [G loss: 3.006611]\n",
      "1127 [D loss: 0.492324, acc.: 92.97%] [G loss: 2.886355]\n",
      "1128 [D loss: 0.498408, acc.: 92.97%] [G loss: 3.073014]\n",
      "1129 [D loss: 0.819381, acc.: 94.53%] [G loss: 2.753179]\n",
      "1130 [D loss: 0.766495, acc.: 89.06%] [G loss: 3.200189]\n",
      "1131 [D loss: 0.442163, acc.: 96.88%] [G loss: 3.590180]\n",
      "1132 [D loss: 0.425437, acc.: 96.09%] [G loss: 3.048758]\n",
      "1133 [D loss: 0.454113, acc.: 95.31%] [G loss: 3.024619]\n",
      "1134 [D loss: 0.441113, acc.: 95.31%] [G loss: 3.320886]\n",
      "1135 [D loss: 0.186507, acc.: 97.66%] [G loss: 2.657713]\n",
      "1136 [D loss: 0.373152, acc.: 94.53%] [G loss: 2.672007]\n",
      "1137 [D loss: 0.652004, acc.: 91.41%] [G loss: 3.852715]\n",
      "1138 [D loss: 0.558319, acc.: 80.47%] [G loss: 5.085524]\n",
      "1139 [D loss: 0.800399, acc.: 83.59%] [G loss: 6.485948]\n",
      "1140 [D loss: 0.934529, acc.: 92.19%] [G loss: 6.196263]\n",
      "1141 [D loss: 0.388602, acc.: 97.66%] [G loss: 5.005921]\n",
      "1142 [D loss: 0.477123, acc.: 93.75%] [G loss: 4.197467]\n",
      "1143 [D loss: 0.612046, acc.: 92.19%] [G loss: 2.182412]\n",
      "1144 [D loss: 0.300435, acc.: 92.19%] [G loss: 2.228642]\n",
      "1145 [D loss: 0.186139, acc.: 98.44%] [G loss: 2.494998]\n",
      "1146 [D loss: 0.204925, acc.: 98.44%] [G loss: 2.232222]\n",
      "1147 [D loss: 0.099148, acc.: 98.44%] [G loss: 2.541437]\n",
      "1148 [D loss: 0.220865, acc.: 97.66%] [G loss: 2.501666]\n",
      "1149 [D loss: 0.477819, acc.: 85.94%] [G loss: 3.630965]\n",
      "1150 [D loss: 0.338664, acc.: 86.72%] [G loss: 4.213267]\n",
      "1151 [D loss: 0.322852, acc.: 94.53%] [G loss: 4.340729]\n",
      "1152 [D loss: 0.542535, acc.: 96.09%] [G loss: 3.228784]\n",
      "1153 [D loss: 0.098693, acc.: 95.31%] [G loss: 3.050088]\n",
      "1154 [D loss: 0.322584, acc.: 96.88%] [G loss: 2.865722]\n",
      "1155 [D loss: 0.329071, acc.: 96.88%] [G loss: 2.601341]\n",
      "1156 [D loss: 0.748461, acc.: 92.19%] [G loss: 2.439795]\n",
      "1157 [D loss: 0.066944, acc.: 98.44%] [G loss: 2.418897]\n",
      "1158 [D loss: 0.559774, acc.: 96.88%] [G loss: 2.237466]\n",
      "1159 [D loss: 0.482155, acc.: 93.75%] [G loss: 2.496397]\n",
      "1160 [D loss: 0.466646, acc.: 94.53%] [G loss: 2.320354]\n",
      "1161 [D loss: 0.395047, acc.: 91.41%] [G loss: 2.565700]\n",
      "1162 [D loss: 0.227470, acc.: 96.88%] [G loss: 2.486139]\n",
      "1163 [D loss: 0.727171, acc.: 92.19%] [G loss: 2.380247]\n",
      "1164 [D loss: 0.484930, acc.: 95.31%] [G loss: 2.579044]\n",
      "1165 [D loss: 0.366237, acc.: 93.75%] [G loss: 3.218998]\n",
      "1166 [D loss: 0.592523, acc.: 92.97%] [G loss: 3.571959]\n",
      "1167 [D loss: 0.613653, acc.: 85.16%] [G loss: 4.369462]\n",
      "1168 [D loss: 1.030501, acc.: 75.78%] [G loss: 4.834892]\n",
      "1169 [D loss: 0.164668, acc.: 98.44%] [G loss: 6.145672]\n",
      "1170 [D loss: 0.144119, acc.: 99.22%] [G loss: 4.939733]\n",
      "1171 [D loss: 0.520893, acc.: 96.88%] [G loss: 3.335179]\n",
      "1172 [D loss: 0.623559, acc.: 91.41%] [G loss: 3.085142]\n",
      "1173 [D loss: 0.471511, acc.: 94.53%] [G loss: 2.774090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174 [D loss: 0.473679, acc.: 94.53%] [G loss: 3.096764]\n",
      "1175 [D loss: 0.958694, acc.: 92.97%] [G loss: 2.984654]\n",
      "1176 [D loss: 0.431283, acc.: 96.88%] [G loss: 2.841341]\n",
      "1177 [D loss: 0.677851, acc.: 96.09%] [G loss: 2.891844]\n",
      "1178 [D loss: 0.805976, acc.: 94.53%] [G loss: 2.444227]\n",
      "1179 [D loss: 0.350518, acc.: 96.09%] [G loss: 2.152921]\n",
      "1180 [D loss: 0.371985, acc.: 95.31%] [G loss: 2.283623]\n",
      "1181 [D loss: 0.739270, acc.: 88.28%] [G loss: 1.948515]\n",
      "1182 [D loss: 0.216793, acc.: 97.66%] [G loss: 2.184646]\n",
      "1183 [D loss: 0.395160, acc.: 89.84%] [G loss: 3.401247]\n",
      "1184 [D loss: 0.801651, acc.: 72.66%] [G loss: 5.791682]\n",
      "1185 [D loss: 1.832942, acc.: 75.78%] [G loss: 6.965136]\n",
      "1186 [D loss: 0.174204, acc.: 97.66%] [G loss: 6.538568]\n",
      "1187 [D loss: 0.937651, acc.: 75.00%] [G loss: 8.297983]\n",
      "1188 [D loss: 0.527958, acc.: 96.09%] [G loss: 7.490407]\n",
      "1189 [D loss: 0.215517, acc.: 92.19%] [G loss: 6.766553]\n",
      "1190 [D loss: 0.182548, acc.: 96.09%] [G loss: 5.754239]\n",
      "1191 [D loss: 0.037550, acc.: 99.22%] [G loss: 5.015674]\n",
      "1192 [D loss: 0.445539, acc.: 96.88%] [G loss: 3.978717]\n",
      "1193 [D loss: 0.291961, acc.: 98.44%] [G loss: 3.220169]\n",
      "1194 [D loss: 0.236798, acc.: 97.66%] [G loss: 2.677462]\n",
      "1195 [D loss: 0.317032, acc.: 96.88%] [G loss: 2.416967]\n",
      "1196 [D loss: 0.259931, acc.: 96.09%] [G loss: 2.519694]\n",
      "1197 [D loss: 0.597726, acc.: 92.97%] [G loss: 2.614928]\n",
      "1198 [D loss: 0.647529, acc.: 92.97%] [G loss: 2.532059]\n",
      "1199 [D loss: 0.243076, acc.: 94.53%] [G loss: 2.443107]\n",
      "1200 [D loss: 0.322563, acc.: 98.44%] [G loss: 2.217644]\n",
      "1201 [D loss: 0.318644, acc.: 98.44%] [G loss: 2.033953]\n",
      "1202 [D loss: 0.267529, acc.: 93.75%] [G loss: 2.497667]\n",
      "1203 [D loss: 0.440463, acc.: 91.41%] [G loss: 2.782941]\n",
      "1204 [D loss: 0.866161, acc.: 85.16%] [G loss: 3.196622]\n",
      "1205 [D loss: 0.650372, acc.: 82.81%] [G loss: 4.008978]\n",
      "1206 [D loss: 0.410596, acc.: 96.88%] [G loss: 4.088735]\n",
      "1207 [D loss: 0.588682, acc.: 93.75%] [G loss: 3.610140]\n",
      "1208 [D loss: 0.292620, acc.: 98.44%] [G loss: 2.989067]\n",
      "1209 [D loss: 0.314822, acc.: 98.44%] [G loss: 2.445171]\n",
      "1210 [D loss: 0.970464, acc.: 91.41%] [G loss: 2.492576]\n",
      "1211 [D loss: 0.692176, acc.: 87.50%] [G loss: 2.568796]\n",
      "1212 [D loss: 0.501723, acc.: 93.75%] [G loss: 3.128101]\n",
      "1213 [D loss: 0.474386, acc.: 93.75%] [G loss: 2.866984]\n",
      "1214 [D loss: 0.502533, acc.: 91.41%] [G loss: 3.452125]\n",
      "1215 [D loss: 0.523728, acc.: 96.88%] [G loss: 3.538775]\n",
      "1216 [D loss: 0.157772, acc.: 99.22%] [G loss: 2.926100]\n",
      "1217 [D loss: 0.208733, acc.: 96.09%] [G loss: 2.381531]\n",
      "1218 [D loss: 0.642423, acc.: 91.41%] [G loss: 2.390505]\n",
      "1219 [D loss: 1.084497, acc.: 85.16%] [G loss: 2.663135]\n",
      "1220 [D loss: 0.414063, acc.: 90.62%] [G loss: 3.788176]\n",
      "1221 [D loss: 0.597538, acc.: 92.19%] [G loss: 3.608002]\n",
      "1222 [D loss: 0.031093, acc.: 100.00%] [G loss: 3.267551]\n",
      "1223 [D loss: 0.302450, acc.: 97.66%] [G loss: 2.992512]\n",
      "1224 [D loss: 0.308917, acc.: 97.66%] [G loss: 2.969917]\n",
      "1225 [D loss: 0.159888, acc.: 99.22%] [G loss: 3.119645]\n",
      "1226 [D loss: 0.145844, acc.: 99.22%] [G loss: 2.877960]\n",
      "1227 [D loss: 0.451848, acc.: 96.09%] [G loss: 2.481128]\n",
      "1228 [D loss: 0.462721, acc.: 94.53%] [G loss: 2.449183]\n",
      "1229 [D loss: 1.113279, acc.: 92.19%] [G loss: 3.143685]\n",
      "1230 [D loss: 0.385847, acc.: 91.41%] [G loss: 3.401466]\n",
      "1231 [D loss: 0.406401, acc.: 91.41%] [G loss: 4.362856]\n",
      "1232 [D loss: 0.648961, acc.: 96.09%] [G loss: 4.033261]\n",
      "1233 [D loss: 0.282616, acc.: 98.44%] [G loss: 3.313471]\n",
      "1234 [D loss: 0.778042, acc.: 95.31%] [G loss: 3.106065]\n",
      "1235 [D loss: 0.536256, acc.: 96.09%] [G loss: 2.756263]\n",
      "1236 [D loss: 0.536639, acc.: 96.88%] [G loss: 2.575967]\n",
      "1237 [D loss: 0.679484, acc.: 96.09%] [G loss: 3.109362]\n",
      "1238 [D loss: 0.040899, acc.: 100.00%] [G loss: 3.120009]\n",
      "1239 [D loss: 0.527868, acc.: 96.88%] [G loss: 3.125031]\n",
      "1240 [D loss: 0.789968, acc.: 95.31%] [G loss: 2.948899]\n",
      "1241 [D loss: 0.409706, acc.: 97.66%] [G loss: 3.004975]\n",
      "1242 [D loss: 0.175256, acc.: 98.44%] [G loss: 2.622442]\n",
      "1243 [D loss: 0.623534, acc.: 91.41%] [G loss: 3.413492]\n",
      "1244 [D loss: 1.169082, acc.: 71.09%] [G loss: 4.774443]\n",
      "1245 [D loss: 1.405507, acc.: 76.56%] [G loss: 7.054727]\n",
      "1246 [D loss: 1.682211, acc.: 76.56%] [G loss: 7.565019]\n",
      "1247 [D loss: 0.381597, acc.: 97.66%] [G loss: 8.805698]\n",
      "1248 [D loss: 0.424538, acc.: 96.09%] [G loss: 7.750748]\n",
      "1249 [D loss: 0.932273, acc.: 92.19%] [G loss: 6.335930]\n",
      "1250 [D loss: 0.669871, acc.: 94.53%] [G loss: 5.602199]\n",
      "1251 [D loss: 1.055107, acc.: 92.97%] [G loss: 5.338997]\n",
      "1252 [D loss: 0.663715, acc.: 96.09%] [G loss: 4.871601]\n",
      "1253 [D loss: 0.268475, acc.: 98.44%] [G loss: 4.298401]\n",
      "1254 [D loss: 0.655446, acc.: 96.09%] [G loss: 3.986449]\n",
      "1255 [D loss: 0.784111, acc.: 95.31%] [G loss: 4.235059]\n",
      "1256 [D loss: 0.773733, acc.: 95.31%] [G loss: 3.716552]\n",
      "1257 [D loss: 0.903745, acc.: 94.53%] [G loss: 3.140847]\n",
      "1258 [D loss: 0.552268, acc.: 96.88%] [G loss: 2.671407]\n",
      "1259 [D loss: 0.680272, acc.: 96.09%] [G loss: 2.534304]\n",
      "1260 [D loss: 0.802613, acc.: 94.53%] [G loss: 2.451216]\n",
      "1261 [D loss: 1.421155, acc.: 91.41%] [G loss: 2.625321]\n",
      "1262 [D loss: 0.686089, acc.: 96.09%] [G loss: 2.679688]\n",
      "1263 [D loss: 0.694719, acc.: 93.75%] [G loss: 2.854141]\n",
      "1264 [D loss: 0.691908, acc.: 96.09%] [G loss: 3.132497]\n",
      "1265 [D loss: 0.899043, acc.: 94.53%] [G loss: 3.079225]\n",
      "1266 [D loss: 1.033659, acc.: 93.75%] [G loss: 2.749179]\n",
      "1267 [D loss: 0.668132, acc.: 95.31%] [G loss: 2.623611]\n",
      "1268 [D loss: 0.413210, acc.: 97.66%] [G loss: 2.479549]\n",
      "1269 [D loss: 0.805790, acc.: 95.31%] [G loss: 2.733392]\n",
      "1270 [D loss: 1.167499, acc.: 92.19%] [G loss: 2.953269]\n",
      "1271 [D loss: 0.658830, acc.: 96.09%] [G loss: 2.580123]\n",
      "1272 [D loss: 0.789190, acc.: 95.31%] [G loss: 2.800605]\n",
      "1273 [D loss: 0.657383, acc.: 96.09%] [G loss: 3.289692]\n",
      "1274 [D loss: 0.517608, acc.: 96.88%] [G loss: 3.380627]\n",
      "1275 [D loss: 1.091211, acc.: 90.62%] [G loss: 3.202564]\n",
      "1276 [D loss: 0.759441, acc.: 82.03%] [G loss: 3.290080]\n",
      "1277 [D loss: 0.877086, acc.: 85.94%] [G loss: 5.096456]\n",
      "1278 [D loss: 0.953623, acc.: 92.19%] [G loss: 5.388754]\n",
      "1279 [D loss: 1.154177, acc.: 92.97%] [G loss: 4.789806]\n",
      "1280 [D loss: 0.133665, acc.: 99.22%] [G loss: 4.330574]\n",
      "1281 [D loss: 0.891967, acc.: 94.53%] [G loss: 3.614808]\n",
      "1282 [D loss: 0.777429, acc.: 95.31%] [G loss: 3.236435]\n",
      "1283 [D loss: 1.150132, acc.: 92.97%] [G loss: 2.996465]\n",
      "1284 [D loss: 0.651032, acc.: 96.09%] [G loss: 2.988160]\n",
      "1285 [D loss: 0.275655, acc.: 98.44%] [G loss: 3.120728]\n",
      "1286 [D loss: 1.268386, acc.: 92.19%] [G loss: 3.601442]\n",
      "1287 [D loss: 0.642761, acc.: 96.09%] [G loss: 3.137415]\n",
      "1288 [D loss: 0.285626, acc.: 98.44%] [G loss: 2.493647]\n",
      "1289 [D loss: 0.672992, acc.: 96.09%] [G loss: 2.401706]\n",
      "1290 [D loss: 0.586139, acc.: 94.53%] [G loss: 2.141337]\n",
      "1291 [D loss: 0.192038, acc.: 89.06%] [G loss: 3.348642]\n",
      "1292 [D loss: 0.781161, acc.: 82.81%] [G loss: 4.089746]\n",
      "1293 [D loss: 0.297080, acc.: 90.62%] [G loss: 5.538981]\n",
      "1294 [D loss: 0.025253, acc.: 99.22%] [G loss: 4.902993]\n",
      "1295 [D loss: 0.021228, acc.: 100.00%] [G loss: 4.390584]\n",
      "1296 [D loss: 0.144448, acc.: 99.22%] [G loss: 3.624099]\n",
      "1297 [D loss: 0.021156, acc.: 100.00%] [G loss: 3.062120]\n",
      "1298 [D loss: 0.147804, acc.: 99.22%] [G loss: 2.611142]\n",
      "1299 [D loss: 0.158437, acc.: 99.22%] [G loss: 2.409088]\n",
      "1300 [D loss: 0.035236, acc.: 100.00%] [G loss: 2.368341]\n",
      "1301 [D loss: 0.284924, acc.: 98.44%] [G loss: 2.490502]\n",
      "1302 [D loss: 0.030641, acc.: 100.00%] [G loss: 2.374237]\n",
      "1303 [D loss: 0.038955, acc.: 100.00%] [G loss: 2.262384]\n",
      "1304 [D loss: 0.211097, acc.: 94.53%] [G loss: 2.292016]\n",
      "1305 [D loss: 0.271638, acc.: 92.19%] [G loss: 2.751052]\n",
      "1306 [D loss: 0.514461, acc.: 91.41%] [G loss: 3.763767]\n",
      "1307 [D loss: 0.233347, acc.: 92.19%] [G loss: 3.999521]\n",
      "1308 [D loss: 0.147505, acc.: 99.22%] [G loss: 4.081280]\n",
      "1309 [D loss: 0.142410, acc.: 99.22%] [G loss: 3.170093]\n",
      "1310 [D loss: 0.018414, acc.: 100.00%] [G loss: 2.913562]\n",
      "1311 [D loss: 0.153898, acc.: 99.22%] [G loss: 2.353305]\n",
      "1312 [D loss: 0.173324, acc.: 99.22%] [G loss: 2.302744]\n",
      "1313 [D loss: 0.037518, acc.: 99.22%] [G loss: 2.287640]\n",
      "1314 [D loss: 0.307315, acc.: 98.44%] [G loss: 2.443689]\n",
      "1315 [D loss: 0.439258, acc.: 97.66%] [G loss: 2.884530]\n",
      "1316 [D loss: 0.471841, acc.: 88.28%] [G loss: 1.612245]\n",
      "1317 [D loss: 0.473220, acc.: 82.81%] [G loss: 3.106977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318 [D loss: 0.764829, acc.: 79.69%] [G loss: 5.474631]\n",
      "1319 [D loss: 0.497609, acc.: 87.50%] [G loss: 6.774199]\n",
      "1320 [D loss: 0.462509, acc.: 89.06%] [G loss: 6.479511]\n",
      "1321 [D loss: 0.670147, acc.: 81.25%] [G loss: 7.255795]\n",
      "1322 [D loss: 0.137896, acc.: 93.75%] [G loss: 6.243711]\n",
      "1323 [D loss: 0.196018, acc.: 90.62%] [G loss: 5.177354]\n",
      "1324 [D loss: 0.374935, acc.: 85.94%] [G loss: 5.023730]\n",
      "1325 [D loss: 0.317542, acc.: 88.28%] [G loss: 3.430892]\n",
      "1326 [D loss: 0.569343, acc.: 74.22%] [G loss: 3.606367]\n",
      "1327 [D loss: 0.250782, acc.: 86.72%] [G loss: 3.259403]\n",
      "1328 [D loss: 0.520046, acc.: 79.69%] [G loss: 2.453569]\n",
      "1329 [D loss: 0.637913, acc.: 75.00%] [G loss: 2.968656]\n",
      "1330 [D loss: 0.665173, acc.: 77.34%] [G loss: 3.899344]\n",
      "1331 [D loss: 1.928137, acc.: 68.75%] [G loss: 6.196882]\n",
      "1332 [D loss: 1.077820, acc.: 78.12%] [G loss: 5.264178]\n",
      "1333 [D loss: 1.057948, acc.: 77.34%] [G loss: 4.473008]\n",
      "1334 [D loss: 0.468723, acc.: 85.16%] [G loss: 4.270262]\n",
      "1335 [D loss: 0.355634, acc.: 86.72%] [G loss: 3.973322]\n",
      "1336 [D loss: 0.451751, acc.: 83.59%] [G loss: 3.551409]\n",
      "1337 [D loss: 0.518683, acc.: 78.91%] [G loss: 4.071784]\n",
      "1338 [D loss: 0.682318, acc.: 78.12%] [G loss: 5.182836]\n",
      "1339 [D loss: 1.052424, acc.: 76.56%] [G loss: 6.265551]\n",
      "1340 [D loss: 0.849478, acc.: 86.72%] [G loss: 5.966898]\n",
      "1341 [D loss: 0.669947, acc.: 88.28%] [G loss: 4.753099]\n",
      "1342 [D loss: 1.615691, acc.: 62.50%] [G loss: 2.837726]\n",
      "1343 [D loss: 0.925451, acc.: 76.56%] [G loss: 4.653474]\n",
      "1344 [D loss: 0.555710, acc.: 81.25%] [G loss: 6.106846]\n",
      "1345 [D loss: 0.458579, acc.: 85.94%] [G loss: 5.492084]\n",
      "1346 [D loss: 1.380498, acc.: 74.22%] [G loss: 4.846228]\n",
      "1347 [D loss: 0.843767, acc.: 80.47%] [G loss: 4.546156]\n",
      "1348 [D loss: 1.319022, acc.: 74.22%] [G loss: 4.666287]\n",
      "1349 [D loss: 1.457675, acc.: 71.88%] [G loss: 4.944881]\n",
      "1350 [D loss: 1.001814, acc.: 78.12%] [G loss: 5.636576]\n",
      "1351 [D loss: 0.420175, acc.: 86.72%] [G loss: 4.850507]\n",
      "1352 [D loss: 0.853860, acc.: 77.34%] [G loss: 4.923878]\n",
      "1353 [D loss: 1.109990, acc.: 73.44%] [G loss: 5.603096]\n",
      "1354 [D loss: 0.687866, acc.: 88.28%] [G loss: 6.037932]\n",
      "1355 [D loss: 0.997052, acc.: 78.12%] [G loss: 5.990515]\n",
      "1356 [D loss: 1.901788, acc.: 71.09%] [G loss: 6.865253]\n",
      "1357 [D loss: 0.948577, acc.: 78.91%] [G loss: 8.267116]\n",
      "1358 [D loss: 0.678949, acc.: 89.06%] [G loss: 7.439443]\n",
      "1359 [D loss: 1.536901, acc.: 70.31%] [G loss: 8.193714]\n",
      "1360 [D loss: 0.711184, acc.: 83.59%] [G loss: 9.074524]\n",
      "1361 [D loss: 1.255399, acc.: 75.78%] [G loss: 10.282093]\n",
      "1362 [D loss: 0.687541, acc.: 85.16%] [G loss: 10.691861]\n",
      "1363 [D loss: 0.676653, acc.: 85.16%] [G loss: 10.550081]\n",
      "1364 [D loss: 0.933104, acc.: 85.16%] [G loss: 9.266101]\n",
      "1365 [D loss: 1.077993, acc.: 78.91%] [G loss: 10.148928]\n",
      "1366 [D loss: 1.706639, acc.: 78.91%] [G loss: 11.209749]\n",
      "1367 [D loss: 0.746864, acc.: 90.62%] [G loss: 9.958877]\n",
      "1368 [D loss: 1.365888, acc.: 82.03%] [G loss: 9.829594]\n",
      "1369 [D loss: 1.056650, acc.: 87.50%] [G loss: 9.200686]\n",
      "1370 [D loss: 0.921659, acc.: 89.06%] [G loss: 8.570741]\n",
      "1371 [D loss: 1.094565, acc.: 82.03%] [G loss: 8.528153]\n",
      "1372 [D loss: 0.926528, acc.: 92.19%] [G loss: 8.357539]\n",
      "1373 [D loss: 0.910512, acc.: 83.59%] [G loss: 7.108823]\n",
      "1374 [D loss: 0.734034, acc.: 87.50%] [G loss: 7.574974]\n",
      "1375 [D loss: 0.831412, acc.: 91.41%] [G loss: 7.080101]\n",
      "1376 [D loss: 1.055588, acc.: 82.81%] [G loss: 6.953328]\n",
      "1377 [D loss: 0.847675, acc.: 90.62%] [G loss: 7.123979]\n",
      "1378 [D loss: 1.718432, acc.: 86.72%] [G loss: 6.547098]\n",
      "1379 [D loss: 1.352158, acc.: 89.06%] [G loss: 6.194850]\n",
      "1380 [D loss: 1.036594, acc.: 82.03%] [G loss: 6.966773]\n",
      "1381 [D loss: 0.547821, acc.: 95.31%] [G loss: 6.630622]\n",
      "1382 [D loss: 1.206719, acc.: 85.16%] [G loss: 5.466098]\n",
      "1383 [D loss: 0.445765, acc.: 96.09%] [G loss: 5.153188]\n",
      "1384 [D loss: 0.398133, acc.: 90.62%] [G loss: 3.268109]\n",
      "1385 [D loss: 0.149290, acc.: 92.97%] [G loss: 3.538523]\n",
      "1386 [D loss: 0.554869, acc.: 86.72%] [G loss: 3.813612]\n",
      "1387 [D loss: 0.484001, acc.: 75.78%] [G loss: 7.214473]\n",
      "1388 [D loss: 1.653654, acc.: 78.12%] [G loss: 8.185234]\n",
      "1389 [D loss: 0.196325, acc.: 97.66%] [G loss: 7.627604]\n",
      "1390 [D loss: 0.686954, acc.: 87.50%] [G loss: 7.314448]\n",
      "1391 [D loss: 0.137443, acc.: 99.22%] [G loss: 6.049788]\n",
      "1392 [D loss: 0.491526, acc.: 94.53%] [G loss: 4.434525]\n",
      "1393 [D loss: 0.656563, acc.: 89.84%] [G loss: 3.671422]\n",
      "1394 [D loss: 0.168169, acc.: 98.44%] [G loss: 2.745920]\n",
      "1395 [D loss: 0.305782, acc.: 92.97%] [G loss: 2.567778]\n",
      "1396 [D loss: 0.140460, acc.: 94.53%] [G loss: 2.678981]\n",
      "1397 [D loss: 0.350577, acc.: 95.31%] [G loss: 2.404592]\n",
      "1398 [D loss: 0.134777, acc.: 94.53%] [G loss: 2.321487]\n",
      "1399 [D loss: 0.369218, acc.: 96.09%] [G loss: 2.316048]\n",
      "1400 [D loss: 0.394866, acc.: 92.19%] [G loss: 2.582511]\n",
      "1401 [D loss: 0.409649, acc.: 90.62%] [G loss: 2.604809]\n",
      "1402 [D loss: 0.238035, acc.: 96.88%] [G loss: 2.738330]\n",
      "1403 [D loss: 0.496267, acc.: 91.41%] [G loss: 2.638351]\n",
      "1404 [D loss: 0.359662, acc.: 94.53%] [G loss: 2.547122]\n",
      "1405 [D loss: 0.546065, acc.: 89.84%] [G loss: 2.958730]\n",
      "1406 [D loss: 0.640607, acc.: 90.62%] [G loss: 3.101772]\n",
      "1407 [D loss: 0.579931, acc.: 86.72%] [G loss: 4.147678]\n",
      "1408 [D loss: 1.050764, acc.: 82.03%] [G loss: 4.383061]\n",
      "1409 [D loss: 0.612933, acc.: 84.38%] [G loss: 5.230802]\n",
      "1410 [D loss: 0.301718, acc.: 90.62%] [G loss: 5.745708]\n",
      "1411 [D loss: 0.526999, acc.: 96.88%] [G loss: 5.050616]\n",
      "1412 [D loss: 0.436430, acc.: 95.31%] [G loss: 3.745821]\n",
      "1413 [D loss: 0.095536, acc.: 94.53%] [G loss: 3.200089]\n",
      "1414 [D loss: 0.191748, acc.: 99.22%] [G loss: 2.658943]\n",
      "1415 [D loss: 0.204173, acc.: 98.44%] [G loss: 2.587861]\n",
      "1416 [D loss: 0.453284, acc.: 94.53%] [G loss: 2.478175]\n",
      "1417 [D loss: 0.209698, acc.: 96.88%] [G loss: 2.712553]\n",
      "1418 [D loss: 0.458923, acc.: 94.53%] [G loss: 3.240122]\n",
      "1419 [D loss: 0.260784, acc.: 88.28%] [G loss: 4.257373]\n",
      "1420 [D loss: 0.981418, acc.: 78.12%] [G loss: 5.451445]\n",
      "1421 [D loss: 0.805732, acc.: 82.03%] [G loss: 8.699663]\n",
      "1422 [D loss: 1.097857, acc.: 89.84%] [G loss: 6.642152]\n",
      "1423 [D loss: 0.393947, acc.: 96.88%] [G loss: 6.360873]\n",
      "1424 [D loss: 0.800609, acc.: 93.75%] [G loss: 4.165619]\n",
      "1425 [D loss: 0.511360, acc.: 92.97%] [G loss: 3.616404]\n",
      "1426 [D loss: 0.197590, acc.: 97.66%] [G loss: 4.020958]\n",
      "1427 [D loss: 0.180541, acc.: 97.66%] [G loss: 3.253573]\n",
      "1428 [D loss: 0.561934, acc.: 96.09%] [G loss: 3.102909]\n",
      "1429 [D loss: 0.313459, acc.: 96.88%] [G loss: 2.570472]\n",
      "1430 [D loss: 0.087113, acc.: 99.22%] [G loss: 2.613136]\n",
      "1431 [D loss: 0.590250, acc.: 96.09%] [G loss: 2.840296]\n",
      "1432 [D loss: 0.558868, acc.: 96.09%] [G loss: 2.751338]\n",
      "1433 [D loss: 0.569423, acc.: 96.09%] [G loss: 2.756830]\n",
      "1434 [D loss: 0.467154, acc.: 95.31%] [G loss: 2.771990]\n",
      "1435 [D loss: 0.808812, acc.: 95.31%] [G loss: 2.912816]\n",
      "1436 [D loss: 0.915113, acc.: 93.75%] [G loss: 2.831705]\n",
      "1437 [D loss: 0.692194, acc.: 95.31%] [G loss: 2.953428]\n",
      "1438 [D loss: 1.065375, acc.: 93.75%] [G loss: 2.779425]\n",
      "1439 [D loss: 0.511207, acc.: 92.97%] [G loss: 2.890695]\n",
      "1440 [D loss: 0.750356, acc.: 82.81%] [G loss: 4.952905]\n",
      "1441 [D loss: 1.037968, acc.: 79.69%] [G loss: 5.905670]\n",
      "1442 [D loss: 1.050531, acc.: 83.59%] [G loss: 6.781539]\n",
      "1443 [D loss: 0.282606, acc.: 96.88%] [G loss: 6.072042]\n",
      "1444 [D loss: 0.291924, acc.: 97.66%] [G loss: 4.834508]\n",
      "1445 [D loss: 1.303387, acc.: 89.84%] [G loss: 4.124369]\n",
      "1446 [D loss: 0.438465, acc.: 96.09%] [G loss: 3.858562]\n",
      "1447 [D loss: 0.906216, acc.: 94.53%] [G loss: 3.443729]\n",
      "1448 [D loss: 0.413188, acc.: 97.66%] [G loss: 3.249003]\n",
      "1449 [D loss: 0.819985, acc.: 93.75%] [G loss: 3.258696]\n",
      "1450 [D loss: 1.181623, acc.: 92.97%] [G loss: 2.946654]\n",
      "1451 [D loss: 0.940609, acc.: 94.53%] [G loss: 2.740462]\n",
      "1452 [D loss: 1.205848, acc.: 91.41%] [G loss: 3.143366]\n",
      "1453 [D loss: 0.801262, acc.: 95.31%] [G loss: 2.896186]\n",
      "1454 [D loss: 0.683036, acc.: 95.31%] [G loss: 2.683746]\n",
      "1455 [D loss: 0.546721, acc.: 96.09%] [G loss: 2.597642]\n",
      "1456 [D loss: 0.943859, acc.: 92.97%] [G loss: 2.734615]\n",
      "1457 [D loss: 0.357647, acc.: 95.31%] [G loss: 3.026887]\n",
      "1458 [D loss: 0.816075, acc.: 95.31%] [G loss: 3.128853]\n",
      "1459 [D loss: 0.666254, acc.: 96.09%] [G loss: 3.233839]\n",
      "1460 [D loss: 0.532822, acc.: 96.88%] [G loss: 3.632678]\n",
      "1461 [D loss: 0.398925, acc.: 97.66%] [G loss: 3.653090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462 [D loss: 0.668002, acc.: 96.09%] [G loss: 3.559418]\n",
      "1463 [D loss: 0.209434, acc.: 97.66%] [G loss: 3.475663]\n",
      "1464 [D loss: 0.903741, acc.: 94.53%] [G loss: 3.240633]\n",
      "1465 [D loss: 1.052248, acc.: 93.75%] [G loss: 3.247927]\n",
      "1466 [D loss: 1.026290, acc.: 88.28%] [G loss: 4.381474]\n",
      "1467 [D loss: 1.086137, acc.: 75.00%] [G loss: 6.722574]\n",
      "1468 [D loss: 2.885594, acc.: 68.75%] [G loss: 7.226782]\n",
      "1469 [D loss: 1.364807, acc.: 78.12%] [G loss: 11.052818]\n",
      "1470 [D loss: 0.255135, acc.: 98.44%] [G loss: 10.784753]\n",
      "1471 [D loss: 0.116416, acc.: 94.53%] [G loss: 8.572069]\n",
      "1472 [D loss: 0.502787, acc.: 83.59%] [G loss: 7.860626]\n",
      "1473 [D loss: 0.097306, acc.: 96.09%] [G loss: 7.609276]\n",
      "1474 [D loss: 0.147566, acc.: 94.53%] [G loss: 4.828560]\n",
      "1475 [D loss: 0.111488, acc.: 96.09%] [G loss: 3.337327]\n",
      "1476 [D loss: 0.351879, acc.: 79.69%] [G loss: 3.469019]\n",
      "1477 [D loss: 0.261222, acc.: 86.72%] [G loss: 4.346372]\n",
      "1478 [D loss: 0.990073, acc.: 74.22%] [G loss: 4.905230]\n",
      "1479 [D loss: 0.704215, acc.: 81.25%] [G loss: 5.367922]\n",
      "1480 [D loss: 0.296299, acc.: 84.38%] [G loss: 4.978268]\n",
      "1481 [D loss: 0.389310, acc.: 78.12%] [G loss: 3.559540]\n",
      "1482 [D loss: 0.658421, acc.: 75.78%] [G loss: 2.951723]\n",
      "1483 [D loss: 0.912096, acc.: 75.78%] [G loss: 3.023324]\n",
      "1484 [D loss: 1.064574, acc.: 71.09%] [G loss: 3.474648]\n",
      "1485 [D loss: 1.083567, acc.: 72.66%] [G loss: 3.078179]\n",
      "1486 [D loss: 1.423956, acc.: 72.66%] [G loss: 5.064577]\n",
      "1487 [D loss: 1.613338, acc.: 70.31%] [G loss: 4.557993]\n",
      "1488 [D loss: 1.054540, acc.: 81.25%] [G loss: 4.585878]\n",
      "1489 [D loss: 1.570897, acc.: 65.62%] [G loss: 6.268413]\n",
      "1490 [D loss: 0.776822, acc.: 80.47%] [G loss: 3.975311]\n",
      "1491 [D loss: 1.555011, acc.: 64.06%] [G loss: 3.287790]\n",
      "1492 [D loss: 1.467083, acc.: 69.53%] [G loss: 4.083467]\n",
      "1493 [D loss: 1.170564, acc.: 75.00%] [G loss: 4.278780]\n",
      "1494 [D loss: 2.856808, acc.: 63.28%] [G loss: 4.909069]\n",
      "1495 [D loss: 0.777826, acc.: 80.47%] [G loss: 4.837169]\n",
      "1496 [D loss: 1.065784, acc.: 75.00%] [G loss: 3.973543]\n",
      "1497 [D loss: 1.299151, acc.: 77.34%] [G loss: 3.785840]\n",
      "1498 [D loss: 1.095202, acc.: 71.09%] [G loss: 4.534954]\n",
      "1499 [D loss: 1.037248, acc.: 79.69%] [G loss: 5.077715]\n",
      "1500 [D loss: 1.043116, acc.: 78.12%] [G loss: 5.296261]\n",
      "1501 [D loss: 1.339356, acc.: 78.12%] [G loss: 5.920601]\n",
      "1502 [D loss: 1.098210, acc.: 80.47%] [G loss: 6.256501]\n",
      "1503 [D loss: 1.384943, acc.: 78.91%] [G loss: 5.383778]\n",
      "1504 [D loss: 1.902255, acc.: 67.97%] [G loss: 7.119785]\n",
      "1505 [D loss: 1.157813, acc.: 88.28%] [G loss: 6.381259]\n",
      "1506 [D loss: 1.610837, acc.: 77.34%] [G loss: 5.197592]\n",
      "1507 [D loss: 2.060964, acc.: 66.41%] [G loss: 7.957459]\n",
      "1508 [D loss: 1.048873, acc.: 89.06%] [G loss: 8.409719]\n",
      "1509 [D loss: 1.583956, acc.: 82.81%] [G loss: 6.923792]\n",
      "1510 [D loss: 1.473266, acc.: 82.03%] [G loss: 5.345773]\n",
      "1511 [D loss: 1.619468, acc.: 71.09%] [G loss: 6.158001]\n",
      "1512 [D loss: 0.780815, acc.: 87.50%] [G loss: 6.595031]\n",
      "1513 [D loss: 1.728576, acc.: 71.09%] [G loss: 8.819592]\n",
      "1514 [D loss: 1.525531, acc.: 86.72%] [G loss: 8.727768]\n",
      "1515 [D loss: 1.048458, acc.: 92.97%] [G loss: 6.928960]\n",
      "1516 [D loss: 1.710173, acc.: 78.91%] [G loss: 5.833067]\n",
      "1517 [D loss: 1.029037, acc.: 93.75%] [G loss: 4.590858]\n",
      "1518 [D loss: 2.246242, acc.: 71.88%] [G loss: 4.842721]\n",
      "1519 [D loss: 1.373096, acc.: 86.72%] [G loss: 5.494468]\n",
      "1520 [D loss: 1.995345, acc.: 82.81%] [G loss: 4.988498]\n",
      "1521 [D loss: 1.489525, acc.: 86.72%] [G loss: 4.779091]\n",
      "1522 [D loss: 1.887112, acc.: 81.25%] [G loss: 5.056232]\n",
      "1523 [D loss: 0.988184, acc.: 88.28%] [G loss: 4.937571]\n",
      "1524 [D loss: 1.254454, acc.: 83.59%] [G loss: 4.761191]\n",
      "1525 [D loss: 1.308607, acc.: 90.62%] [G loss: 4.399743]\n",
      "1526 [D loss: 0.606920, acc.: 93.75%] [G loss: 4.187453]\n",
      "1527 [D loss: 1.561839, acc.: 90.62%] [G loss: 4.045789]\n",
      "1528 [D loss: 1.479780, acc.: 87.50%] [G loss: 4.094601]\n",
      "1529 [D loss: 0.945158, acc.: 90.62%] [G loss: 4.141725]\n",
      "1530 [D loss: 0.718848, acc.: 93.75%] [G loss: 4.253639]\n",
      "1531 [D loss: 0.696226, acc.: 93.75%] [G loss: 3.757083]\n",
      "1532 [D loss: 1.571320, acc.: 89.84%] [G loss: 3.972733]\n",
      "1533 [D loss: 0.974218, acc.: 92.97%] [G loss: 3.791318]\n",
      "1534 [D loss: 0.836800, acc.: 92.19%] [G loss: 4.024231]\n",
      "1535 [D loss: 1.660367, acc.: 89.06%] [G loss: 3.627404]\n",
      "1536 [D loss: 1.099088, acc.: 92.19%] [G loss: 3.329886]\n",
      "1537 [D loss: 0.891142, acc.: 90.62%] [G loss: 4.450021]\n",
      "1538 [D loss: 0.764444, acc.: 89.06%] [G loss: 6.491338]\n",
      "1539 [D loss: 1.062618, acc.: 79.69%] [G loss: 8.380844]\n",
      "1540 [D loss: 0.645328, acc.: 96.09%] [G loss: 8.165220]\n",
      "1541 [D loss: 0.897796, acc.: 82.03%] [G loss: 6.934253]\n",
      "1542 [D loss: 0.279673, acc.: 98.44%] [G loss: 5.899288]\n",
      "1543 [D loss: 0.355370, acc.: 94.53%] [G loss: 6.518859]\n",
      "1544 [D loss: 0.109468, acc.: 95.31%] [G loss: 5.629032]\n",
      "1545 [D loss: 0.017278, acc.: 100.00%] [G loss: 5.182763]\n",
      "1546 [D loss: 0.039948, acc.: 99.22%] [G loss: 4.046952]\n",
      "1547 [D loss: 0.451163, acc.: 95.31%] [G loss: 3.751140]\n",
      "1548 [D loss: 0.082616, acc.: 97.66%] [G loss: 3.547751]\n",
      "1549 [D loss: 0.317221, acc.: 97.66%] [G loss: 3.344526]\n",
      "1550 [D loss: 0.208157, acc.: 96.88%] [G loss: 3.566740]\n",
      "1551 [D loss: 0.056347, acc.: 98.44%] [G loss: 3.623434]\n",
      "1552 [D loss: 0.093251, acc.: 97.66%] [G loss: 3.525505]\n",
      "1553 [D loss: 0.406297, acc.: 84.38%] [G loss: 5.123116]\n",
      "1554 [D loss: 1.029057, acc.: 78.91%] [G loss: 7.767636]\n",
      "1555 [D loss: 1.290750, acc.: 79.69%] [G loss: 10.518770]\n",
      "1556 [D loss: 0.126320, acc.: 99.22%] [G loss: 12.615696]\n",
      "1557 [D loss: 0.000566, acc.: 100.00%] [G loss: 10.469305]\n",
      "1558 [D loss: 0.020907, acc.: 100.00%] [G loss: 8.817729]\n",
      "1559 [D loss: 0.287085, acc.: 92.19%] [G loss: 8.381666]\n",
      "1560 [D loss: 0.051089, acc.: 97.66%] [G loss: 7.049721]\n",
      "1561 [D loss: 0.154386, acc.: 89.84%] [G loss: 7.600507]\n",
      "1562 [D loss: 0.016030, acc.: 100.00%] [G loss: 6.926860]\n",
      "1563 [D loss: 0.045775, acc.: 99.22%] [G loss: 4.703565]\n",
      "1564 [D loss: 0.074665, acc.: 98.44%] [G loss: 3.782729]\n",
      "1565 [D loss: 0.064747, acc.: 99.22%] [G loss: 3.276769]\n",
      "1566 [D loss: 0.135303, acc.: 92.97%] [G loss: 3.123809]\n",
      "1567 [D loss: 0.218247, acc.: 89.06%] [G loss: 4.250262]\n",
      "1568 [D loss: 0.982741, acc.: 78.12%] [G loss: 6.310336]\n",
      "1569 [D loss: 3.206721, acc.: 71.88%] [G loss: 7.980928]\n",
      "1570 [D loss: 0.268903, acc.: 98.44%] [G loss: 11.281801]\n",
      "1571 [D loss: 0.271245, acc.: 92.97%] [G loss: 9.372000]\n",
      "1572 [D loss: 0.042606, acc.: 97.66%] [G loss: 8.283097]\n",
      "1573 [D loss: 0.427306, acc.: 96.09%] [G loss: 6.777877]\n",
      "1574 [D loss: 0.485634, acc.: 85.94%] [G loss: 6.376292]\n",
      "1575 [D loss: 0.038466, acc.: 98.44%] [G loss: 6.321885]\n",
      "1576 [D loss: 0.204098, acc.: 95.31%] [G loss: 5.075546]\n",
      "1577 [D loss: 0.243943, acc.: 93.75%] [G loss: 4.354476]\n",
      "1578 [D loss: 0.225719, acc.: 95.31%] [G loss: 3.767934]\n",
      "1579 [D loss: 0.453937, acc.: 95.31%] [G loss: 3.097579]\n",
      "1580 [D loss: 0.279623, acc.: 85.94%] [G loss: 5.071439]\n",
      "1581 [D loss: 0.961825, acc.: 73.44%] [G loss: 6.195142]\n",
      "1582 [D loss: 3.215151, acc.: 67.19%] [G loss: 10.548158]\n",
      "1583 [D loss: 0.378782, acc.: 97.66%] [G loss: 11.080652]\n",
      "1584 [D loss: 0.360316, acc.: 92.97%] [G loss: 9.316201]\n",
      "1585 [D loss: 0.412611, acc.: 90.62%] [G loss: 8.679766]\n",
      "1586 [D loss: 0.563709, acc.: 95.31%] [G loss: 8.529575]\n",
      "1587 [D loss: 0.557890, acc.: 89.06%] [G loss: 9.325710]\n",
      "1588 [D loss: 0.632376, acc.: 96.09%] [G loss: 8.528572]\n",
      "1589 [D loss: 0.317801, acc.: 94.53%] [G loss: 6.384613]\n",
      "1590 [D loss: 0.432430, acc.: 97.66%] [G loss: 5.192337]\n",
      "1591 [D loss: 0.388361, acc.: 91.41%] [G loss: 5.506770]\n",
      "1592 [D loss: 0.391960, acc.: 97.66%] [G loss: 4.486239]\n",
      "1593 [D loss: 0.300731, acc.: 96.88%] [G loss: 3.619938]\n",
      "1594 [D loss: 0.715093, acc.: 93.75%] [G loss: 3.908275]\n",
      "1595 [D loss: 0.464300, acc.: 94.53%] [G loss: 3.686098]\n",
      "1596 [D loss: 1.079832, acc.: 73.44%] [G loss: 4.988364]\n",
      "1597 [D loss: 1.370401, acc.: 72.66%] [G loss: 6.974606]\n",
      "1598 [D loss: 1.989847, acc.: 71.88%] [G loss: 9.449949]\n",
      "1599 [D loss: 0.504745, acc.: 96.88%] [G loss: 10.826856]\n",
      "1600 [D loss: 0.128510, acc.: 99.22%] [G loss: 8.568228]\n",
      "1601 [D loss: 0.716342, acc.: 85.94%] [G loss: 8.714581]\n",
      "1602 [D loss: 0.138928, acc.: 99.22%] [G loss: 8.125467]\n",
      "1603 [D loss: 0.606959, acc.: 93.75%] [G loss: 8.329969]\n",
      "1604 [D loss: 0.522966, acc.: 96.88%] [G loss: 7.355382]\n",
      "1605 [D loss: 0.028392, acc.: 99.22%] [G loss: 6.595487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606 [D loss: 0.309715, acc.: 98.44%] [G loss: 6.310599]\n",
      "1607 [D loss: 0.397912, acc.: 97.66%] [G loss: 5.857750]\n",
      "1608 [D loss: 0.839660, acc.: 92.19%] [G loss: 5.778264]\n",
      "1609 [D loss: 0.639509, acc.: 96.09%] [G loss: 5.504728]\n",
      "1610 [D loss: 0.522988, acc.: 96.88%] [G loss: 4.936003]\n",
      "1611 [D loss: 0.308344, acc.: 96.09%] [G loss: 4.669604]\n",
      "1612 [D loss: 0.404504, acc.: 97.66%] [G loss: 4.849492]\n",
      "1613 [D loss: 0.783270, acc.: 95.31%] [G loss: 4.215395]\n",
      "1614 [D loss: 0.551324, acc.: 96.09%] [G loss: 3.876904]\n",
      "1615 [D loss: 0.538548, acc.: 96.88%] [G loss: 3.640773]\n",
      "1616 [D loss: 0.413403, acc.: 97.66%] [G loss: 3.068074]\n",
      "1617 [D loss: 0.553335, acc.: 96.88%] [G loss: 3.156000]\n",
      "1618 [D loss: 0.470133, acc.: 94.53%] [G loss: 3.547367]\n",
      "1619 [D loss: 0.652306, acc.: 75.00%] [G loss: 7.268855]\n",
      "1620 [D loss: 1.238109, acc.: 78.12%] [G loss: 10.498586]\n",
      "1621 [D loss: 0.378200, acc.: 97.66%] [G loss: 8.855021]\n",
      "1622 [D loss: 0.256177, acc.: 91.41%] [G loss: 10.145008]\n",
      "1623 [D loss: 0.756929, acc.: 95.31%] [G loss: 9.668154]\n",
      "1624 [D loss: 0.005895, acc.: 100.00%] [G loss: 8.610988]\n",
      "1625 [D loss: 0.769342, acc.: 95.31%] [G loss: 6.860356]\n",
      "1626 [D loss: 0.157350, acc.: 98.44%] [G loss: 5.998835]\n",
      "1627 [D loss: 1.146380, acc.: 92.97%] [G loss: 5.523764]\n",
      "1628 [D loss: 1.028033, acc.: 93.75%] [G loss: 5.032990]\n",
      "1629 [D loss: 0.889587, acc.: 94.53%] [G loss: 4.628398]\n",
      "1630 [D loss: 0.717091, acc.: 94.53%] [G loss: 4.090147]\n",
      "1631 [D loss: 0.784578, acc.: 95.31%] [G loss: 3.872810]\n",
      "1632 [D loss: 0.048487, acc.: 99.22%] [G loss: 3.446816]\n",
      "1633 [D loss: 0.913081, acc.: 94.53%] [G loss: 3.531652]\n",
      "1634 [D loss: 0.410134, acc.: 96.88%] [G loss: 3.596373]\n",
      "1635 [D loss: 0.558348, acc.: 96.88%] [G loss: 3.372073]\n",
      "1636 [D loss: 0.813904, acc.: 93.75%] [G loss: 3.448356]\n",
      "1637 [D loss: 0.708712, acc.: 92.97%] [G loss: 3.697976]\n",
      "1638 [D loss: 0.984872, acc.: 93.75%] [G loss: 4.461732]\n",
      "1639 [D loss: 0.132854, acc.: 99.22%] [G loss: 4.414865]\n",
      "1640 [D loss: 0.262712, acc.: 98.44%] [G loss: 3.928996]\n",
      "1641 [D loss: 0.397848, acc.: 97.66%] [G loss: 3.758850]\n",
      "1642 [D loss: 0.893653, acc.: 94.53%] [G loss: 3.548733]\n",
      "1643 [D loss: 0.774586, acc.: 95.31%] [G loss: 3.642316]\n",
      "1644 [D loss: 0.388284, acc.: 97.66%] [G loss: 4.206342]\n",
      "1645 [D loss: 0.389028, acc.: 97.66%] [G loss: 3.976162]\n",
      "1646 [D loss: 0.687274, acc.: 94.53%] [G loss: 3.361100]\n",
      "1647 [D loss: 0.676080, acc.: 95.31%] [G loss: 3.904402]\n",
      "1648 [D loss: 0.903002, acc.: 94.53%] [G loss: 3.755756]\n",
      "1649 [D loss: 1.158744, acc.: 92.97%] [G loss: 3.199508]\n",
      "1650 [D loss: 0.781413, acc.: 95.31%] [G loss: 3.271116]\n",
      "1651 [D loss: 0.769488, acc.: 95.31%] [G loss: 3.445100]\n",
      "1652 [D loss: 0.646702, acc.: 96.09%] [G loss: 3.276138]\n",
      "1653 [D loss: 1.275827, acc.: 92.19%] [G loss: 3.015292]\n",
      "1654 [D loss: 0.410334, acc.: 97.66%] [G loss: 3.274558]\n",
      "1655 [D loss: 0.531333, acc.: 96.88%] [G loss: 3.547596]\n",
      "1656 [D loss: 0.644422, acc.: 96.09%] [G loss: 4.109505]\n",
      "1657 [D loss: 0.391257, acc.: 97.66%] [G loss: 3.721592]\n",
      "1658 [D loss: 0.395558, acc.: 97.66%] [G loss: 3.108259]\n",
      "1659 [D loss: 0.649160, acc.: 96.09%] [G loss: 2.958109]\n",
      "1660 [D loss: 0.902126, acc.: 94.53%] [G loss: 3.212475]\n",
      "1661 [D loss: 0.535203, acc.: 96.09%] [G loss: 3.199914]\n",
      "1662 [D loss: 0.290367, acc.: 98.44%] [G loss: 3.426607]\n",
      "1663 [D loss: 0.525635, acc.: 96.88%] [G loss: 3.780119]\n",
      "1664 [D loss: 0.260871, acc.: 98.44%] [G loss: 4.033521]\n",
      "1665 [D loss: 0.516073, acc.: 96.88%] [G loss: 3.548401]\n",
      "1666 [D loss: 0.891173, acc.: 94.53%] [G loss: 3.720975]\n",
      "1667 [D loss: 0.676630, acc.: 95.31%] [G loss: 3.504896]\n",
      "1668 [D loss: 0.770154, acc.: 95.31%] [G loss: 3.324754]\n",
      "1669 [D loss: 0.774514, acc.: 95.31%] [G loss: 2.965316]\n",
      "1670 [D loss: 0.796422, acc.: 93.75%] [G loss: 2.908474]\n",
      "1671 [D loss: 1.052299, acc.: 93.75%] [G loss: 3.391740]\n",
      "1672 [D loss: 0.915174, acc.: 94.53%] [G loss: 3.694623]\n",
      "1673 [D loss: 0.515189, acc.: 96.88%] [G loss: 3.774069]\n",
      "1674 [D loss: 0.894596, acc.: 94.53%] [G loss: 3.490228]\n",
      "1675 [D loss: 0.888514, acc.: 94.53%] [G loss: 3.960680]\n",
      "1676 [D loss: 1.013074, acc.: 93.75%] [G loss: 4.532913]\n",
      "1677 [D loss: 0.386270, acc.: 97.66%] [G loss: 3.980038]\n",
      "1678 [D loss: 0.767020, acc.: 95.31%] [G loss: 3.394903]\n",
      "1679 [D loss: 0.263680, acc.: 98.44%] [G loss: 3.212170]\n",
      "1680 [D loss: 0.807252, acc.: 94.53%] [G loss: 2.992398]\n",
      "1681 [D loss: 0.534454, acc.: 96.09%] [G loss: 2.744052]\n",
      "1682 [D loss: 0.407060, acc.: 97.66%] [G loss: 2.255968]\n",
      "1683 [D loss: 0.400701, acc.: 97.66%] [G loss: 2.724089]\n",
      "1684 [D loss: 0.265602, acc.: 98.44%] [G loss: 3.158539]\n",
      "1685 [D loss: 0.137580, acc.: 99.22%] [G loss: 3.102993]\n",
      "1686 [D loss: 0.139950, acc.: 99.22%] [G loss: 2.894090]\n",
      "1687 [D loss: 1.024477, acc.: 93.75%] [G loss: 2.755377]\n",
      "1688 [D loss: 0.143483, acc.: 99.22%] [G loss: 2.688256]\n",
      "1689 [D loss: 0.895923, acc.: 94.53%] [G loss: 3.063263]\n",
      "1690 [D loss: 0.893361, acc.: 93.75%] [G loss: 3.740226]\n",
      "1691 [D loss: 1.014414, acc.: 93.75%] [G loss: 3.408771]\n",
      "1692 [D loss: 0.786584, acc.: 95.31%] [G loss: 2.821680]\n",
      "1693 [D loss: 0.157589, acc.: 99.22%] [G loss: 3.008582]\n",
      "1694 [D loss: 0.906193, acc.: 94.53%] [G loss: 3.451988]\n",
      "1695 [D loss: 0.545746, acc.: 96.88%] [G loss: 4.251790]\n",
      "1696 [D loss: 0.785226, acc.: 95.31%] [G loss: 4.624707]\n",
      "1697 [D loss: 0.506075, acc.: 96.88%] [G loss: 4.307486]\n",
      "1698 [D loss: 0.257504, acc.: 98.44%] [G loss: 3.499210]\n",
      "1699 [D loss: 0.636342, acc.: 96.09%] [G loss: 3.142583]\n",
      "1700 [D loss: 0.260378, acc.: 98.44%] [G loss: 3.008552]\n",
      "1701 [D loss: 0.894771, acc.: 94.53%] [G loss: 3.126339]\n",
      "1702 [D loss: 0.258664, acc.: 98.44%] [G loss: 3.484878]\n",
      "1703 [D loss: 0.896621, acc.: 93.75%] [G loss: 3.330211]\n",
      "1704 [D loss: 0.261886, acc.: 98.44%] [G loss: 3.368646]\n",
      "1705 [D loss: 0.012483, acc.: 100.00%] [G loss: 3.428766]\n",
      "1706 [D loss: 0.141019, acc.: 99.22%] [G loss: 3.632437]\n",
      "1707 [D loss: 0.393634, acc.: 97.66%] [G loss: 3.174090]\n",
      "1708 [D loss: 1.027976, acc.: 93.75%] [G loss: 2.718997]\n",
      "1709 [D loss: 0.396105, acc.: 97.66%] [G loss: 2.421113]\n",
      "1710 [D loss: 0.517536, acc.: 96.88%] [G loss: 2.609309]\n",
      "1711 [D loss: 0.513446, acc.: 96.88%] [G loss: 2.834294]\n",
      "1712 [D loss: 0.513860, acc.: 96.88%] [G loss: 2.898628]\n",
      "1713 [D loss: 0.512560, acc.: 96.88%] [G loss: 3.159684]\n",
      "1714 [D loss: 0.392287, acc.: 97.66%] [G loss: 2.875914]\n",
      "1715 [D loss: 0.521278, acc.: 96.88%] [G loss: 3.057055]\n",
      "1716 [D loss: 0.656930, acc.: 95.31%] [G loss: 3.249964]\n",
      "1717 [D loss: 0.649378, acc.: 96.09%] [G loss: 2.979009]\n",
      "1718 [D loss: 0.898589, acc.: 94.53%] [G loss: 2.730998]\n",
      "1719 [D loss: 0.766841, acc.: 95.31%] [G loss: 2.894883]\n",
      "1720 [D loss: 0.384546, acc.: 97.66%] [G loss: 3.341445]\n",
      "1721 [D loss: 0.890141, acc.: 94.53%] [G loss: 3.712509]\n",
      "1722 [D loss: 0.131703, acc.: 99.22%] [G loss: 3.658108]\n",
      "1723 [D loss: 0.390700, acc.: 97.66%] [G loss: 2.846599]\n",
      "1724 [D loss: 0.904793, acc.: 94.53%] [G loss: 2.463885]\n",
      "1725 [D loss: 0.266624, acc.: 98.44%] [G loss: 2.696519]\n",
      "1726 [D loss: 0.387852, acc.: 97.66%] [G loss: 2.779071]\n",
      "1727 [D loss: 0.640984, acc.: 96.09%] [G loss: 3.023067]\n",
      "1728 [D loss: 0.386749, acc.: 97.66%] [G loss: 3.008403]\n",
      "1729 [D loss: 0.010006, acc.: 100.00%] [G loss: 2.979397]\n",
      "1730 [D loss: 0.388245, acc.: 97.66%] [G loss: 3.086482]\n",
      "1731 [D loss: 0.386687, acc.: 97.66%] [G loss: 3.247252]\n",
      "1732 [D loss: 0.386144, acc.: 97.66%] [G loss: 3.357853]\n",
      "1733 [D loss: 0.892437, acc.: 94.53%] [G loss: 3.146223]\n",
      "1734 [D loss: 0.649674, acc.: 96.09%] [G loss: 2.684006]\n",
      "1735 [D loss: 0.548708, acc.: 96.88%] [G loss: 3.657983]\n",
      "1736 [D loss: 0.600703, acc.: 81.25%] [G loss: 8.117598]\n",
      "1737 [D loss: 1.316695, acc.: 76.56%] [G loss: 14.131546]\n",
      "1738 [D loss: 0.925784, acc.: 91.41%] [G loss: 14.832115]\n",
      "1739 [D loss: 0.881459, acc.: 94.53%] [G loss: 14.327043]\n",
      "1740 [D loss: 0.377781, acc.: 97.66%] [G loss: 13.214388]\n",
      "1741 [D loss: 0.629651, acc.: 96.09%] [G loss: 12.813793]\n",
      "1742 [D loss: 0.251946, acc.: 98.44%] [G loss: 12.263722]\n",
      "1743 [D loss: 0.629897, acc.: 96.09%] [G loss: 11.280169]\n",
      "1744 [D loss: 0.505241, acc.: 96.88%] [G loss: 10.633818]\n",
      "1745 [D loss: 0.379409, acc.: 97.66%] [G loss: 9.435505]\n",
      "1746 [D loss: 0.508946, acc.: 96.88%] [G loss: 8.481599]\n",
      "1747 [D loss: 0.384435, acc.: 97.66%] [G loss: 7.484530]\n",
      "1748 [D loss: 0.514568, acc.: 96.88%] [G loss: 6.422100]\n",
      "1749 [D loss: 0.510258, acc.: 96.88%] [G loss: 6.240613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750 [D loss: 0.890496, acc.: 94.53%] [G loss: 5.906577]\n",
      "1751 [D loss: 0.514146, acc.: 96.88%] [G loss: 5.546717]\n",
      "1752 [D loss: 0.261406, acc.: 98.44%] [G loss: 5.411427]\n",
      "1753 [D loss: 0.388429, acc.: 97.66%] [G loss: 5.198998]\n",
      "1754 [D loss: 0.635488, acc.: 96.09%] [G loss: 4.780179]\n",
      "1755 [D loss: 0.383555, acc.: 97.66%] [G loss: 5.059393]\n",
      "1756 [D loss: 0.385289, acc.: 97.66%] [G loss: 4.822477]\n",
      "1757 [D loss: 1.021072, acc.: 93.75%] [G loss: 4.160891]\n",
      "1758 [D loss: 1.019096, acc.: 93.75%] [G loss: 4.055942]\n",
      "1759 [D loss: 0.764250, acc.: 95.31%] [G loss: 3.400274]\n",
      "1760 [D loss: 0.639579, acc.: 96.09%] [G loss: 3.186845]\n",
      "1761 [D loss: 0.768803, acc.: 95.31%] [G loss: 3.091965]\n",
      "1762 [D loss: 0.389322, acc.: 97.66%] [G loss: 2.984588]\n",
      "1763 [D loss: 0.765818, acc.: 95.31%] [G loss: 2.986525]\n",
      "1764 [D loss: 0.767329, acc.: 95.31%] [G loss: 3.118134]\n",
      "1765 [D loss: 0.406475, acc.: 97.66%] [G loss: 2.917994]\n",
      "1766 [D loss: 0.792871, acc.: 95.31%] [G loss: 2.912718]\n",
      "1767 [D loss: 0.388594, acc.: 97.66%] [G loss: 2.882175]\n",
      "1768 [D loss: 0.628693, acc.: 92.97%] [G loss: 2.757946]\n",
      "1769 [D loss: 0.354895, acc.: 82.81%] [G loss: 4.127031]\n",
      "1770 [D loss: 1.954216, acc.: 69.53%] [G loss: 7.833405]\n",
      "1771 [D loss: 2.078784, acc.: 80.47%] [G loss: 10.967460]\n",
      "1772 [D loss: 0.000079, acc.: 100.00%] [G loss: 11.929002]\n",
      "1773 [D loss: 0.000428, acc.: 100.00%] [G loss: 10.614531]\n",
      "1774 [D loss: 0.128229, acc.: 99.22%] [G loss: 9.500463]\n",
      "1775 [D loss: 0.146595, acc.: 99.22%] [G loss: 9.129340]\n",
      "1776 [D loss: 0.160484, acc.: 97.66%] [G loss: 9.268879]\n",
      "1777 [D loss: 0.004860, acc.: 100.00%] [G loss: 8.731079]\n",
      "1778 [D loss: 0.263007, acc.: 98.44%] [G loss: 8.309057]\n",
      "1779 [D loss: 0.398356, acc.: 97.66%] [G loss: 7.315834]\n",
      "1780 [D loss: 0.135710, acc.: 99.22%] [G loss: 6.359907]\n",
      "1781 [D loss: 0.151218, acc.: 98.44%] [G loss: 5.428679]\n",
      "1782 [D loss: 0.019252, acc.: 99.22%] [G loss: 4.373568]\n",
      "1783 [D loss: 0.016259, acc.: 100.00%] [G loss: 3.482827]\n",
      "1784 [D loss: 0.025583, acc.: 100.00%] [G loss: 2.968440]\n",
      "1785 [D loss: 0.028446, acc.: 100.00%] [G loss: 2.400311]\n",
      "1786 [D loss: 0.049668, acc.: 99.22%] [G loss: 2.665848]\n",
      "1787 [D loss: 0.034761, acc.: 100.00%] [G loss: 2.333499]\n",
      "1788 [D loss: 0.313320, acc.: 98.44%] [G loss: 2.364624]\n",
      "1789 [D loss: 0.165088, acc.: 99.22%] [G loss: 2.170505]\n",
      "1790 [D loss: 0.571287, acc.: 94.53%] [G loss: 2.360719]\n",
      "1791 [D loss: 0.068830, acc.: 100.00%] [G loss: 2.688036]\n",
      "1792 [D loss: 0.214208, acc.: 93.75%] [G loss: 3.506770]\n",
      "1793 [D loss: 0.367424, acc.: 79.69%] [G loss: 7.016715]\n",
      "1794 [D loss: 0.281107, acc.: 97.66%] [G loss: 8.433607]\n",
      "1795 [D loss: 0.191423, acc.: 92.19%] [G loss: 6.215294]\n",
      "1796 [D loss: 0.306562, acc.: 89.06%] [G loss: 5.884566]\n",
      "1797 [D loss: 0.256746, acc.: 89.84%] [G loss: 6.179601]\n",
      "1798 [D loss: 0.003046, acc.: 100.00%] [G loss: 6.178815]\n",
      "1799 [D loss: 0.013394, acc.: 100.00%] [G loss: 5.197346]\n",
      "1800 [D loss: 0.021377, acc.: 100.00%] [G loss: 4.295659]\n",
      "1801 [D loss: 0.215088, acc.: 96.09%] [G loss: 5.187382]\n",
      "1802 [D loss: 0.012909, acc.: 100.00%] [G loss: 4.021304]\n",
      "1803 [D loss: 0.141468, acc.: 99.22%] [G loss: 3.471388]\n",
      "1804 [D loss: 0.188077, acc.: 97.66%] [G loss: 2.871585]\n",
      "1805 [D loss: 0.290538, acc.: 98.44%] [G loss: 2.395380]\n",
      "1806 [D loss: 0.359951, acc.: 94.53%] [G loss: 2.816674]\n",
      "1807 [D loss: 0.444735, acc.: 80.47%] [G loss: 5.529941]\n",
      "1808 [D loss: 0.892716, acc.: 83.59%] [G loss: 7.251029]\n",
      "1809 [D loss: 0.341734, acc.: 91.41%] [G loss: 7.635801]\n",
      "1810 [D loss: 0.255426, acc.: 98.44%] [G loss: 6.094624]\n",
      "1811 [D loss: 0.140675, acc.: 99.22%] [G loss: 4.594175]\n",
      "1812 [D loss: 0.285760, acc.: 98.44%] [G loss: 4.043146]\n",
      "1813 [D loss: 0.279746, acc.: 98.44%] [G loss: 3.694179]\n",
      "1814 [D loss: 0.532543, acc.: 96.88%] [G loss: 3.195048]\n",
      "1815 [D loss: 0.171139, acc.: 99.22%] [G loss: 3.132828]\n",
      "1816 [D loss: 0.170611, acc.: 99.22%] [G loss: 2.901515]\n",
      "1817 [D loss: 0.677875, acc.: 95.31%] [G loss: 3.031541]\n",
      "1818 [D loss: 0.552089, acc.: 96.09%] [G loss: 2.727455]\n",
      "1819 [D loss: 0.178583, acc.: 99.22%] [G loss: 2.733976]\n",
      "1820 [D loss: 0.167337, acc.: 99.22%] [G loss: 2.626950]\n",
      "1821 [D loss: 0.423394, acc.: 97.66%] [G loss: 2.557076]\n",
      "1822 [D loss: 0.182080, acc.: 99.22%] [G loss: 3.030372]\n",
      "1823 [D loss: 0.413141, acc.: 96.88%] [G loss: 3.806490]\n",
      "1824 [D loss: 0.176683, acc.: 99.22%] [G loss: 3.626746]\n",
      "1825 [D loss: 0.272688, acc.: 98.44%] [G loss: 3.033190]\n",
      "1826 [D loss: 0.158553, acc.: 99.22%] [G loss: 2.716676]\n",
      "1827 [D loss: 0.424083, acc.: 97.66%] [G loss: 2.934915]\n",
      "1828 [D loss: 0.291796, acc.: 98.44%] [G loss: 3.011371]\n",
      "1829 [D loss: 0.281789, acc.: 98.44%] [G loss: 2.989329]\n",
      "1830 [D loss: 0.162066, acc.: 99.22%] [G loss: 2.961110]\n",
      "1831 [D loss: 0.684419, acc.: 92.97%] [G loss: 3.233073]\n",
      "1832 [D loss: 0.198096, acc.: 97.66%] [G loss: 3.126670]\n",
      "1833 [D loss: 0.137943, acc.: 92.19%] [G loss: 3.984755]\n",
      "1834 [D loss: 0.885576, acc.: 75.78%] [G loss: 6.600871]\n",
      "1835 [D loss: 1.367409, acc.: 78.91%] [G loss: 8.896297]\n",
      "1836 [D loss: 0.260839, acc.: 98.44%] [G loss: 8.783075]\n",
      "1837 [D loss: 0.556075, acc.: 94.53%] [G loss: 8.651007]\n",
      "1838 [D loss: 0.400846, acc.: 97.66%] [G loss: 7.816633]\n",
      "1839 [D loss: 0.141221, acc.: 99.22%] [G loss: 7.060932]\n",
      "1840 [D loss: 0.156691, acc.: 99.22%] [G loss: 6.388929]\n",
      "1841 [D loss: 0.422932, acc.: 97.66%] [G loss: 6.938772]\n",
      "1842 [D loss: 0.135387, acc.: 99.22%] [G loss: 6.303783]\n",
      "1843 [D loss: 0.137938, acc.: 99.22%] [G loss: 5.629725]\n",
      "1844 [D loss: 0.305856, acc.: 96.88%] [G loss: 6.152190]\n",
      "1845 [D loss: 0.385766, acc.: 97.66%] [G loss: 4.947242]\n",
      "1846 [D loss: 0.145072, acc.: 99.22%] [G loss: 4.983175]\n",
      "1847 [D loss: 0.665259, acc.: 96.09%] [G loss: 3.898314]\n",
      "1848 [D loss: 0.561677, acc.: 96.09%] [G loss: 3.938508]\n",
      "1849 [D loss: 0.264387, acc.: 98.44%] [G loss: 3.639953]\n",
      "1850 [D loss: 0.275247, acc.: 98.44%] [G loss: 3.429959]\n",
      "1851 [D loss: 0.547142, acc.: 96.88%] [G loss: 3.648131]\n",
      "1852 [D loss: 0.408098, acc.: 97.66%] [G loss: 3.389161]\n",
      "1853 [D loss: 0.274662, acc.: 98.44%] [G loss: 2.897401]\n",
      "1854 [D loss: 0.304714, acc.: 98.44%] [G loss: 3.118627]\n",
      "1855 [D loss: 0.042990, acc.: 99.22%] [G loss: 2.922946]\n",
      "1856 [D loss: 0.192454, acc.: 98.44%] [G loss: 3.474456]\n",
      "1857 [D loss: 0.397938, acc.: 97.66%] [G loss: 3.827402]\n",
      "1858 [D loss: 0.522434, acc.: 96.88%] [G loss: 2.982307]\n",
      "1859 [D loss: 0.405021, acc.: 97.66%] [G loss: 2.655776]\n",
      "1860 [D loss: 0.165351, acc.: 98.44%] [G loss: 2.470275]\n",
      "1861 [D loss: 0.301859, acc.: 98.44%] [G loss: 2.932644]\n",
      "1862 [D loss: 0.303134, acc.: 96.88%] [G loss: 3.166610]\n",
      "1863 [D loss: 0.582442, acc.: 96.09%] [G loss: 3.510226]\n",
      "1864 [D loss: 0.600341, acc.: 93.75%] [G loss: 4.651052]\n",
      "1865 [D loss: 0.156392, acc.: 99.22%] [G loss: 4.797156]\n",
      "1866 [D loss: 0.004087, acc.: 100.00%] [G loss: 4.231913]\n",
      "1867 [D loss: 0.131780, acc.: 99.22%] [G loss: 3.601387]\n",
      "1868 [D loss: 0.392932, acc.: 97.66%] [G loss: 3.878912]\n",
      "1869 [D loss: 0.642555, acc.: 96.09%] [G loss: 3.779965]\n",
      "1870 [D loss: 0.268146, acc.: 98.44%] [G loss: 3.069744]\n",
      "1871 [D loss: 0.151184, acc.: 99.22%] [G loss: 2.535829]\n",
      "1872 [D loss: 0.782092, acc.: 95.31%] [G loss: 2.767255]\n",
      "1873 [D loss: 0.398100, acc.: 97.66%] [G loss: 2.697343]\n",
      "1874 [D loss: 0.144445, acc.: 99.22%] [G loss: 2.944315]\n",
      "1875 [D loss: 0.520486, acc.: 96.88%] [G loss: 3.338651]\n",
      "1876 [D loss: 0.520090, acc.: 96.88%] [G loss: 3.266914]\n",
      "1877 [D loss: 0.143747, acc.: 99.22%] [G loss: 2.912112]\n",
      "1878 [D loss: 0.429708, acc.: 96.88%] [G loss: 2.995594]\n",
      "1879 [D loss: 0.168940, acc.: 98.44%] [G loss: 3.353932]\n",
      "1880 [D loss: 0.281579, acc.: 98.44%] [G loss: 3.403948]\n",
      "1881 [D loss: 0.137948, acc.: 99.22%] [G loss: 3.379076]\n",
      "1882 [D loss: 0.651159, acc.: 96.09%] [G loss: 2.890793]\n",
      "1883 [D loss: 0.402232, acc.: 97.66%] [G loss: 3.059688]\n",
      "1884 [D loss: 0.268858, acc.: 97.66%] [G loss: 2.841339]\n",
      "1885 [D loss: 0.140020, acc.: 99.22%] [G loss: 2.855946]\n",
      "1886 [D loss: 0.136547, acc.: 99.22%] [G loss: 2.654586]\n",
      "1887 [D loss: 0.420628, acc.: 97.66%] [G loss: 2.544889]\n",
      "1888 [D loss: 0.831783, acc.: 94.53%] [G loss: 3.492544]\n",
      "1889 [D loss: 0.636318, acc.: 83.59%] [G loss: 6.312840]\n",
      "1890 [D loss: 1.198390, acc.: 81.25%] [G loss: 9.927696]\n",
      "1891 [D loss: 0.377843, acc.: 97.66%] [G loss: 9.902174]\n",
      "1892 [D loss: 0.756700, acc.: 95.31%] [G loss: 8.339804]\n",
      "1893 [D loss: 0.640191, acc.: 96.09%] [G loss: 6.658609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894 [D loss: 0.522620, acc.: 96.88%] [G loss: 6.369873]\n",
      "1895 [D loss: 0.278463, acc.: 97.66%] [G loss: 6.175148]\n",
      "1896 [D loss: 0.258002, acc.: 98.44%] [G loss: 4.949441]\n",
      "1897 [D loss: 0.509548, acc.: 96.88%] [G loss: 4.562176]\n",
      "1898 [D loss: 0.136426, acc.: 99.22%] [G loss: 4.088519]\n",
      "1899 [D loss: 0.383924, acc.: 97.66%] [G loss: 4.158783]\n",
      "1900 [D loss: 0.511974, acc.: 96.88%] [G loss: 4.116891]\n",
      "1901 [D loss: 0.134845, acc.: 99.22%] [G loss: 3.887917]\n",
      "1902 [D loss: 0.394706, acc.: 97.66%] [G loss: 3.244633]\n",
      "1903 [D loss: 0.399031, acc.: 97.66%] [G loss: 3.056710]\n",
      "1904 [D loss: 0.515559, acc.: 94.53%] [G loss: 2.519110]\n",
      "1905 [D loss: 0.292973, acc.: 92.97%] [G loss: 2.324840]\n",
      "1906 [D loss: 0.400361, acc.: 89.06%] [G loss: 5.416085]\n",
      "1907 [D loss: 1.416712, acc.: 71.88%] [G loss: 7.873730]\n",
      "1908 [D loss: 0.028145, acc.: 100.00%] [G loss: 7.449153]\n",
      "1909 [D loss: 0.142498, acc.: 89.06%] [G loss: 10.288149]\n",
      "1910 [D loss: 0.001717, acc.: 100.00%] [G loss: 9.770560]\n",
      "1911 [D loss: 0.260541, acc.: 97.66%] [G loss: 8.363694]\n",
      "1912 [D loss: 0.204489, acc.: 93.75%] [G loss: 6.981845]\n",
      "1913 [D loss: 0.158981, acc.: 98.44%] [G loss: 5.999262]\n",
      "1914 [D loss: 0.029447, acc.: 100.00%] [G loss: 5.270873]\n",
      "1915 [D loss: 0.020156, acc.: 100.00%] [G loss: 5.820152]\n",
      "1916 [D loss: 0.031425, acc.: 100.00%] [G loss: 5.411129]\n",
      "1917 [D loss: 0.052268, acc.: 99.22%] [G loss: 4.923429]\n",
      "1918 [D loss: 0.271617, acc.: 98.44%] [G loss: 4.335064]\n",
      "1919 [D loss: 0.047428, acc.: 100.00%] [G loss: 3.709001]\n",
      "1920 [D loss: 0.178220, acc.: 98.44%] [G loss: 3.867875]\n",
      "1921 [D loss: 0.038231, acc.: 99.22%] [G loss: 3.505269]\n",
      "1922 [D loss: 0.078436, acc.: 96.88%] [G loss: 3.164809]\n",
      "1923 [D loss: 0.044263, acc.: 99.22%] [G loss: 2.586569]\n",
      "1924 [D loss: 0.295649, acc.: 98.44%] [G loss: 1.992270]\n",
      "1925 [D loss: 0.212505, acc.: 97.66%] [G loss: 1.883682]\n",
      "1926 [D loss: 0.093729, acc.: 96.88%] [G loss: 2.303242]\n",
      "1927 [D loss: 0.069540, acc.: 97.66%] [G loss: 2.499732]\n",
      "1928 [D loss: 0.263761, acc.: 96.09%] [G loss: 3.414211]\n",
      "1929 [D loss: 0.157298, acc.: 90.62%] [G loss: 3.988529]\n",
      "1930 [D loss: 0.552103, acc.: 78.91%] [G loss: 5.634943]\n",
      "1931 [D loss: 0.778883, acc.: 80.47%] [G loss: 7.672881]\n",
      "1932 [D loss: 0.134011, acc.: 99.22%] [G loss: 7.721238]\n",
      "1933 [D loss: 0.263586, acc.: 98.44%] [G loss: 6.065271]\n",
      "1934 [D loss: 0.172609, acc.: 97.66%] [G loss: 4.468992]\n",
      "1935 [D loss: 0.297188, acc.: 98.44%] [G loss: 3.711288]\n",
      "1936 [D loss: 0.086569, acc.: 96.09%] [G loss: 3.373450]\n",
      "1937 [D loss: 0.067603, acc.: 98.44%] [G loss: 2.481882]\n",
      "1938 [D loss: 0.226178, acc.: 96.09%] [G loss: 2.270074]\n",
      "1939 [D loss: 0.268875, acc.: 92.19%] [G loss: 2.798943]\n",
      "1940 [D loss: 0.172268, acc.: 89.06%] [G loss: 3.103553]\n",
      "1941 [D loss: 0.365687, acc.: 80.47%] [G loss: 4.100107]\n",
      "1942 [D loss: 0.182477, acc.: 90.62%] [G loss: 5.272485]\n",
      "1943 [D loss: 0.026100, acc.: 100.00%] [G loss: 5.320646]\n",
      "1944 [D loss: 0.157266, acc.: 98.44%] [G loss: 4.075701]\n",
      "1945 [D loss: 0.543079, acc.: 96.88%] [G loss: 3.407291]\n",
      "1946 [D loss: 0.054440, acc.: 100.00%] [G loss: 3.105398]\n",
      "1947 [D loss: 0.280261, acc.: 98.44%] [G loss: 2.514894]\n",
      "1948 [D loss: 0.354707, acc.: 96.88%] [G loss: 3.099945]\n",
      "1949 [D loss: 0.558390, acc.: 96.09%] [G loss: 3.159834]\n",
      "1950 [D loss: 0.248047, acc.: 92.97%] [G loss: 3.658216]\n",
      "1951 [D loss: 0.169159, acc.: 97.66%] [G loss: 3.689626]\n",
      "1952 [D loss: 0.158147, acc.: 99.22%] [G loss: 2.660707]\n",
      "1953 [D loss: 0.243371, acc.: 96.09%] [G loss: 2.099792]\n",
      "1954 [D loss: 0.267824, acc.: 93.75%] [G loss: 3.609914]\n",
      "1955 [D loss: 0.263892, acc.: 89.84%] [G loss: 3.409480]\n",
      "1956 [D loss: 0.197732, acc.: 87.50%] [G loss: 5.097379]\n",
      "1957 [D loss: 0.017208, acc.: 100.00%] [G loss: 5.818537]\n",
      "1958 [D loss: 0.163463, acc.: 97.66%] [G loss: 4.818041]\n",
      "1959 [D loss: 0.274630, acc.: 98.44%] [G loss: 3.950366]\n",
      "1960 [D loss: 0.189717, acc.: 95.31%] [G loss: 4.164640]\n",
      "1961 [D loss: 0.521338, acc.: 96.88%] [G loss: 3.766752]\n",
      "1962 [D loss: 0.034114, acc.: 100.00%] [G loss: 2.822202]\n",
      "1963 [D loss: 0.173483, acc.: 99.22%] [G loss: 2.415107]\n",
      "1964 [D loss: 0.233237, acc.: 96.88%] [G loss: 2.974123]\n",
      "1965 [D loss: 0.535914, acc.: 95.31%] [G loss: 2.848678]\n",
      "1966 [D loss: 0.154398, acc.: 99.22%] [G loss: 2.532609]\n",
      "1967 [D loss: 0.295107, acc.: 97.66%] [G loss: 2.915477]\n",
      "1968 [D loss: 0.646544, acc.: 96.09%] [G loss: 2.792971]\n",
      "1969 [D loss: 0.302817, acc.: 97.66%] [G loss: 2.632422]\n",
      "1970 [D loss: 0.293652, acc.: 98.44%] [G loss: 2.667265]\n",
      "1971 [D loss: 0.150596, acc.: 99.22%] [G loss: 2.487280]\n",
      "1972 [D loss: 0.157408, acc.: 99.22%] [G loss: 3.463645]\n",
      "1973 [D loss: 0.010752, acc.: 100.00%] [G loss: 4.069517]\n",
      "1974 [D loss: 0.183312, acc.: 96.09%] [G loss: 3.697370]\n",
      "1975 [D loss: 0.196858, acc.: 88.28%] [G loss: 4.380400]\n",
      "1976 [D loss: 0.772634, acc.: 84.38%] [G loss: 6.186965]\n",
      "1977 [D loss: 0.252106, acc.: 84.38%] [G loss: 8.227569]\n",
      "1978 [D loss: 0.252128, acc.: 98.44%] [G loss: 8.143413]\n",
      "1979 [D loss: 0.127742, acc.: 99.22%] [G loss: 6.202394]\n",
      "1980 [D loss: 0.147052, acc.: 99.22%] [G loss: 5.009130]\n",
      "1981 [D loss: 0.266124, acc.: 98.44%] [G loss: 4.688648]\n",
      "1982 [D loss: 0.133123, acc.: 99.22%] [G loss: 3.672151]\n",
      "1983 [D loss: 0.274595, acc.: 98.44%] [G loss: 3.309991]\n",
      "1984 [D loss: 0.656601, acc.: 96.09%] [G loss: 3.400508]\n",
      "1985 [D loss: 0.515306, acc.: 96.88%] [G loss: 3.127650]\n",
      "1986 [D loss: 0.790638, acc.: 95.31%] [G loss: 2.979142]\n",
      "1987 [D loss: 0.520063, acc.: 96.88%] [G loss: 2.995704]\n",
      "1988 [D loss: 0.404169, acc.: 97.66%] [G loss: 2.917176]\n",
      "1989 [D loss: 0.275352, acc.: 98.44%] [G loss: 2.658321]\n",
      "1990 [D loss: 0.527922, acc.: 96.88%] [G loss: 2.964361]\n",
      "1991 [D loss: 0.516752, acc.: 96.88%] [G loss: 3.470931]\n",
      "1992 [D loss: 0.266545, acc.: 98.44%] [G loss: 3.436771]\n",
      "1993 [D loss: 0.800150, acc.: 95.31%] [G loss: 2.684729]\n",
      "1994 [D loss: 0.822463, acc.: 95.31%] [G loss: 3.405173]\n",
      "1995 [D loss: 0.756294, acc.: 88.28%] [G loss: 3.171618]\n",
      "1996 [D loss: 0.429495, acc.: 97.66%] [G loss: 3.643567]\n",
      "1997 [D loss: 0.256201, acc.: 98.44%] [G loss: 4.147925]\n",
      "1998 [D loss: 0.509457, acc.: 96.88%] [G loss: 3.656152]\n",
      "1999 [D loss: 0.770450, acc.: 95.31%] [G loss: 2.942176]\n",
      "2000 [D loss: 0.404937, acc.: 97.66%] [G loss: 2.706333]\n",
      "2001 [D loss: 0.525712, acc.: 96.88%] [G loss: 2.760757]\n",
      "2002 [D loss: 0.018520, acc.: 100.00%] [G loss: 2.550274]\n",
      "2003 [D loss: 0.282059, acc.: 98.44%] [G loss: 2.594666]\n",
      "2004 [D loss: 0.525993, acc.: 96.88%] [G loss: 2.930875]\n",
      "2005 [D loss: 0.266377, acc.: 98.44%] [G loss: 3.170158]\n",
      "2006 [D loss: 0.905977, acc.: 94.53%] [G loss: 2.942047]\n",
      "2007 [D loss: 0.534432, acc.: 96.88%] [G loss: 2.728904]\n",
      "2008 [D loss: 0.907171, acc.: 93.75%] [G loss: 2.306774]\n",
      "2009 [D loss: 0.445891, acc.: 96.88%] [G loss: 2.231647]\n",
      "2010 [D loss: 0.526069, acc.: 96.88%] [G loss: 2.750027]\n",
      "2011 [D loss: 0.521550, acc.: 96.88%] [G loss: 2.879140]\n",
      "2012 [D loss: 0.394199, acc.: 97.66%] [G loss: 3.002874]\n",
      "2013 [D loss: 0.393183, acc.: 97.66%] [G loss: 2.779814]\n",
      "2014 [D loss: 0.646344, acc.: 96.09%] [G loss: 2.523760]\n",
      "2015 [D loss: 0.151334, acc.: 99.22%] [G loss: 2.337807]\n",
      "2016 [D loss: 0.557057, acc.: 95.31%] [G loss: 2.817503]\n",
      "2017 [D loss: 0.531429, acc.: 91.41%] [G loss: 5.250046]\n",
      "2018 [D loss: 0.547149, acc.: 89.84%] [G loss: 7.281161]\n",
      "2019 [D loss: 0.802936, acc.: 92.97%] [G loss: 6.805031]\n",
      "2020 [D loss: 0.884399, acc.: 94.53%] [G loss: 5.503278]\n",
      "2021 [D loss: 0.520530, acc.: 96.88%] [G loss: 4.816385]\n",
      "2022 [D loss: 0.888336, acc.: 94.53%] [G loss: 4.354496]\n",
      "2023 [D loss: 0.386826, acc.: 97.66%] [G loss: 3.656662]\n",
      "2024 [D loss: 0.389619, acc.: 97.66%] [G loss: 3.310491]\n",
      "2025 [D loss: 0.640818, acc.: 96.09%] [G loss: 3.059956]\n",
      "2026 [D loss: 0.393444, acc.: 97.66%] [G loss: 2.958133]\n",
      "2027 [D loss: 0.586022, acc.: 94.53%] [G loss: 2.063093]\n",
      "2028 [D loss: 0.109887, acc.: 96.09%] [G loss: 2.517995]\n",
      "2029 [D loss: 0.219884, acc.: 91.41%] [G loss: 3.135469]\n",
      "2030 [D loss: 0.504652, acc.: 80.47%] [G loss: 5.923635]\n",
      "2031 [D loss: 0.052159, acc.: 97.66%] [G loss: 7.195186]\n",
      "2032 [D loss: 0.024140, acc.: 99.22%] [G loss: 6.177563]\n",
      "2033 [D loss: 0.140188, acc.: 99.22%] [G loss: 4.632106]\n",
      "2034 [D loss: 0.150668, acc.: 98.44%] [G loss: 3.855566]\n",
      "2035 [D loss: 0.017523, acc.: 100.00%] [G loss: 3.408106]\n",
      "2036 [D loss: 0.157956, acc.: 98.44%] [G loss: 3.025344]\n",
      "2037 [D loss: 0.044664, acc.: 98.44%] [G loss: 2.569206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038 [D loss: 0.292663, acc.: 98.44%] [G loss: 2.081869]\n",
      "2039 [D loss: 0.179486, acc.: 99.22%] [G loss: 2.245575]\n",
      "2040 [D loss: 0.163512, acc.: 99.22%] [G loss: 2.037573]\n",
      "2041 [D loss: 0.061840, acc.: 99.22%] [G loss: 2.102753]\n",
      "2042 [D loss: 0.061785, acc.: 96.88%] [G loss: 2.097381]\n",
      "2043 [D loss: 0.290444, acc.: 88.28%] [G loss: 3.824034]\n",
      "2044 [D loss: 0.369856, acc.: 85.94%] [G loss: 5.443656]\n",
      "2045 [D loss: 0.047205, acc.: 98.44%] [G loss: 4.497663]\n",
      "2046 [D loss: 0.164615, acc.: 98.44%] [G loss: 4.367145]\n",
      "2047 [D loss: 0.031619, acc.: 100.00%] [G loss: 3.850348]\n",
      "2048 [D loss: 0.018786, acc.: 100.00%] [G loss: 2.725895]\n",
      "2049 [D loss: 0.059098, acc.: 97.66%] [G loss: 2.783548]\n",
      "2050 [D loss: 0.204128, acc.: 97.66%] [G loss: 2.417653]\n",
      "2051 [D loss: 0.210237, acc.: 97.66%] [G loss: 2.954684]\n",
      "2052 [D loss: 0.086723, acc.: 98.44%] [G loss: 3.055507]\n",
      "2053 [D loss: 0.416548, acc.: 93.75%] [G loss: 3.220129]\n",
      "2054 [D loss: 0.408815, acc.: 89.06%] [G loss: 5.314135]\n",
      "2055 [D loss: 0.148032, acc.: 99.22%] [G loss: 4.500598]\n",
      "2056 [D loss: 0.164383, acc.: 99.22%] [G loss: 3.389919]\n",
      "2057 [D loss: 0.252300, acc.: 94.53%] [G loss: 3.370120]\n",
      "2058 [D loss: 0.196662, acc.: 96.09%] [G loss: 3.069614]\n",
      "2059 [D loss: 0.235301, acc.: 95.31%] [G loss: 3.150045]\n",
      "2060 [D loss: 0.080058, acc.: 96.09%] [G loss: 2.667829]\n",
      "2061 [D loss: 0.310106, acc.: 89.84%] [G loss: 4.041739]\n",
      "2062 [D loss: 0.230023, acc.: 94.53%] [G loss: 4.463997]\n",
      "2063 [D loss: 0.190221, acc.: 96.88%] [G loss: 3.209201]\n",
      "2064 [D loss: 0.219969, acc.: 96.88%] [G loss: 3.160589]\n",
      "2065 [D loss: 0.179179, acc.: 97.66%] [G loss: 3.136176]\n",
      "2066 [D loss: 0.200503, acc.: 96.88%] [G loss: 3.397872]\n",
      "2067 [D loss: 0.168224, acc.: 98.44%] [G loss: 3.346594]\n",
      "2068 [D loss: 0.332820, acc.: 97.66%] [G loss: 2.827532]\n",
      "2069 [D loss: 0.198183, acc.: 96.09%] [G loss: 2.278990]\n",
      "2070 [D loss: 0.398234, acc.: 92.97%] [G loss: 3.067048]\n",
      "2071 [D loss: 0.600872, acc.: 91.41%] [G loss: 3.247285]\n",
      "2072 [D loss: 0.297347, acc.: 83.59%] [G loss: 6.806402]\n",
      "2073 [D loss: 0.094408, acc.: 95.31%] [G loss: 7.856125]\n",
      "2074 [D loss: 0.262481, acc.: 98.44%] [G loss: 6.272436]\n",
      "2075 [D loss: 0.023647, acc.: 99.22%] [G loss: 4.751430]\n",
      "2076 [D loss: 0.290536, acc.: 98.44%] [G loss: 3.494100]\n",
      "2077 [D loss: 0.677758, acc.: 94.53%] [G loss: 2.786923]\n",
      "2078 [D loss: 0.424783, acc.: 97.66%] [G loss: 2.675235]\n",
      "2079 [D loss: 0.484509, acc.: 92.97%] [G loss: 3.069138]\n",
      "2080 [D loss: 0.401330, acc.: 92.19%] [G loss: 3.928426]\n",
      "2081 [D loss: 0.170202, acc.: 96.88%] [G loss: 4.330638]\n",
      "2082 [D loss: 0.167187, acc.: 98.44%] [G loss: 3.840815]\n",
      "2083 [D loss: 0.304220, acc.: 96.88%] [G loss: 3.726979]\n",
      "2084 [D loss: 0.398268, acc.: 97.66%] [G loss: 3.110715]\n",
      "2085 [D loss: 0.314497, acc.: 97.66%] [G loss: 2.330662]\n",
      "2086 [D loss: 0.755084, acc.: 89.84%] [G loss: 2.863479]\n",
      "2087 [D loss: 0.634295, acc.: 89.84%] [G loss: 4.907016]\n",
      "2088 [D loss: 0.840885, acc.: 82.81%] [G loss: 5.526626]\n",
      "2089 [D loss: 0.422530, acc.: 96.09%] [G loss: 6.218114]\n",
      "2090 [D loss: 0.640428, acc.: 96.09%] [G loss: 5.317073]\n",
      "2091 [D loss: 0.266841, acc.: 98.44%] [G loss: 4.848281]\n",
      "2092 [D loss: 0.278302, acc.: 98.44%] [G loss: 4.247935]\n",
      "2093 [D loss: 0.780258, acc.: 94.53%] [G loss: 3.788320]\n",
      "2094 [D loss: 0.647483, acc.: 89.06%] [G loss: 3.178144]\n",
      "2095 [D loss: 0.241713, acc.: 86.72%] [G loss: 2.907711]\n",
      "2096 [D loss: 0.181475, acc.: 90.62%] [G loss: 5.080112]\n",
      "2097 [D loss: 0.415234, acc.: 81.25%] [G loss: 6.229531]\n",
      "2098 [D loss: 0.013716, acc.: 100.00%] [G loss: 5.098993]\n",
      "2099 [D loss: 0.203460, acc.: 89.84%] [G loss: 7.529441]\n",
      "2100 [D loss: 0.125317, acc.: 93.75%] [G loss: 7.016812]\n",
      "2101 [D loss: 0.083169, acc.: 96.88%] [G loss: 6.814754]\n",
      "2102 [D loss: 0.053192, acc.: 99.22%] [G loss: 5.889854]\n",
      "2103 [D loss: 0.026816, acc.: 100.00%] [G loss: 5.097102]\n",
      "2104 [D loss: 0.287185, acc.: 98.44%] [G loss: 3.938636]\n",
      "2105 [D loss: 0.192250, acc.: 97.66%] [G loss: 3.541243]\n",
      "2106 [D loss: 0.073931, acc.: 98.44%] [G loss: 2.892136]\n",
      "2107 [D loss: 0.238835, acc.: 96.09%] [G loss: 3.148752]\n",
      "2108 [D loss: 0.095708, acc.: 94.53%] [G loss: 2.666065]\n",
      "2109 [D loss: 0.341688, acc.: 85.16%] [G loss: 3.488376]\n",
      "2110 [D loss: 0.452322, acc.: 91.41%] [G loss: 3.160839]\n",
      "2111 [D loss: 0.254401, acc.: 95.31%] [G loss: 3.240578]\n",
      "2112 [D loss: 0.039445, acc.: 100.00%] [G loss: 3.205565]\n",
      "2113 [D loss: 0.058342, acc.: 100.00%] [G loss: 3.000207]\n",
      "2114 [D loss: 0.064458, acc.: 99.22%] [G loss: 2.205640]\n",
      "2115 [D loss: 0.164505, acc.: 92.97%] [G loss: 2.403394]\n",
      "2116 [D loss: 0.120170, acc.: 95.31%] [G loss: 3.741828]\n",
      "2117 [D loss: 0.235288, acc.: 88.28%] [G loss: 3.877455]\n",
      "2118 [D loss: 0.246022, acc.: 93.75%] [G loss: 4.402163]\n",
      "2119 [D loss: 0.151263, acc.: 99.22%] [G loss: 3.725246]\n",
      "2120 [D loss: 0.216210, acc.: 96.09%] [G loss: 2.992963]\n",
      "2121 [D loss: 0.308994, acc.: 97.66%] [G loss: 2.899541]\n",
      "2122 [D loss: 0.224453, acc.: 96.88%] [G loss: 2.914595]\n",
      "2123 [D loss: 0.200825, acc.: 96.09%] [G loss: 2.773385]\n",
      "2124 [D loss: 0.079643, acc.: 99.22%] [G loss: 2.940883]\n",
      "2125 [D loss: 0.179897, acc.: 97.66%] [G loss: 2.592895]\n",
      "2126 [D loss: 0.395242, acc.: 92.97%] [G loss: 2.996674]\n",
      "2127 [D loss: 0.751245, acc.: 75.78%] [G loss: 5.869099]\n",
      "2128 [D loss: 0.953084, acc.: 84.38%] [G loss: 7.359493]\n",
      "2129 [D loss: 0.131101, acc.: 99.22%] [G loss: 6.230203]\n",
      "2130 [D loss: 0.758666, acc.: 90.62%] [G loss: 5.358600]\n",
      "2131 [D loss: 0.457761, acc.: 96.09%] [G loss: 5.452821]\n",
      "2132 [D loss: 0.292357, acc.: 97.66%] [G loss: 5.848948]\n",
      "2133 [D loss: 0.397226, acc.: 97.66%] [G loss: 5.253710]\n",
      "2134 [D loss: 0.432916, acc.: 97.66%] [G loss: 4.737011]\n",
      "2135 [D loss: 0.414033, acc.: 96.88%] [G loss: 4.445924]\n",
      "2136 [D loss: 0.288737, acc.: 98.44%] [G loss: 4.244844]\n",
      "2137 [D loss: 0.146087, acc.: 99.22%] [G loss: 4.014527]\n",
      "2138 [D loss: 0.809999, acc.: 93.75%] [G loss: 3.310432]\n",
      "2139 [D loss: 0.443555, acc.: 96.88%] [G loss: 3.465647]\n",
      "2140 [D loss: 0.533390, acc.: 96.88%] [G loss: 2.953274]\n",
      "2141 [D loss: 0.176183, acc.: 99.22%] [G loss: 2.823915]\n",
      "2142 [D loss: 0.439255, acc.: 97.66%] [G loss: 3.286812]\n",
      "2143 [D loss: 0.158221, acc.: 99.22%] [G loss: 3.022605]\n",
      "2144 [D loss: 0.441674, acc.: 96.88%] [G loss: 3.576772]\n",
      "2145 [D loss: 0.017350, acc.: 100.00%] [G loss: 3.338500]\n",
      "2146 [D loss: 0.414597, acc.: 97.66%] [G loss: 3.052230]\n",
      "2147 [D loss: 0.281205, acc.: 98.44%] [G loss: 2.944335]\n",
      "2148 [D loss: 0.302679, acc.: 97.66%] [G loss: 2.407655]\n",
      "2149 [D loss: 0.343440, acc.: 97.66%] [G loss: 2.765419]\n",
      "2150 [D loss: 0.421150, acc.: 96.09%] [G loss: 2.465436]\n",
      "2151 [D loss: 0.162211, acc.: 99.22%] [G loss: 2.468664]\n",
      "2152 [D loss: 0.401646, acc.: 97.66%] [G loss: 2.885924]\n",
      "2153 [D loss: 0.143554, acc.: 99.22%] [G loss: 2.874989]\n",
      "2154 [D loss: 0.394929, acc.: 97.66%] [G loss: 2.654901]\n",
      "2155 [D loss: 0.185071, acc.: 98.44%] [G loss: 2.807628]\n",
      "2156 [D loss: 0.291211, acc.: 97.66%] [G loss: 2.771912]\n",
      "2157 [D loss: 0.544283, acc.: 96.88%] [G loss: 2.360693]\n",
      "2158 [D loss: 0.423153, acc.: 97.66%] [G loss: 2.571811]\n",
      "2159 [D loss: 0.816863, acc.: 92.97%] [G loss: 2.526609]\n",
      "2160 [D loss: 0.435904, acc.: 96.88%] [G loss: 2.743130]\n",
      "2161 [D loss: 0.519519, acc.: 94.53%] [G loss: 1.186163]\n",
      "2162 [D loss: 0.216334, acc.: 92.97%] [G loss: 2.516693]\n",
      "2163 [D loss: 0.041225, acc.: 98.44%] [G loss: 3.297860]\n",
      "2164 [D loss: 0.061023, acc.: 100.00%] [G loss: 2.716117]\n",
      "2165 [D loss: 0.037944, acc.: 99.22%] [G loss: 2.572926]\n",
      "2166 [D loss: 0.073394, acc.: 97.66%] [G loss: 2.414830]\n",
      "2167 [D loss: 0.219670, acc.: 85.16%] [G loss: 6.135793]\n",
      "2168 [D loss: 0.693682, acc.: 86.72%] [G loss: 6.579336]\n",
      "2169 [D loss: 0.080933, acc.: 98.44%] [G loss: 5.676225]\n",
      "2170 [D loss: 0.164877, acc.: 99.22%] [G loss: 4.626700]\n",
      "2171 [D loss: 0.064419, acc.: 98.44%] [G loss: 4.019027]\n",
      "2172 [D loss: 0.100583, acc.: 95.31%] [G loss: 3.641611]\n",
      "2173 [D loss: 0.237948, acc.: 89.06%] [G loss: 4.818994]\n",
      "2174 [D loss: 0.159479, acc.: 97.66%] [G loss: 4.323998]\n",
      "2175 [D loss: 0.067241, acc.: 99.22%] [G loss: 3.396408]\n",
      "2176 [D loss: 0.369841, acc.: 87.50%] [G loss: 3.636451]\n",
      "2177 [D loss: 0.200195, acc.: 96.09%] [G loss: 3.565093]\n",
      "2178 [D loss: 0.241652, acc.: 95.31%] [G loss: 3.289479]\n",
      "2179 [D loss: 0.560051, acc.: 84.38%] [G loss: 2.000712]\n",
      "2180 [D loss: 0.216103, acc.: 89.84%] [G loss: 2.117732]\n",
      "2181 [D loss: 0.125094, acc.: 96.88%] [G loss: 2.901531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2182 [D loss: 0.139978, acc.: 92.97%] [G loss: 3.238732]\n",
      "2183 [D loss: 0.513211, acc.: 81.25%] [G loss: 4.806061]\n",
      "2184 [D loss: 0.307657, acc.: 89.06%] [G loss: 3.765181]\n",
      "2185 [D loss: 0.207466, acc.: 89.84%] [G loss: 3.397083]\n",
      "2186 [D loss: 0.313447, acc.: 84.38%] [G loss: 3.670939]\n",
      "2187 [D loss: 0.379938, acc.: 82.03%] [G loss: 5.177839]\n",
      "2188 [D loss: 0.220471, acc.: 95.31%] [G loss: 4.889335]\n",
      "2189 [D loss: 0.282616, acc.: 92.97%] [G loss: 3.701027]\n",
      "2190 [D loss: 0.168290, acc.: 92.97%] [G loss: 2.989665]\n",
      "2191 [D loss: 0.449032, acc.: 82.81%] [G loss: 2.838303]\n",
      "2192 [D loss: 0.349453, acc.: 91.41%] [G loss: 3.417519]\n",
      "2193 [D loss: 0.655344, acc.: 78.12%] [G loss: 4.974713]\n",
      "2194 [D loss: 0.515540, acc.: 79.69%] [G loss: 4.525442]\n",
      "2195 [D loss: 0.404396, acc.: 85.16%] [G loss: 5.536898]\n",
      "2196 [D loss: 0.152902, acc.: 99.22%] [G loss: 5.516679]\n",
      "2197 [D loss: 0.153287, acc.: 93.75%] [G loss: 3.716424]\n",
      "2198 [D loss: 0.240175, acc.: 94.53%] [G loss: 3.360718]\n",
      "2199 [D loss: 0.233657, acc.: 96.09%] [G loss: 2.906860]\n",
      "2200 [D loss: 0.346312, acc.: 87.50%] [G loss: 3.356227]\n",
      "2201 [D loss: 0.572769, acc.: 94.53%] [G loss: 3.270030]\n",
      "2202 [D loss: 0.543751, acc.: 91.41%] [G loss: 4.367405]\n",
      "2203 [D loss: 0.390572, acc.: 90.62%] [G loss: 4.937935]\n",
      "2204 [D loss: 0.364225, acc.: 94.53%] [G loss: 4.286596]\n",
      "2205 [D loss: 0.402660, acc.: 83.59%] [G loss: 4.810802]\n",
      "2206 [D loss: 0.175718, acc.: 97.66%] [G loss: 4.853642]\n",
      "2207 [D loss: 0.308366, acc.: 97.66%] [G loss: 3.862871]\n",
      "2208 [D loss: 0.519165, acc.: 91.41%] [G loss: 4.130964]\n",
      "2209 [D loss: 0.329462, acc.: 93.75%] [G loss: 4.091054]\n",
      "2210 [D loss: 0.593480, acc.: 87.50%] [G loss: 5.647699]\n",
      "2211 [D loss: 0.136919, acc.: 99.22%] [G loss: 5.165390]\n",
      "2212 [D loss: 0.198741, acc.: 96.88%] [G loss: 3.474547]\n",
      "2213 [D loss: 0.591865, acc.: 93.75%] [G loss: 3.811888]\n",
      "2214 [D loss: 0.254930, acc.: 93.75%] [G loss: 3.162239]\n",
      "2215 [D loss: 0.556902, acc.: 87.50%] [G loss: 4.042397]\n",
      "2216 [D loss: 0.142927, acc.: 94.53%] [G loss: 3.790452]\n",
      "2217 [D loss: 0.541929, acc.: 82.81%] [G loss: 6.071157]\n",
      "2218 [D loss: 0.234278, acc.: 94.53%] [G loss: 6.756452]\n",
      "2219 [D loss: 0.385494, acc.: 97.66%] [G loss: 5.586067]\n",
      "2220 [D loss: 0.259907, acc.: 95.31%] [G loss: 5.410036]\n",
      "2221 [D loss: 0.294137, acc.: 96.88%] [G loss: 5.194777]\n",
      "2222 [D loss: 0.760494, acc.: 89.84%] [G loss: 5.018633]\n",
      "2223 [D loss: 0.546245, acc.: 96.88%] [G loss: 4.520146]\n",
      "2224 [D loss: 0.223727, acc.: 96.09%] [G loss: 4.242671]\n",
      "2225 [D loss: 0.412638, acc.: 97.66%] [G loss: 4.048739]\n",
      "2226 [D loss: 0.683918, acc.: 94.53%] [G loss: 3.826057]\n",
      "2227 [D loss: 0.199175, acc.: 98.44%] [G loss: 4.140851]\n",
      "2228 [D loss: 0.447376, acc.: 95.31%] [G loss: 4.000201]\n",
      "2229 [D loss: 0.682234, acc.: 94.53%] [G loss: 4.119845]\n",
      "2230 [D loss: 0.310413, acc.: 97.66%] [G loss: 4.088012]\n",
      "2231 [D loss: 0.427937, acc.: 96.88%] [G loss: 4.227630]\n",
      "2232 [D loss: 0.683282, acc.: 94.53%] [G loss: 4.016348]\n",
      "2233 [D loss: 0.322610, acc.: 96.88%] [G loss: 3.913985]\n",
      "2234 [D loss: 0.591185, acc.: 92.97%] [G loss: 3.935933]\n",
      "2235 [D loss: 0.905104, acc.: 90.62%] [G loss: 4.800681]\n",
      "2236 [D loss: 0.657283, acc.: 78.12%] [G loss: 7.240189]\n",
      "2237 [D loss: 0.629924, acc.: 96.09%] [G loss: 9.108425]\n",
      "2238 [D loss: 0.253083, acc.: 98.44%] [G loss: 7.439800]\n",
      "2239 [D loss: 0.254698, acc.: 98.44%] [G loss: 5.665158]\n",
      "2240 [D loss: 0.779709, acc.: 93.75%] [G loss: 4.583071]\n",
      "2241 [D loss: 0.547501, acc.: 96.09%] [G loss: 4.095984]\n",
      "2242 [D loss: 0.286652, acc.: 97.66%] [G loss: 3.805255]\n",
      "2243 [D loss: 0.290309, acc.: 98.44%] [G loss: 3.601377]\n",
      "2244 [D loss: 0.967964, acc.: 92.97%] [G loss: 3.018042]\n",
      "2245 [D loss: 0.632936, acc.: 92.97%] [G loss: 3.618981]\n",
      "2246 [D loss: 0.303284, acc.: 98.44%] [G loss: 3.525643]\n",
      "2247 [D loss: 0.311270, acc.: 96.09%] [G loss: 3.214191]\n",
      "2248 [D loss: 0.553077, acc.: 88.28%] [G loss: 4.434454]\n",
      "2249 [D loss: 0.623028, acc.: 89.84%] [G loss: 5.139578]\n",
      "2250 [D loss: 0.170327, acc.: 98.44%] [G loss: 5.632411]\n",
      "2251 [D loss: 0.143018, acc.: 99.22%] [G loss: 4.961895]\n",
      "2252 [D loss: 0.272165, acc.: 98.44%] [G loss: 4.371230]\n",
      "2253 [D loss: 0.286242, acc.: 98.44%] [G loss: 3.815661]\n",
      "2254 [D loss: 0.030965, acc.: 100.00%] [G loss: 3.950651]\n",
      "2255 [D loss: 0.283167, acc.: 97.66%] [G loss: 3.357626]\n",
      "2256 [D loss: 0.418048, acc.: 96.88%] [G loss: 3.301428]\n",
      "2257 [D loss: 0.606234, acc.: 93.75%] [G loss: 4.055712]\n",
      "2258 [D loss: 0.285995, acc.: 97.66%] [G loss: 3.987500]\n",
      "2259 [D loss: 0.533344, acc.: 96.88%] [G loss: 3.154150]\n",
      "2260 [D loss: 0.165823, acc.: 98.44%] [G loss: 3.112561]\n",
      "2261 [D loss: 0.283601, acc.: 97.66%] [G loss: 3.125951]\n",
      "2262 [D loss: 0.023807, acc.: 99.22%] [G loss: 2.797544]\n",
      "2263 [D loss: 0.450103, acc.: 96.09%] [G loss: 2.846829]\n",
      "2264 [D loss: 0.903254, acc.: 86.72%] [G loss: 4.190121]\n",
      "2265 [D loss: 0.393530, acc.: 91.41%] [G loss: 4.239304]\n",
      "2266 [D loss: 0.427079, acc.: 84.38%] [G loss: 5.046433]\n",
      "2267 [D loss: 1.323223, acc.: 73.44%] [G loss: 8.571866]\n",
      "2268 [D loss: 0.252160, acc.: 98.44%] [G loss: 10.275415]\n",
      "2269 [D loss: 0.252132, acc.: 98.44%] [G loss: 9.294722]\n",
      "2270 [D loss: 0.378920, acc.: 97.66%] [G loss: 8.112316]\n",
      "2271 [D loss: 0.504701, acc.: 96.88%] [G loss: 6.864428]\n",
      "2272 [D loss: 0.393116, acc.: 96.88%] [G loss: 5.904707]\n",
      "2273 [D loss: 0.280241, acc.: 98.44%] [G loss: 4.947038]\n",
      "2274 [D loss: 0.166824, acc.: 99.22%] [G loss: 4.677924]\n",
      "2275 [D loss: 0.528511, acc.: 96.09%] [G loss: 4.710227]\n",
      "2276 [D loss: 0.284797, acc.: 97.66%] [G loss: 4.504852]\n",
      "2277 [D loss: 0.426956, acc.: 96.09%] [G loss: 4.369343]\n",
      "2278 [D loss: 0.396573, acc.: 97.66%] [G loss: 4.341774]\n",
      "2279 [D loss: 0.149019, acc.: 99.22%] [G loss: 4.278543]\n",
      "2280 [D loss: 0.276893, acc.: 98.44%] [G loss: 3.845877]\n",
      "2281 [D loss: 0.271166, acc.: 98.44%] [G loss: 3.590320]\n",
      "2282 [D loss: 0.018409, acc.: 100.00%] [G loss: 3.757310]\n",
      "2283 [D loss: 0.013036, acc.: 100.00%] [G loss: 3.422468]\n",
      "2284 [D loss: 0.149921, acc.: 99.22%] [G loss: 3.444447]\n",
      "2285 [D loss: 0.424216, acc.: 96.09%] [G loss: 2.850941]\n",
      "2286 [D loss: 0.545565, acc.: 96.88%] [G loss: 2.338637]\n",
      "2287 [D loss: 0.684939, acc.: 96.09%] [G loss: 2.610454]\n",
      "2288 [D loss: 0.538294, acc.: 96.88%] [G loss: 2.622342]\n",
      "2289 [D loss: 0.278282, acc.: 98.44%] [G loss: 2.640369]\n",
      "2290 [D loss: 0.025414, acc.: 100.00%] [G loss: 2.597861]\n",
      "2291 [D loss: 0.413980, acc.: 97.66%] [G loss: 2.502734]\n",
      "2292 [D loss: 0.541183, acc.: 96.88%] [G loss: 2.640448]\n",
      "2293 [D loss: 0.534840, acc.: 96.09%] [G loss: 2.798487]\n",
      "2294 [D loss: 0.269525, acc.: 98.44%] [G loss: 2.718257]\n",
      "2295 [D loss: 0.284652, acc.: 98.44%] [G loss: 3.058901]\n",
      "2296 [D loss: 0.458342, acc.: 94.53%] [G loss: 3.229893]\n",
      "2297 [D loss: 0.138372, acc.: 99.22%] [G loss: 3.446499]\n",
      "2298 [D loss: 0.674764, acc.: 95.31%] [G loss: 2.961679]\n",
      "2299 [D loss: 0.277828, acc.: 97.66%] [G loss: 2.074995]\n",
      "2300 [D loss: 0.065918, acc.: 100.00%] [G loss: 2.760413]\n",
      "2301 [D loss: 0.135023, acc.: 93.75%] [G loss: 2.816259]\n",
      "2302 [D loss: 0.515200, acc.: 74.22%] [G loss: 5.181852]\n",
      "2303 [D loss: 0.277339, acc.: 93.75%] [G loss: 6.161339]\n",
      "2304 [D loss: 0.387710, acc.: 91.41%] [G loss: 5.940009]\n",
      "2305 [D loss: 0.152072, acc.: 94.53%] [G loss: 6.203665]\n",
      "2306 [D loss: 0.054011, acc.: 99.22%] [G loss: 5.561052]\n",
      "2307 [D loss: 0.106965, acc.: 96.09%] [G loss: 5.748144]\n",
      "2308 [D loss: 0.060525, acc.: 100.00%] [G loss: 5.496823]\n",
      "2309 [D loss: 0.023232, acc.: 100.00%] [G loss: 5.352664]\n",
      "2310 [D loss: 0.074894, acc.: 98.44%] [G loss: 4.576955]\n",
      "2311 [D loss: 0.154753, acc.: 92.19%] [G loss: 4.661022]\n",
      "2312 [D loss: 0.104251, acc.: 96.88%] [G loss: 4.474370]\n",
      "2313 [D loss: 0.077925, acc.: 96.09%] [G loss: 3.561938]\n",
      "2314 [D loss: 0.198317, acc.: 96.88%] [G loss: 3.249993]\n",
      "2315 [D loss: 0.062580, acc.: 98.44%] [G loss: 2.753075]\n",
      "2316 [D loss: 0.125080, acc.: 96.88%] [G loss: 3.569110]\n",
      "2317 [D loss: 0.212644, acc.: 96.09%] [G loss: 2.886957]\n",
      "2318 [D loss: 0.152294, acc.: 90.62%] [G loss: 3.301714]\n",
      "2319 [D loss: 0.216786, acc.: 89.06%] [G loss: 4.035427]\n",
      "2320 [D loss: 0.135352, acc.: 94.53%] [G loss: 4.385828]\n",
      "2321 [D loss: 0.212116, acc.: 92.19%] [G loss: 3.752477]\n",
      "2322 [D loss: 0.260823, acc.: 85.94%] [G loss: 4.866116]\n",
      "2323 [D loss: 0.078475, acc.: 97.66%] [G loss: 4.836136]\n",
      "2324 [D loss: 0.057527, acc.: 98.44%] [G loss: 3.682740]\n",
      "2325 [D loss: 0.064856, acc.: 97.66%] [G loss: 3.727483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2326 [D loss: 0.085859, acc.: 97.66%] [G loss: 2.873280]\n",
      "2327 [D loss: 0.129845, acc.: 95.31%] [G loss: 3.198842]\n",
      "2328 [D loss: 0.260834, acc.: 96.09%] [G loss: 3.000956]\n",
      "2329 [D loss: 0.429963, acc.: 85.16%] [G loss: 4.027160]\n",
      "2330 [D loss: 0.229587, acc.: 93.75%] [G loss: 4.091300]\n",
      "2331 [D loss: 0.139505, acc.: 96.88%] [G loss: 3.888268]\n",
      "2332 [D loss: 0.176778, acc.: 89.06%] [G loss: 4.885088]\n",
      "2333 [D loss: 0.162764, acc.: 99.22%] [G loss: 5.109210]\n",
      "2334 [D loss: 0.032975, acc.: 100.00%] [G loss: 3.742404]\n",
      "2335 [D loss: 0.179695, acc.: 99.22%] [G loss: 3.103617]\n",
      "2336 [D loss: 0.075717, acc.: 97.66%] [G loss: 2.759624]\n",
      "2337 [D loss: 0.033497, acc.: 99.22%] [G loss: 2.664577]\n",
      "2338 [D loss: 0.080133, acc.: 98.44%] [G loss: 2.307415]\n",
      "2339 [D loss: 0.058655, acc.: 100.00%] [G loss: 2.333807]\n",
      "2340 [D loss: 0.559558, acc.: 96.88%] [G loss: 2.483044]\n",
      "2341 [D loss: 0.182432, acc.: 99.22%] [G loss: 2.667624]\n",
      "2342 [D loss: 0.207261, acc.: 89.84%] [G loss: 3.796847]\n",
      "2343 [D loss: 1.499900, acc.: 71.88%] [G loss: 6.177694]\n",
      "2344 [D loss: 0.457535, acc.: 85.94%] [G loss: 9.044449]\n",
      "2345 [D loss: 0.005391, acc.: 100.00%] [G loss: 9.202305]\n",
      "2346 [D loss: 0.259383, acc.: 98.44%] [G loss: 6.857923]\n",
      "2347 [D loss: 0.169492, acc.: 98.44%] [G loss: 5.708794]\n",
      "2348 [D loss: 0.204715, acc.: 98.44%] [G loss: 5.021053]\n",
      "2349 [D loss: 0.041437, acc.: 99.22%] [G loss: 4.693986]\n",
      "2350 [D loss: 0.174809, acc.: 98.44%] [G loss: 4.205536]\n",
      "2351 [D loss: 0.078633, acc.: 99.22%] [G loss: 4.654854]\n",
      "2352 [D loss: 0.312398, acc.: 96.09%] [G loss: 4.678953]\n",
      "2353 [D loss: 0.051505, acc.: 99.22%] [G loss: 4.370366]\n",
      "2354 [D loss: 0.032929, acc.: 100.00%] [G loss: 4.055831]\n",
      "2355 [D loss: 0.785117, acc.: 95.31%] [G loss: 3.706967]\n",
      "2356 [D loss: 0.044411, acc.: 100.00%] [G loss: 3.539649]\n",
      "2357 [D loss: 0.157222, acc.: 99.22%] [G loss: 3.894526]\n",
      "2358 [D loss: 0.163510, acc.: 99.22%] [G loss: 3.765594]\n",
      "2359 [D loss: 0.268853, acc.: 98.44%] [G loss: 3.638297]\n",
      "2360 [D loss: 0.059552, acc.: 98.44%] [G loss: 3.724039]\n",
      "2361 [D loss: 0.426702, acc.: 96.88%] [G loss: 3.836978]\n",
      "2362 [D loss: 0.045159, acc.: 97.66%] [G loss: 3.080982]\n",
      "2363 [D loss: 0.055740, acc.: 100.00%] [G loss: 3.100600]\n",
      "2364 [D loss: 0.033601, acc.: 100.00%] [G loss: 2.720829]\n",
      "2365 [D loss: 0.170582, acc.: 99.22%] [G loss: 2.561074]\n",
      "2366 [D loss: 0.042084, acc.: 100.00%] [G loss: 3.150117]\n",
      "2367 [D loss: 0.275120, acc.: 98.44%] [G loss: 3.108716]\n",
      "2368 [D loss: 0.022676, acc.: 100.00%] [G loss: 3.261285]\n",
      "2369 [D loss: 0.280854, acc.: 98.44%] [G loss: 3.181538]\n",
      "2370 [D loss: 0.413249, acc.: 96.88%] [G loss: 2.980827]\n",
      "2371 [D loss: 0.359450, acc.: 93.75%] [G loss: 2.297025]\n",
      "2372 [D loss: 0.488090, acc.: 83.59%] [G loss: 5.916635]\n",
      "2373 [D loss: 0.251197, acc.: 93.75%] [G loss: 7.434959]\n",
      "2374 [D loss: 0.252901, acc.: 98.44%] [G loss: 5.801215]\n",
      "2375 [D loss: 0.129097, acc.: 99.22%] [G loss: 4.668271]\n",
      "2376 [D loss: 0.131221, acc.: 99.22%] [G loss: 3.835404]\n",
      "2377 [D loss: 0.389019, acc.: 97.66%] [G loss: 3.588685]\n",
      "2378 [D loss: 0.524464, acc.: 96.88%] [G loss: 3.308521]\n",
      "2379 [D loss: 0.283707, acc.: 98.44%] [G loss: 2.843820]\n",
      "2380 [D loss: 0.280294, acc.: 97.66%] [G loss: 2.519932]\n",
      "2381 [D loss: 0.040876, acc.: 100.00%] [G loss: 2.572509]\n",
      "2382 [D loss: 0.419505, acc.: 97.66%] [G loss: 2.716569]\n",
      "2383 [D loss: 0.402999, acc.: 97.66%] [G loss: 2.891798]\n",
      "2384 [D loss: 0.269674, acc.: 98.44%] [G loss: 2.673143]\n",
      "2385 [D loss: 0.265693, acc.: 98.44%] [G loss: 2.733813]\n",
      "2386 [D loss: 0.147792, acc.: 99.22%] [G loss: 2.541083]\n",
      "2387 [D loss: 0.163933, acc.: 96.88%] [G loss: 2.384796]\n",
      "2388 [D loss: 0.409440, acc.: 97.66%] [G loss: 2.377728]\n",
      "2389 [D loss: 0.269703, acc.: 98.44%] [G loss: 3.074336]\n",
      "2390 [D loss: 0.144582, acc.: 99.22%] [G loss: 2.605928]\n",
      "2391 [D loss: 0.275186, acc.: 98.44%] [G loss: 2.720302]\n",
      "2392 [D loss: 0.172108, acc.: 97.66%] [G loss: 2.465504]\n",
      "2393 [D loss: 0.220587, acc.: 98.44%] [G loss: 4.182673]\n",
      "2394 [D loss: 0.727198, acc.: 85.94%] [G loss: 5.163262]\n",
      "2395 [D loss: 0.299646, acc.: 86.72%] [G loss: 6.024919]\n",
      "2396 [D loss: 0.406417, acc.: 97.66%] [G loss: 6.647017]\n",
      "2397 [D loss: 0.383514, acc.: 97.66%] [G loss: 6.617835]\n",
      "2398 [D loss: 0.006299, acc.: 100.00%] [G loss: 5.563036]\n",
      "2399 [D loss: 0.298362, acc.: 97.66%] [G loss: 5.154154]\n",
      "2400 [D loss: 0.263106, acc.: 98.44%] [G loss: 4.620105]\n",
      "2401 [D loss: 0.386487, acc.: 97.66%] [G loss: 4.562613]\n",
      "2402 [D loss: 0.260035, acc.: 98.44%] [G loss: 4.469100]\n",
      "2403 [D loss: 0.135705, acc.: 99.22%] [G loss: 3.839484]\n",
      "2404 [D loss: 0.523283, acc.: 96.88%] [G loss: 3.735768]\n",
      "2405 [D loss: 0.639647, acc.: 96.09%] [G loss: 3.745842]\n",
      "2406 [D loss: 0.541222, acc.: 94.53%] [G loss: 4.064708]\n",
      "2407 [D loss: 0.289192, acc.: 98.44%] [G loss: 4.437581]\n",
      "2408 [D loss: 0.301263, acc.: 97.66%] [G loss: 3.299218]\n",
      "2409 [D loss: 0.899370, acc.: 94.53%] [G loss: 2.803488]\n",
      "2410 [D loss: 0.151182, acc.: 99.22%] [G loss: 2.512483]\n",
      "2411 [D loss: 0.146530, acc.: 99.22%] [G loss: 2.344488]\n",
      "2412 [D loss: 0.275137, acc.: 98.44%] [G loss: 2.253624]\n",
      "2413 [D loss: 0.022067, acc.: 100.00%] [G loss: 2.374866]\n",
      "2414 [D loss: 0.018335, acc.: 100.00%] [G loss: 2.688327]\n",
      "2415 [D loss: 0.140902, acc.: 99.22%] [G loss: 2.877024]\n",
      "2416 [D loss: 0.026152, acc.: 100.00%] [G loss: 2.568882]\n",
      "2417 [D loss: 0.149502, acc.: 99.22%] [G loss: 2.186293]\n",
      "2418 [D loss: 0.156666, acc.: 99.22%] [G loss: 2.334821]\n",
      "2419 [D loss: 0.146001, acc.: 99.22%] [G loss: 2.824785]\n",
      "2420 [D loss: 0.013538, acc.: 100.00%] [G loss: 2.951820]\n",
      "2421 [D loss: 0.270610, acc.: 98.44%] [G loss: 2.506670]\n",
      "2422 [D loss: 0.279280, acc.: 98.44%] [G loss: 2.467605]\n",
      "2423 [D loss: 0.031650, acc.: 100.00%] [G loss: 2.156792]\n",
      "2424 [D loss: 0.044407, acc.: 99.22%] [G loss: 2.623053]\n",
      "2425 [D loss: 0.013528, acc.: 100.00%] [G loss: 2.802868]\n",
      "2426 [D loss: 0.019730, acc.: 100.00%] [G loss: 2.767788]\n",
      "2427 [D loss: 0.148824, acc.: 99.22%] [G loss: 2.672088]\n",
      "2428 [D loss: 0.020802, acc.: 100.00%] [G loss: 2.500701]\n",
      "2429 [D loss: 0.277636, acc.: 98.44%] [G loss: 2.171364]\n",
      "2430 [D loss: 0.039705, acc.: 100.00%] [G loss: 2.674757]\n",
      "2431 [D loss: 0.022980, acc.: 100.00%] [G loss: 2.909424]\n",
      "2432 [D loss: 0.012755, acc.: 100.00%] [G loss: 2.761900]\n",
      "2433 [D loss: 0.140627, acc.: 99.22%] [G loss: 2.813809]\n",
      "2434 [D loss: 0.264886, acc.: 98.44%] [G loss: 2.592585]\n",
      "2435 [D loss: 0.275726, acc.: 98.44%] [G loss: 2.444416]\n",
      "2436 [D loss: 0.152132, acc.: 99.22%] [G loss: 2.397704]\n",
      "2437 [D loss: 0.023130, acc.: 100.00%] [G loss: 2.900907]\n",
      "2438 [D loss: 0.137380, acc.: 99.22%] [G loss: 3.190967]\n",
      "2439 [D loss: 0.260868, acc.: 98.44%] [G loss: 3.170084]\n",
      "2440 [D loss: 0.018135, acc.: 100.00%] [G loss: 2.424816]\n",
      "2441 [D loss: 0.283041, acc.: 98.44%] [G loss: 2.451600]\n",
      "2442 [D loss: 0.172836, acc.: 99.22%] [G loss: 2.538607]\n",
      "2443 [D loss: 0.310096, acc.: 86.72%] [G loss: 6.427379]\n",
      "2444 [D loss: 0.336838, acc.: 92.97%] [G loss: 8.109379]\n",
      "2445 [D loss: 0.102675, acc.: 96.09%] [G loss: 6.835557]\n",
      "2446 [D loss: 0.257493, acc.: 98.44%] [G loss: 5.964744]\n",
      "2447 [D loss: 0.278817, acc.: 98.44%] [G loss: 5.079762]\n",
      "2448 [D loss: 0.133032, acc.: 99.22%] [G loss: 4.696297]\n",
      "2449 [D loss: 0.133753, acc.: 99.22%] [G loss: 4.456899]\n",
      "2450 [D loss: 0.132499, acc.: 99.22%] [G loss: 3.694860]\n",
      "2451 [D loss: 0.134004, acc.: 99.22%] [G loss: 3.364489]\n",
      "2452 [D loss: 0.392027, acc.: 97.66%] [G loss: 3.419539]\n",
      "2453 [D loss: 0.386194, acc.: 97.66%] [G loss: 3.263402]\n",
      "2454 [D loss: 0.017653, acc.: 100.00%] [G loss: 3.089862]\n",
      "2455 [D loss: 0.142945, acc.: 99.22%] [G loss: 2.949426]\n",
      "2456 [D loss: 0.277746, acc.: 97.66%] [G loss: 2.871095]\n",
      "2457 [D loss: 0.013074, acc.: 100.00%] [G loss: 2.866046]\n",
      "2458 [D loss: 0.270589, acc.: 97.66%] [G loss: 2.768307]\n",
      "2459 [D loss: 0.029738, acc.: 100.00%] [G loss: 2.656031]\n",
      "2460 [D loss: 0.146449, acc.: 99.22%] [G loss: 2.633581]\n",
      "2461 [D loss: 0.276978, acc.: 98.44%] [G loss: 2.612222]\n",
      "2462 [D loss: 0.020351, acc.: 100.00%] [G loss: 2.485508]\n",
      "2463 [D loss: 0.146868, acc.: 99.22%] [G loss: 2.565416]\n",
      "2464 [D loss: 0.264862, acc.: 98.44%] [G loss: 2.929228]\n",
      "2465 [D loss: 0.265370, acc.: 98.44%] [G loss: 2.813470]\n",
      "2466 [D loss: 0.137678, acc.: 99.22%] [G loss: 3.149217]\n",
      "2467 [D loss: 0.014277, acc.: 100.00%] [G loss: 3.201254]\n",
      "2468 [D loss: 0.167190, acc.: 97.66%] [G loss: 2.528434]\n",
      "2469 [D loss: 0.333130, acc.: 95.31%] [G loss: 3.683431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470 [D loss: 0.588677, acc.: 90.62%] [G loss: 4.539799]\n",
      "2471 [D loss: 0.293682, acc.: 96.88%] [G loss: 4.023250]\n",
      "2472 [D loss: 0.268546, acc.: 98.44%] [G loss: 3.954562]\n",
      "2473 [D loss: 0.132138, acc.: 99.22%] [G loss: 3.685628]\n",
      "2474 [D loss: 0.133178, acc.: 99.22%] [G loss: 3.536410]\n",
      "2475 [D loss: 0.132699, acc.: 99.22%] [G loss: 3.419822]\n",
      "2476 [D loss: 0.009314, acc.: 100.00%] [G loss: 3.545106]\n",
      "2477 [D loss: 0.259470, acc.: 98.44%] [G loss: 3.507386]\n",
      "2478 [D loss: 0.259211, acc.: 98.44%] [G loss: 3.350698]\n",
      "2479 [D loss: 0.516453, acc.: 96.88%] [G loss: 3.276179]\n",
      "2480 [D loss: 0.259684, acc.: 98.44%] [G loss: 3.192057]\n",
      "2481 [D loss: 0.397095, acc.: 97.66%] [G loss: 2.939117]\n",
      "2482 [D loss: 0.141402, acc.: 99.22%] [G loss: 2.610041]\n",
      "2483 [D loss: 0.143201, acc.: 99.22%] [G loss: 2.641554]\n",
      "2484 [D loss: 0.014175, acc.: 100.00%] [G loss: 2.688192]\n",
      "2485 [D loss: 0.264049, acc.: 98.44%] [G loss: 2.767833]\n",
      "2486 [D loss: 0.265719, acc.: 98.44%] [G loss: 2.830448]\n",
      "2487 [D loss: 0.009843, acc.: 100.00%] [G loss: 2.926302]\n",
      "2488 [D loss: 0.139028, acc.: 99.22%] [G loss: 2.662518]\n",
      "2489 [D loss: 0.145705, acc.: 99.22%] [G loss: 2.593708]\n",
      "2490 [D loss: 0.527531, acc.: 96.88%] [G loss: 2.951123]\n",
      "2491 [D loss: 0.260990, acc.: 98.44%] [G loss: 3.094833]\n",
      "2492 [D loss: 0.392508, acc.: 97.66%] [G loss: 2.732018]\n",
      "2493 [D loss: 0.019871, acc.: 100.00%] [G loss: 2.666185]\n",
      "2494 [D loss: 0.269260, acc.: 98.44%] [G loss: 2.470647]\n",
      "2495 [D loss: 0.272755, acc.: 98.44%] [G loss: 2.762967]\n",
      "2496 [D loss: 0.389838, acc.: 97.66%] [G loss: 2.718019]\n",
      "2497 [D loss: 0.141623, acc.: 99.22%] [G loss: 2.552292]\n",
      "2498 [D loss: 0.144067, acc.: 99.22%] [G loss: 2.699574]\n",
      "2499 [D loss: 0.259539, acc.: 98.44%] [G loss: 3.121810]\n",
      "2500 [D loss: 0.292210, acc.: 97.66%] [G loss: 3.367161]\n",
      "2501 [D loss: 0.900871, acc.: 94.53%] [G loss: 3.047953]\n",
      "2502 [D loss: 0.149245, acc.: 99.22%] [G loss: 2.544178]\n",
      "2503 [D loss: 0.140039, acc.: 99.22%] [G loss: 2.809348]\n",
      "2504 [D loss: 0.265982, acc.: 98.44%] [G loss: 2.658043]\n",
      "2505 [D loss: 0.142967, acc.: 99.22%] [G loss: 2.748116]\n",
      "2506 [D loss: 0.417597, acc.: 96.88%] [G loss: 2.821917]\n",
      "2507 [D loss: 0.392187, acc.: 97.66%] [G loss: 2.599724]\n",
      "2508 [D loss: 0.421278, acc.: 96.88%] [G loss: 2.496956]\n",
      "2509 [D loss: 0.387075, acc.: 97.66%] [G loss: 2.756842]\n",
      "2510 [D loss: 0.268614, acc.: 98.44%] [G loss: 2.984019]\n",
      "2511 [D loss: 0.142597, acc.: 99.22%] [G loss: 2.653707]\n",
      "2512 [D loss: 0.138967, acc.: 99.22%] [G loss: 2.598784]\n",
      "2513 [D loss: 0.014882, acc.: 100.00%] [G loss: 2.683780]\n",
      "2514 [D loss: 0.260984, acc.: 98.44%] [G loss: 3.010777]\n",
      "2515 [D loss: 0.131737, acc.: 99.22%] [G loss: 3.158634]\n",
      "2516 [D loss: 0.387385, acc.: 97.66%] [G loss: 3.121663]\n",
      "2517 [D loss: 0.175616, acc.: 97.66%] [G loss: 2.761813]\n",
      "2518 [D loss: 0.027045, acc.: 100.00%] [G loss: 2.814935]\n",
      "2519 [D loss: 0.150348, acc.: 98.44%] [G loss: 2.907497]\n",
      "2520 [D loss: 0.135001, acc.: 99.22%] [G loss: 2.960495]\n",
      "2521 [D loss: 0.134564, acc.: 99.22%] [G loss: 3.001595]\n",
      "2522 [D loss: 0.008583, acc.: 100.00%] [G loss: 2.874890]\n",
      "2523 [D loss: 0.137642, acc.: 99.22%] [G loss: 2.331335]\n",
      "2524 [D loss: 0.264506, acc.: 98.44%] [G loss: 2.313241]\n",
      "2525 [D loss: 0.137283, acc.: 99.22%] [G loss: 2.399793]\n",
      "2526 [D loss: 0.012083, acc.: 100.00%] [G loss: 2.354642]\n",
      "2527 [D loss: 0.138067, acc.: 99.22%] [G loss: 2.507704]\n",
      "2528 [D loss: 0.263650, acc.: 98.44%] [G loss: 2.792747]\n",
      "2529 [D loss: 0.131985, acc.: 99.22%] [G loss: 3.396376]\n",
      "2530 [D loss: 0.013247, acc.: 99.22%] [G loss: 3.731152]\n",
      "2531 [D loss: 0.007797, acc.: 100.00%] [G loss: 2.895651]\n",
      "2532 [D loss: 0.014621, acc.: 100.00%] [G loss: 2.332734]\n",
      "2533 [D loss: 0.016446, acc.: 100.00%] [G loss: 2.412842]\n",
      "2534 [D loss: 0.140322, acc.: 99.22%] [G loss: 2.418435]\n",
      "2535 [D loss: 0.014357, acc.: 100.00%] [G loss: 2.628509]\n",
      "2536 [D loss: 0.267310, acc.: 98.44%] [G loss: 2.763138]\n",
      "2537 [D loss: 0.261934, acc.: 98.44%] [G loss: 2.827556]\n",
      "2538 [D loss: 0.134382, acc.: 99.22%] [G loss: 2.981104]\n",
      "2539 [D loss: 0.004914, acc.: 100.00%] [G loss: 3.618970]\n",
      "2540 [D loss: 0.005041, acc.: 100.00%] [G loss: 3.219392]\n",
      "2541 [D loss: 0.260366, acc.: 98.44%] [G loss: 2.556924]\n",
      "2542 [D loss: 0.006867, acc.: 100.00%] [G loss: 2.611834]\n",
      "2543 [D loss: 0.395605, acc.: 97.66%] [G loss: 2.531284]\n",
      "2544 [D loss: 0.267781, acc.: 98.44%] [G loss: 2.740359]\n",
      "2545 [D loss: 0.142385, acc.: 99.22%] [G loss: 2.840472]\n",
      "2546 [D loss: 0.180925, acc.: 98.44%] [G loss: 2.855662]\n",
      "2547 [D loss: 0.384038, acc.: 97.66%] [G loss: 3.036623]\n",
      "2548 [D loss: 0.135277, acc.: 99.22%] [G loss: 2.982326]\n",
      "2549 [D loss: 0.005442, acc.: 100.00%] [G loss: 3.086261]\n",
      "2550 [D loss: 0.131897, acc.: 99.22%] [G loss: 3.217513]\n",
      "2551 [D loss: 0.386018, acc.: 97.66%] [G loss: 3.211973]\n",
      "2552 [D loss: 0.006523, acc.: 100.00%] [G loss: 2.953500]\n",
      "2553 [D loss: 0.134799, acc.: 99.22%] [G loss: 2.882510]\n",
      "2554 [D loss: 0.008460, acc.: 100.00%] [G loss: 2.941833]\n",
      "2555 [D loss: 0.136701, acc.: 99.22%] [G loss: 2.863130]\n",
      "2556 [D loss: 0.264403, acc.: 98.44%] [G loss: 2.802414]\n",
      "2557 [D loss: 0.637448, acc.: 96.09%] [G loss: 2.754000]\n",
      "2558 [D loss: 0.387252, acc.: 97.66%] [G loss: 2.690805]\n",
      "2559 [D loss: 0.138619, acc.: 99.22%] [G loss: 2.782235]\n",
      "2560 [D loss: 0.135048, acc.: 99.22%] [G loss: 2.751433]\n",
      "2561 [D loss: 0.008601, acc.: 100.00%] [G loss: 2.680330]\n",
      "2562 [D loss: 0.260834, acc.: 98.44%] [G loss: 2.808960]\n",
      "2563 [D loss: 0.261541, acc.: 98.44%] [G loss: 2.796054]\n",
      "2564 [D loss: 0.009131, acc.: 100.00%] [G loss: 2.765532]\n",
      "2565 [D loss: 0.134997, acc.: 99.22%] [G loss: 2.903712]\n",
      "2566 [D loss: 0.132710, acc.: 99.22%] [G loss: 2.835896]\n",
      "2567 [D loss: 0.263070, acc.: 98.44%] [G loss: 2.757383]\n",
      "2568 [D loss: 0.263987, acc.: 98.44%] [G loss: 2.632794]\n",
      "2569 [D loss: 0.133685, acc.: 99.22%] [G loss: 2.754584]\n",
      "2570 [D loss: 0.260697, acc.: 98.44%] [G loss: 3.141974]\n",
      "2571 [D loss: 0.130647, acc.: 99.22%] [G loss: 3.377563]\n",
      "2572 [D loss: 0.257651, acc.: 98.44%] [G loss: 3.081577]\n",
      "2573 [D loss: 0.387929, acc.: 97.66%] [G loss: 2.707220]\n",
      "2574 [D loss: 0.388890, acc.: 97.66%] [G loss: 2.634033]\n",
      "2575 [D loss: 0.136545, acc.: 99.22%] [G loss: 2.746812]\n",
      "2576 [D loss: 0.387890, acc.: 97.66%] [G loss: 2.828823]\n",
      "2577 [D loss: 0.136028, acc.: 99.22%] [G loss: 3.036761]\n",
      "2578 [D loss: 0.012946, acc.: 100.00%] [G loss: 3.018452]\n",
      "2579 [D loss: 0.148000, acc.: 99.22%] [G loss: 3.044568]\n",
      "2580 [D loss: 0.134144, acc.: 99.22%] [G loss: 3.331595]\n",
      "2581 [D loss: 0.008334, acc.: 100.00%] [G loss: 3.358386]\n",
      "2582 [D loss: 0.132327, acc.: 99.22%] [G loss: 3.455411]\n",
      "2583 [D loss: 0.257121, acc.: 98.44%] [G loss: 3.459215]\n",
      "2584 [D loss: 0.381934, acc.: 97.66%] [G loss: 3.376085]\n",
      "2585 [D loss: 0.257468, acc.: 98.44%] [G loss: 3.127185]\n",
      "2586 [D loss: 0.133996, acc.: 99.22%] [G loss: 2.995751]\n",
      "2587 [D loss: 0.261331, acc.: 98.44%] [G loss: 2.791896]\n",
      "2588 [D loss: 0.135587, acc.: 99.22%] [G loss: 2.671952]\n",
      "2589 [D loss: 0.385364, acc.: 97.66%] [G loss: 2.656148]\n",
      "2590 [D loss: 0.018958, acc.: 99.22%] [G loss: 2.772345]\n",
      "2591 [D loss: 0.134276, acc.: 99.22%] [G loss: 2.848708]\n",
      "2592 [D loss: 0.259141, acc.: 98.44%] [G loss: 2.995759]\n",
      "2593 [D loss: 0.384041, acc.: 97.66%] [G loss: 3.200111]\n",
      "2594 [D loss: 0.130836, acc.: 99.22%] [G loss: 2.970252]\n",
      "2595 [D loss: 0.258787, acc.: 98.44%] [G loss: 2.878926]\n",
      "2596 [D loss: 0.134947, acc.: 99.22%] [G loss: 2.854738]\n",
      "2597 [D loss: 0.388269, acc.: 97.66%] [G loss: 2.675813]\n",
      "2598 [D loss: 0.260208, acc.: 98.44%] [G loss: 3.156453]\n",
      "2599 [D loss: 0.132655, acc.: 99.22%] [G loss: 2.982611]\n",
      "2600 [D loss: 0.258870, acc.: 98.44%] [G loss: 2.926487]\n",
      "2601 [D loss: 0.262547, acc.: 98.44%] [G loss: 2.734572]\n",
      "2602 [D loss: 0.260611, acc.: 98.44%] [G loss: 2.811456]\n",
      "2603 [D loss: 0.513015, acc.: 96.88%] [G loss: 2.989257]\n",
      "2604 [D loss: 0.387611, acc.: 97.66%] [G loss: 2.794863]\n",
      "2605 [D loss: 0.261020, acc.: 98.44%] [G loss: 2.826048]\n",
      "2606 [D loss: 0.383290, acc.: 97.66%] [G loss: 2.999165]\n",
      "2607 [D loss: 0.383671, acc.: 97.66%] [G loss: 3.087302]\n",
      "2608 [D loss: 0.132093, acc.: 99.22%] [G loss: 3.183110]\n",
      "2609 [D loss: 0.257128, acc.: 98.44%] [G loss: 3.036474]\n",
      "2610 [D loss: 0.258730, acc.: 98.44%] [G loss: 3.176942]\n",
      "2611 [D loss: 0.107796, acc.: 99.22%] [G loss: 2.450440]\n",
      "2612 [D loss: 0.101176, acc.: 97.66%] [G loss: 4.754174]\n",
      "2613 [D loss: 0.767782, acc.: 87.50%] [G loss: 7.795573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2614 [D loss: 0.998329, acc.: 83.59%] [G loss: 10.397417]\n",
      "2615 [D loss: 0.578820, acc.: 86.72%] [G loss: 12.341606]\n",
      "2616 [D loss: 0.217014, acc.: 96.88%] [G loss: 11.760064]\n",
      "2617 [D loss: 0.306273, acc.: 96.09%] [G loss: 11.248123]\n",
      "2618 [D loss: 0.003176, acc.: 100.00%] [G loss: 11.131734]\n",
      "2619 [D loss: 0.133280, acc.: 99.22%] [G loss: 10.013818]\n",
      "2620 [D loss: 0.228642, acc.: 96.88%] [G loss: 9.653904]\n",
      "2621 [D loss: 0.163064, acc.: 92.19%] [G loss: 14.401031]\n",
      "2622 [D loss: 0.128546, acc.: 99.22%] [G loss: 14.616596]\n",
      "2623 [D loss: 0.231943, acc.: 96.09%] [G loss: 14.844856]\n",
      "2624 [D loss: 0.127733, acc.: 99.22%] [G loss: 14.151695]\n",
      "2625 [D loss: 0.017130, acc.: 99.22%] [G loss: 13.580857]\n",
      "2626 [D loss: 0.254743, acc.: 98.44%] [G loss: 12.854867]\n",
      "2627 [D loss: 0.129013, acc.: 99.22%] [G loss: 12.740452]\n",
      "2628 [D loss: 0.159454, acc.: 97.66%] [G loss: 11.507914]\n",
      "2629 [D loss: 0.253919, acc.: 98.44%] [G loss: 11.398494]\n",
      "2630 [D loss: 0.128439, acc.: 99.22%] [G loss: 10.128923]\n",
      "2631 [D loss: 0.049193, acc.: 97.66%] [G loss: 9.883636]\n",
      "2632 [D loss: 0.126272, acc.: 99.22%] [G loss: 9.892915]\n",
      "2633 [D loss: 0.126922, acc.: 99.22%] [G loss: 7.944089]\n",
      "2634 [D loss: 0.258345, acc.: 98.44%] [G loss: 6.788610]\n",
      "2635 [D loss: 0.261614, acc.: 98.44%] [G loss: 5.742720]\n",
      "2636 [D loss: 0.019768, acc.: 100.00%] [G loss: 5.373677]\n",
      "2637 [D loss: 0.406106, acc.: 97.66%] [G loss: 5.447055]\n",
      "2638 [D loss: 0.256474, acc.: 98.44%] [G loss: 5.033462]\n",
      "2639 [D loss: 0.013552, acc.: 100.00%] [G loss: 4.613076]\n",
      "2640 [D loss: 0.015847, acc.: 100.00%] [G loss: 4.278209]\n",
      "2641 [D loss: 0.137695, acc.: 99.22%] [G loss: 3.855611]\n",
      "2642 [D loss: 0.150067, acc.: 99.22%] [G loss: 3.809424]\n",
      "2643 [D loss: 0.269226, acc.: 98.44%] [G loss: 3.676117]\n",
      "2644 [D loss: 0.280934, acc.: 98.44%] [G loss: 3.713507]\n",
      "2645 [D loss: 0.259525, acc.: 98.44%] [G loss: 3.424911]\n",
      "2646 [D loss: 0.009475, acc.: 100.00%] [G loss: 3.203482]\n",
      "2647 [D loss: 0.141584, acc.: 99.22%] [G loss: 3.130968]\n",
      "2648 [D loss: 0.411210, acc.: 97.66%] [G loss: 2.899671]\n",
      "2649 [D loss: 0.042174, acc.: 97.66%] [G loss: 3.469213]\n",
      "2650 [D loss: 0.363566, acc.: 97.66%] [G loss: 5.602227]\n",
      "2651 [D loss: 0.135839, acc.: 99.22%] [G loss: 6.633208]\n",
      "2652 [D loss: 0.015955, acc.: 100.00%] [G loss: 5.515159]\n",
      "2653 [D loss: 0.007696, acc.: 100.00%] [G loss: 4.771962]\n",
      "2654 [D loss: 0.132928, acc.: 99.22%] [G loss: 4.069211]\n",
      "2655 [D loss: 0.010978, acc.: 100.00%] [G loss: 3.822284]\n",
      "2656 [D loss: 0.265462, acc.: 98.44%] [G loss: 3.197962]\n",
      "2657 [D loss: 0.402254, acc.: 97.66%] [G loss: 3.359818]\n",
      "2658 [D loss: 0.262368, acc.: 98.44%] [G loss: 3.329826]\n",
      "2659 [D loss: 0.137440, acc.: 99.22%] [G loss: 2.950165]\n",
      "2660 [D loss: 0.266598, acc.: 98.44%] [G loss: 2.867627]\n",
      "2661 [D loss: 0.267044, acc.: 98.44%] [G loss: 2.855670]\n",
      "2662 [D loss: 0.265361, acc.: 98.44%] [G loss: 2.715537]\n",
      "2663 [D loss: 0.157526, acc.: 97.66%] [G loss: 2.494385]\n",
      "2664 [D loss: 0.189925, acc.: 99.22%] [G loss: 4.374689]\n",
      "2665 [D loss: 0.156286, acc.: 99.22%] [G loss: 5.191198]\n",
      "2666 [D loss: 0.008505, acc.: 100.00%] [G loss: 4.286789]\n",
      "2667 [D loss: 0.386765, acc.: 97.66%] [G loss: 3.672122]\n",
      "2668 [D loss: 0.142571, acc.: 98.44%] [G loss: 3.270652]\n",
      "2669 [D loss: 0.135653, acc.: 99.22%] [G loss: 3.226775]\n",
      "2670 [D loss: 0.010482, acc.: 100.00%] [G loss: 3.112458]\n",
      "2671 [D loss: 0.008735, acc.: 100.00%] [G loss: 3.221140]\n",
      "2672 [D loss: 0.169228, acc.: 98.44%] [G loss: 2.661237]\n",
      "2673 [D loss: 0.029808, acc.: 99.22%] [G loss: 2.507047]\n",
      "2674 [D loss: 0.021819, acc.: 100.00%] [G loss: 2.620509]\n",
      "2675 [D loss: 0.267023, acc.: 98.44%] [G loss: 2.508672]\n",
      "2676 [D loss: 0.013317, acc.: 100.00%] [G loss: 2.455777]\n",
      "2677 [D loss: 0.010755, acc.: 100.00%] [G loss: 2.356673]\n",
      "2678 [D loss: 0.137940, acc.: 99.22%] [G loss: 2.299654]\n",
      "2679 [D loss: 0.007891, acc.: 100.00%] [G loss: 2.762266]\n",
      "2680 [D loss: 0.015554, acc.: 100.00%] [G loss: 2.703446]\n",
      "2681 [D loss: 0.138082, acc.: 99.22%] [G loss: 2.845464]\n",
      "2682 [D loss: 0.142943, acc.: 99.22%] [G loss: 2.531403]\n",
      "2683 [D loss: 0.016989, acc.: 100.00%] [G loss: 2.650726]\n",
      "2684 [D loss: 0.013799, acc.: 100.00%] [G loss: 2.449682]\n",
      "2685 [D loss: 0.012341, acc.: 100.00%] [G loss: 2.555750]\n",
      "2686 [D loss: 0.009327, acc.: 100.00%] [G loss: 3.291258]\n",
      "2687 [D loss: 0.130521, acc.: 99.22%] [G loss: 3.946468]\n",
      "2688 [D loss: 0.014777, acc.: 100.00%] [G loss: 2.482718]\n",
      "2689 [D loss: 0.037656, acc.: 100.00%] [G loss: 2.951437]\n",
      "2690 [D loss: 0.057392, acc.: 99.22%] [G loss: 4.288079]\n",
      "2691 [D loss: 0.032870, acc.: 99.22%] [G loss: 4.802750]\n",
      "2692 [D loss: 0.130798, acc.: 99.22%] [G loss: 4.485638]\n",
      "2693 [D loss: 0.005628, acc.: 100.00%] [G loss: 3.833102]\n",
      "2694 [D loss: 0.009354, acc.: 100.00%] [G loss: 3.725177]\n",
      "2695 [D loss: 0.258062, acc.: 98.44%] [G loss: 3.287232]\n",
      "2696 [D loss: 0.133065, acc.: 99.22%] [G loss: 3.449217]\n",
      "2697 [D loss: 0.259304, acc.: 98.44%] [G loss: 3.404223]\n",
      "2698 [D loss: 0.141549, acc.: 99.22%] [G loss: 3.586707]\n",
      "2699 [D loss: 0.016169, acc.: 100.00%] [G loss: 3.215318]\n",
      "2700 [D loss: 0.519209, acc.: 96.88%] [G loss: 3.233214]\n",
      "2701 [D loss: 0.009299, acc.: 100.00%] [G loss: 3.124593]\n",
      "2702 [D loss: 0.011457, acc.: 100.00%] [G loss: 2.953566]\n",
      "2703 [D loss: 0.014158, acc.: 100.00%] [G loss: 2.870624]\n",
      "2704 [D loss: 0.139006, acc.: 99.22%] [G loss: 3.158600]\n",
      "2705 [D loss: 0.011312, acc.: 100.00%] [G loss: 3.258092]\n",
      "2706 [D loss: 0.139891, acc.: 99.22%] [G loss: 2.930146]\n",
      "2707 [D loss: 0.009453, acc.: 100.00%] [G loss: 3.083505]\n",
      "2708 [D loss: 0.012789, acc.: 100.00%] [G loss: 3.152913]\n",
      "2709 [D loss: 0.010399, acc.: 100.00%] [G loss: 3.036673]\n",
      "2710 [D loss: 0.024485, acc.: 100.00%] [G loss: 2.776673]\n",
      "2711 [D loss: 0.014557, acc.: 100.00%] [G loss: 2.946778]\n",
      "2712 [D loss: 0.131195, acc.: 99.22%] [G loss: 3.082555]\n",
      "2713 [D loss: 0.138449, acc.: 99.22%] [G loss: 2.980610]\n",
      "2714 [D loss: 0.011710, acc.: 100.00%] [G loss: 2.862345]\n",
      "2715 [D loss: 0.389201, acc.: 97.66%] [G loss: 2.553797]\n",
      "2716 [D loss: 0.014027, acc.: 100.00%] [G loss: 2.501941]\n",
      "2717 [D loss: 0.264846, acc.: 98.44%] [G loss: 2.417268]\n",
      "2718 [D loss: 0.013036, acc.: 100.00%] [G loss: 2.644329]\n",
      "2719 [D loss: 0.137655, acc.: 99.22%] [G loss: 2.934205]\n",
      "2720 [D loss: 0.134786, acc.: 99.22%] [G loss: 2.969775]\n",
      "2721 [D loss: 0.265018, acc.: 98.44%] [G loss: 2.968012]\n",
      "2722 [D loss: 0.264122, acc.: 98.44%] [G loss: 3.126647]\n",
      "2723 [D loss: 0.008709, acc.: 100.00%] [G loss: 3.179037]\n",
      "2724 [D loss: 0.262639, acc.: 98.44%] [G loss: 2.791584]\n",
      "2725 [D loss: 0.147583, acc.: 99.22%] [G loss: 2.575456]\n",
      "2726 [D loss: 0.013410, acc.: 100.00%] [G loss: 2.790972]\n",
      "2727 [D loss: 0.134279, acc.: 99.22%] [G loss: 2.701026]\n",
      "2728 [D loss: 0.262621, acc.: 98.44%] [G loss: 2.732580]\n",
      "2729 [D loss: 0.135957, acc.: 99.22%] [G loss: 2.985400]\n",
      "2730 [D loss: 0.133036, acc.: 99.22%] [G loss: 3.017723]\n",
      "2731 [D loss: 0.007600, acc.: 100.00%] [G loss: 2.788293]\n",
      "2732 [D loss: 0.142342, acc.: 99.22%] [G loss: 2.785405]\n",
      "2733 [D loss: 0.167607, acc.: 99.22%] [G loss: 3.362093]\n",
      "2734 [D loss: 0.283508, acc.: 97.66%] [G loss: 4.765824]\n",
      "2735 [D loss: 0.012266, acc.: 100.00%] [G loss: 4.448680]\n",
      "2736 [D loss: 0.129014, acc.: 99.22%] [G loss: 3.963444]\n",
      "2737 [D loss: 0.003655, acc.: 100.00%] [G loss: 3.542052]\n",
      "2738 [D loss: 0.256091, acc.: 98.44%] [G loss: 3.505365]\n",
      "2739 [D loss: 0.130015, acc.: 99.22%] [G loss: 4.056699]\n",
      "2740 [D loss: 0.257435, acc.: 98.44%] [G loss: 4.039830]\n",
      "2741 [D loss: 0.009766, acc.: 100.00%] [G loss: 2.999306]\n",
      "2742 [D loss: 0.013744, acc.: 100.00%] [G loss: 2.724884]\n",
      "2743 [D loss: 0.015525, acc.: 100.00%] [G loss: 2.827482]\n",
      "2744 [D loss: 0.133218, acc.: 99.22%] [G loss: 3.088480]\n",
      "2745 [D loss: 0.008811, acc.: 100.00%] [G loss: 3.110135]\n",
      "2746 [D loss: 0.386878, acc.: 97.66%] [G loss: 3.100302]\n",
      "2747 [D loss: 0.258759, acc.: 98.44%] [G loss: 3.205696]\n",
      "2748 [D loss: 0.008999, acc.: 100.00%] [G loss: 3.005592]\n",
      "2749 [D loss: 0.013896, acc.: 100.00%] [G loss: 2.784874]\n",
      "2750 [D loss: 0.010487, acc.: 100.00%] [G loss: 2.938761]\n",
      "2751 [D loss: 0.135301, acc.: 99.22%] [G loss: 2.858716]\n",
      "2752 [D loss: 0.262486, acc.: 98.44%] [G loss: 3.033232]\n",
      "2753 [D loss: 0.132637, acc.: 99.22%] [G loss: 2.915792]\n",
      "2754 [D loss: 0.134559, acc.: 99.22%] [G loss: 2.920143]\n",
      "2755 [D loss: 0.258106, acc.: 98.44%] [G loss: 2.862713]\n",
      "2756 [D loss: 0.004243, acc.: 100.00%] [G loss: 3.050763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2757 [D loss: 0.007112, acc.: 100.00%] [G loss: 2.712823]\n",
      "2758 [D loss: 0.008591, acc.: 100.00%] [G loss: 2.523475]\n",
      "2759 [D loss: 0.388102, acc.: 97.66%] [G loss: 2.660149]\n",
      "2760 [D loss: 0.260652, acc.: 98.44%] [G loss: 2.723229]\n",
      "2761 [D loss: 0.008640, acc.: 100.00%] [G loss: 2.940111]\n",
      "2762 [D loss: 0.131776, acc.: 99.22%] [G loss: 3.195711]\n",
      "2763 [D loss: 0.383632, acc.: 97.66%] [G loss: 2.974149]\n",
      "2764 [D loss: 0.260735, acc.: 98.44%] [G loss: 2.915303]\n",
      "2765 [D loss: 0.260598, acc.: 98.44%] [G loss: 2.929737]\n",
      "2766 [D loss: 0.162626, acc.: 98.44%] [G loss: 3.015715]\n",
      "2767 [D loss: 0.135353, acc.: 99.22%] [G loss: 2.831820]\n",
      "2768 [D loss: 0.258577, acc.: 98.44%] [G loss: 2.740697]\n",
      "2769 [D loss: 0.006695, acc.: 100.00%] [G loss: 2.764287]\n",
      "2770 [D loss: 0.006038, acc.: 100.00%] [G loss: 2.840412]\n",
      "2771 [D loss: 0.006842, acc.: 100.00%] [G loss: 2.857973]\n",
      "2772 [D loss: 0.006765, acc.: 100.00%] [G loss: 2.961760]\n",
      "2773 [D loss: 0.257594, acc.: 98.44%] [G loss: 3.048774]\n",
      "2774 [D loss: 0.132530, acc.: 99.22%] [G loss: 2.893858]\n",
      "2775 [D loss: 0.010611, acc.: 100.00%] [G loss: 2.660228]\n",
      "2776 [D loss: 0.264410, acc.: 98.44%] [G loss: 2.617425]\n",
      "2777 [D loss: 0.260731, acc.: 98.44%] [G loss: 2.772489]\n",
      "2778 [D loss: 0.131959, acc.: 99.22%] [G loss: 3.382094]\n",
      "2779 [D loss: 0.382038, acc.: 97.66%] [G loss: 3.620166]\n",
      "2780 [D loss: 0.004429, acc.: 100.00%] [G loss: 3.017310]\n",
      "2781 [D loss: 0.132086, acc.: 99.22%] [G loss: 2.802411]\n",
      "2782 [D loss: 0.256510, acc.: 98.44%] [G loss: 2.924390]\n",
      "2783 [D loss: 0.006752, acc.: 100.00%] [G loss: 2.843071]\n",
      "2784 [D loss: 0.004973, acc.: 100.00%] [G loss: 3.013962]\n",
      "2785 [D loss: 0.131732, acc.: 99.22%] [G loss: 3.210812]\n",
      "2786 [D loss: 0.006361, acc.: 100.00%] [G loss: 3.170710]\n",
      "2787 [D loss: 0.134311, acc.: 99.22%] [G loss: 3.123353]\n",
      "2788 [D loss: 0.003348, acc.: 100.00%] [G loss: 3.253964]\n",
      "2789 [D loss: 0.507867, acc.: 96.88%] [G loss: 3.134001]\n",
      "2790 [D loss: 0.259625, acc.: 98.44%] [G loss: 2.694792]\n",
      "2791 [D loss: 0.135186, acc.: 99.22%] [G loss: 2.844536]\n",
      "2792 [D loss: 0.007358, acc.: 100.00%] [G loss: 2.760119]\n",
      "2793 [D loss: 0.132946, acc.: 99.22%] [G loss: 2.861282]\n",
      "2794 [D loss: 0.133344, acc.: 99.22%] [G loss: 2.812344]\n",
      "2795 [D loss: 0.005691, acc.: 100.00%] [G loss: 2.910227]\n",
      "2796 [D loss: 0.255842, acc.: 98.44%] [G loss: 3.336769]\n",
      "2797 [D loss: 0.255191, acc.: 98.44%] [G loss: 3.466598]\n",
      "2798 [D loss: 0.131913, acc.: 99.22%] [G loss: 2.959464]\n",
      "2799 [D loss: 0.259583, acc.: 98.44%] [G loss: 2.738640]\n",
      "2800 [D loss: 0.130354, acc.: 99.22%] [G loss: 3.038824]\n",
      "2801 [D loss: 0.129419, acc.: 99.22%] [G loss: 3.922060]\n",
      "2802 [D loss: 0.256085, acc.: 98.44%] [G loss: 3.292800]\n",
      "2803 [D loss: 0.134795, acc.: 99.22%] [G loss: 2.774379]\n",
      "2804 [D loss: 0.261589, acc.: 98.44%] [G loss: 2.704748]\n",
      "2805 [D loss: 0.135450, acc.: 99.22%] [G loss: 2.966969]\n",
      "2806 [D loss: 0.013927, acc.: 100.00%] [G loss: 2.880902]\n",
      "2807 [D loss: 0.133074, acc.: 99.22%] [G loss: 3.111718]\n",
      "2808 [D loss: 0.382797, acc.: 97.66%] [G loss: 2.928410]\n",
      "2809 [D loss: 0.256292, acc.: 98.44%] [G loss: 2.894319]\n",
      "2810 [D loss: 0.005365, acc.: 100.00%] [G loss: 2.797765]\n",
      "2811 [D loss: 0.005147, acc.: 100.00%] [G loss: 2.937505]\n",
      "2812 [D loss: 0.256452, acc.: 98.44%] [G loss: 3.155403]\n",
      "2813 [D loss: 0.516820, acc.: 96.09%] [G loss: 3.431573]\n",
      "2814 [D loss: 0.129290, acc.: 99.22%] [G loss: 3.323478]\n",
      "2815 [D loss: 0.132675, acc.: 99.22%] [G loss: 2.963094]\n",
      "2816 [D loss: 0.007961, acc.: 100.00%] [G loss: 2.973496]\n",
      "2817 [D loss: 0.131762, acc.: 99.22%] [G loss: 3.017598]\n",
      "2818 [D loss: 0.128973, acc.: 99.22%] [G loss: 3.642762]\n",
      "2819 [D loss: 0.255035, acc.: 98.44%] [G loss: 3.767646]\n",
      "2820 [D loss: 0.133605, acc.: 99.22%] [G loss: 3.053581]\n",
      "2821 [D loss: 0.136168, acc.: 99.22%] [G loss: 3.086805]\n",
      "2822 [D loss: 0.007748, acc.: 100.00%] [G loss: 3.256566]\n",
      "2823 [D loss: 0.257202, acc.: 98.44%] [G loss: 3.076331]\n",
      "2824 [D loss: 0.132326, acc.: 99.22%] [G loss: 3.051243]\n",
      "2825 [D loss: 0.131945, acc.: 99.22%] [G loss: 3.060337]\n",
      "2826 [D loss: 0.507734, acc.: 96.88%] [G loss: 3.305294]\n",
      "2827 [D loss: 0.254407, acc.: 98.44%] [G loss: 3.642436]\n",
      "2828 [D loss: 0.008367, acc.: 100.00%] [G loss: 3.125768]\n",
      "2829 [D loss: 0.385464, acc.: 97.66%] [G loss: 2.919015]\n",
      "2830 [D loss: 0.135651, acc.: 99.22%] [G loss: 2.819452]\n",
      "2831 [D loss: 0.132451, acc.: 99.22%] [G loss: 2.865344]\n",
      "2832 [D loss: 0.135696, acc.: 99.22%] [G loss: 2.747738]\n",
      "2833 [D loss: 0.259155, acc.: 98.44%] [G loss: 2.709335]\n",
      "2834 [D loss: 0.257705, acc.: 98.44%] [G loss: 2.765374]\n",
      "2835 [D loss: 0.004721, acc.: 100.00%] [G loss: 3.021296]\n",
      "2836 [D loss: 0.380896, acc.: 97.66%] [G loss: 3.580741]\n",
      "2837 [D loss: 0.254787, acc.: 98.44%] [G loss: 3.766721]\n",
      "2838 [D loss: 0.128964, acc.: 99.22%] [G loss: 3.680440]\n",
      "2839 [D loss: 0.259669, acc.: 98.44%] [G loss: 2.918742]\n",
      "2840 [D loss: 0.260868, acc.: 98.44%] [G loss: 2.800306]\n",
      "2841 [D loss: 0.258001, acc.: 98.44%] [G loss: 2.983510]\n",
      "2842 [D loss: 0.258135, acc.: 98.44%] [G loss: 2.996860]\n",
      "2843 [D loss: 0.129903, acc.: 99.22%] [G loss: 3.055170]\n",
      "2844 [D loss: 0.131685, acc.: 99.22%] [G loss: 3.096992]\n",
      "2845 [D loss: 0.257402, acc.: 98.44%] [G loss: 3.168864]\n",
      "2846 [D loss: 0.762060, acc.: 95.31%] [G loss: 3.083149]\n",
      "2847 [D loss: 0.005999, acc.: 100.00%] [G loss: 3.113464]\n",
      "2848 [D loss: 0.004955, acc.: 100.00%] [G loss: 3.028954]\n",
      "2849 [D loss: 0.257440, acc.: 98.44%] [G loss: 3.256633]\n",
      "2850 [D loss: 0.134518, acc.: 99.22%] [G loss: 2.699742]\n",
      "2851 [D loss: 0.384833, acc.: 97.66%] [G loss: 2.723938]\n",
      "2852 [D loss: 0.129803, acc.: 99.22%] [G loss: 3.177794]\n",
      "2853 [D loss: 0.382448, acc.: 97.66%] [G loss: 3.686833]\n",
      "2854 [D loss: 0.129519, acc.: 99.22%] [G loss: 3.791603]\n",
      "2855 [D loss: 0.008562, acc.: 100.00%] [G loss: 3.158107]\n",
      "2856 [D loss: 0.260601, acc.: 98.44%] [G loss: 2.882841]\n",
      "2857 [D loss: 0.256546, acc.: 98.44%] [G loss: 3.177620]\n",
      "2858 [D loss: 0.256556, acc.: 98.44%] [G loss: 3.432605]\n",
      "2859 [D loss: 0.131050, acc.: 99.22%] [G loss: 3.164258]\n",
      "2860 [D loss: 0.132653, acc.: 99.22%] [G loss: 2.881048]\n",
      "2861 [D loss: 0.009818, acc.: 100.00%] [G loss: 2.900905]\n",
      "2862 [D loss: 0.136275, acc.: 99.22%] [G loss: 2.935465]\n",
      "2863 [D loss: 0.130783, acc.: 99.22%] [G loss: 3.346001]\n",
      "2864 [D loss: 0.002186, acc.: 100.00%] [G loss: 4.019838]\n",
      "2865 [D loss: 0.255252, acc.: 98.44%] [G loss: 3.408810]\n",
      "2866 [D loss: 0.128723, acc.: 99.22%] [G loss: 3.405427]\n",
      "2867 [D loss: 0.507293, acc.: 96.88%] [G loss: 3.527106]\n",
      "2868 [D loss: 0.129692, acc.: 99.22%] [G loss: 3.398544]\n",
      "2869 [D loss: 0.258064, acc.: 98.44%] [G loss: 3.120152]\n",
      "2870 [D loss: 0.132764, acc.: 99.22%] [G loss: 2.977602]\n",
      "2871 [D loss: 0.383120, acc.: 97.66%] [G loss: 3.020295]\n",
      "2872 [D loss: 0.004188, acc.: 100.00%] [G loss: 3.284091]\n",
      "2873 [D loss: 0.256569, acc.: 98.44%] [G loss: 3.475427]\n",
      "2874 [D loss: 0.508881, acc.: 96.88%] [G loss: 3.180583]\n",
      "2875 [D loss: 0.132096, acc.: 99.22%] [G loss: 2.900748]\n",
      "2876 [D loss: 0.257652, acc.: 98.44%] [G loss: 2.923236]\n",
      "2877 [D loss: 0.130883, acc.: 99.22%] [G loss: 3.129347]\n",
      "2878 [D loss: 0.129634, acc.: 99.22%] [G loss: 3.349201]\n",
      "2879 [D loss: 0.254640, acc.: 98.44%] [G loss: 3.537167]\n",
      "2880 [D loss: 0.005259, acc.: 100.00%] [G loss: 3.193868]\n",
      "2881 [D loss: 0.382641, acc.: 97.66%] [G loss: 3.101930]\n",
      "2882 [D loss: 0.132368, acc.: 99.22%] [G loss: 3.018441]\n",
      "2883 [D loss: 0.130776, acc.: 99.22%] [G loss: 3.135527]\n",
      "2884 [D loss: 0.130650, acc.: 99.22%] [G loss: 3.228835]\n",
      "2885 [D loss: 0.256077, acc.: 98.44%] [G loss: 3.411990]\n",
      "2886 [D loss: 0.383319, acc.: 97.66%] [G loss: 3.253318]\n",
      "2887 [D loss: 0.383397, acc.: 97.66%] [G loss: 2.891539]\n",
      "2888 [D loss: 0.383625, acc.: 97.66%] [G loss: 3.041636]\n",
      "2889 [D loss: 0.007721, acc.: 100.00%] [G loss: 3.040998]\n",
      "2890 [D loss: 0.255660, acc.: 98.44%] [G loss: 3.462445]\n",
      "2891 [D loss: 0.003589, acc.: 100.00%] [G loss: 3.527983]\n",
      "2892 [D loss: 0.884691, acc.: 94.53%] [G loss: 3.526201]\n",
      "2893 [D loss: 0.256736, acc.: 98.44%] [G loss: 3.282556]\n",
      "2894 [D loss: 0.004593, acc.: 100.00%] [G loss: 3.298228]\n",
      "2895 [D loss: 0.006261, acc.: 100.00%] [G loss: 3.061929]\n",
      "2896 [D loss: 0.508334, acc.: 96.88%] [G loss: 3.627902]\n",
      "2897 [D loss: 0.255239, acc.: 98.44%] [G loss: 3.176214]\n",
      "2898 [D loss: 0.383012, acc.: 97.66%] [G loss: 3.137662]\n",
      "2899 [D loss: 0.130069, acc.: 99.22%] [G loss: 3.601434]\n",
      "2900 [D loss: 0.255473, acc.: 98.44%] [G loss: 3.374313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2901 [D loss: 0.382438, acc.: 97.66%] [G loss: 3.020373]\n",
      "2902 [D loss: 0.509728, acc.: 96.88%] [G loss: 2.965848]\n",
      "2903 [D loss: 0.130549, acc.: 99.22%] [G loss: 3.169009]\n",
      "2904 [D loss: 0.004162, acc.: 100.00%] [G loss: 3.233155]\n",
      "2905 [D loss: 0.004326, acc.: 100.00%] [G loss: 3.190792]\n",
      "2906 [D loss: 0.255613, acc.: 98.44%] [G loss: 3.035709]\n",
      "2907 [D loss: 0.381797, acc.: 97.66%] [G loss: 3.468548]\n",
      "2908 [D loss: 0.255015, acc.: 98.44%] [G loss: 3.734154]\n",
      "2909 [D loss: 0.128754, acc.: 99.22%] [G loss: 3.619770]\n",
      "2910 [D loss: 0.255540, acc.: 98.44%] [G loss: 3.551412]\n",
      "2911 [D loss: 0.382582, acc.: 97.66%] [G loss: 3.197342]\n",
      "2912 [D loss: 0.385378, acc.: 97.66%] [G loss: 2.817852]\n",
      "2913 [D loss: 0.133170, acc.: 99.22%] [G loss: 2.887771]\n",
      "2914 [D loss: 0.635230, acc.: 96.09%] [G loss: 3.172328]\n",
      "2915 [D loss: 0.380540, acc.: 97.66%] [G loss: 3.699512]\n",
      "2916 [D loss: 0.380977, acc.: 97.66%] [G loss: 3.492346]\n",
      "2917 [D loss: 0.507108, acc.: 96.88%] [G loss: 3.295983]\n",
      "2918 [D loss: 0.384154, acc.: 97.66%] [G loss: 3.115415]\n",
      "2919 [D loss: 0.382240, acc.: 97.66%] [G loss: 3.058263]\n",
      "2920 [D loss: 0.130064, acc.: 99.22%] [G loss: 3.219488]\n",
      "2921 [D loss: 0.129155, acc.: 99.22%] [G loss: 3.388305]\n",
      "2922 [D loss: 0.128930, acc.: 99.22%] [G loss: 3.241461]\n",
      "2923 [D loss: 0.257408, acc.: 98.44%] [G loss: 2.954300]\n",
      "2924 [D loss: 0.130060, acc.: 99.22%] [G loss: 3.047934]\n",
      "2925 [D loss: 0.002765, acc.: 100.00%] [G loss: 3.359020]\n",
      "2926 [D loss: 0.633256, acc.: 96.09%] [G loss: 3.589859]\n",
      "2927 [D loss: 0.128575, acc.: 99.22%] [G loss: 3.684449]\n",
      "2928 [D loss: 0.005362, acc.: 100.00%] [G loss: 2.995779]\n",
      "2929 [D loss: 0.132098, acc.: 99.22%] [G loss: 2.908554]\n",
      "2930 [D loss: 0.257921, acc.: 98.44%] [G loss: 2.940456]\n",
      "2931 [D loss: 0.129630, acc.: 99.22%] [G loss: 3.236335]\n",
      "2932 [D loss: 0.287414, acc.: 97.66%] [G loss: 3.489478]\n",
      "2933 [D loss: 0.002814, acc.: 100.00%] [G loss: 3.468568]\n",
      "2934 [D loss: 0.506193, acc.: 96.88%] [G loss: 3.239337]\n",
      "2935 [D loss: 0.256072, acc.: 98.44%] [G loss: 3.211637]\n",
      "2936 [D loss: 0.382319, acc.: 97.66%] [G loss: 3.177642]\n",
      "2937 [D loss: 0.130775, acc.: 99.22%] [G loss: 3.080689]\n",
      "2938 [D loss: 0.003250, acc.: 100.00%] [G loss: 3.200112]\n",
      "2939 [D loss: 0.132842, acc.: 99.22%] [G loss: 3.134432]\n",
      "2940 [D loss: 0.132424, acc.: 99.22%] [G loss: 3.213406]\n",
      "2941 [D loss: 0.002833, acc.: 100.00%] [G loss: 3.558227]\n",
      "2942 [D loss: 0.128434, acc.: 99.22%] [G loss: 3.544300]\n",
      "2943 [D loss: 0.002620, acc.: 100.00%] [G loss: 3.520236]\n",
      "2944 [D loss: 0.128680, acc.: 99.22%] [G loss: 3.434056]\n",
      "2945 [D loss: 0.381713, acc.: 97.66%] [G loss: 3.199344]\n",
      "2946 [D loss: 0.129240, acc.: 99.22%] [G loss: 3.228706]\n",
      "2947 [D loss: 0.003702, acc.: 100.00%] [G loss: 3.065083]\n",
      "2948 [D loss: 0.380556, acc.: 97.66%] [G loss: 3.253214]\n",
      "2949 [D loss: 0.254859, acc.: 98.44%] [G loss: 3.262278]\n",
      "2950 [D loss: 0.507504, acc.: 96.88%] [G loss: 3.274719]\n",
      "2951 [D loss: 0.129553, acc.: 99.22%] [G loss: 3.257627]\n",
      "2952 [D loss: 0.381273, acc.: 97.66%] [G loss: 3.216041]\n",
      "2953 [D loss: 0.128809, acc.: 99.22%] [G loss: 3.410419]\n",
      "2954 [D loss: 0.128900, acc.: 99.22%] [G loss: 3.691562]\n",
      "2955 [D loss: 0.128628, acc.: 99.22%] [G loss: 3.601773]\n",
      "2956 [D loss: 0.382908, acc.: 97.66%] [G loss: 3.069818]\n",
      "2957 [D loss: 0.130679, acc.: 99.22%] [G loss: 3.218976]\n",
      "2958 [D loss: 0.254462, acc.: 98.44%] [G loss: 3.454839]\n",
      "2959 [D loss: 0.254312, acc.: 98.44%] [G loss: 3.555221]\n",
      "2960 [D loss: 0.129187, acc.: 99.22%] [G loss: 3.268089]\n",
      "2961 [D loss: 0.130154, acc.: 99.22%] [G loss: 3.078463]\n",
      "2962 [D loss: 0.003844, acc.: 100.00%] [G loss: 3.132658]\n",
      "2963 [D loss: 0.006664, acc.: 100.00%] [G loss: 3.308950]\n",
      "2964 [D loss: 0.255539, acc.: 98.44%] [G loss: 3.386533]\n",
      "2965 [D loss: 0.256098, acc.: 98.44%] [G loss: 3.006045]\n",
      "2966 [D loss: 0.003733, acc.: 100.00%] [G loss: 3.208552]\n",
      "2967 [D loss: 0.255248, acc.: 98.44%] [G loss: 3.522345]\n",
      "2968 [D loss: 0.380725, acc.: 97.66%] [G loss: 3.471074]\n",
      "2969 [D loss: 0.003599, acc.: 100.00%] [G loss: 3.382654]\n",
      "2970 [D loss: 0.255984, acc.: 98.44%] [G loss: 3.095572]\n",
      "2971 [D loss: 0.380548, acc.: 97.66%] [G loss: 3.271313]\n",
      "2972 [D loss: 0.128141, acc.: 99.22%] [G loss: 3.613331]\n",
      "2973 [D loss: 0.002876, acc.: 100.00%] [G loss: 3.564157]\n",
      "2974 [D loss: 0.256263, acc.: 98.44%] [G loss: 3.196121]\n",
      "2975 [D loss: 0.129097, acc.: 99.22%] [G loss: 3.099720]\n",
      "2976 [D loss: 0.258690, acc.: 98.44%] [G loss: 3.143210]\n",
      "2977 [D loss: 0.129102, acc.: 99.22%] [G loss: 3.315467]\n",
      "2978 [D loss: 0.128109, acc.: 99.22%] [G loss: 3.600523]\n",
      "2979 [D loss: 0.381073, acc.: 97.66%] [G loss: 3.456417]\n",
      "2980 [D loss: 0.380522, acc.: 97.66%] [G loss: 3.498085]\n",
      "2981 [D loss: 0.255097, acc.: 98.44%] [G loss: 3.367539]\n",
      "2982 [D loss: 0.254428, acc.: 98.44%] [G loss: 3.585576]\n",
      "2983 [D loss: 0.129367, acc.: 99.22%] [G loss: 3.131041]\n",
      "2984 [D loss: 0.255790, acc.: 98.44%] [G loss: 3.195254]\n",
      "2985 [D loss: 0.130664, acc.: 99.22%] [G loss: 3.021113]\n",
      "2986 [D loss: 0.381112, acc.: 97.66%] [G loss: 2.860122]\n",
      "2987 [D loss: 0.003517, acc.: 100.00%] [G loss: 2.978656]\n",
      "2988 [D loss: 0.003138, acc.: 100.00%] [G loss: 3.316777]\n",
      "2989 [D loss: 0.506100, acc.: 96.88%] [G loss: 3.835351]\n",
      "2990 [D loss: 0.757465, acc.: 95.31%] [G loss: 3.769896]\n",
      "2991 [D loss: 0.380071, acc.: 97.66%] [G loss: 3.788904]\n",
      "2992 [D loss: 0.380876, acc.: 97.66%] [G loss: 3.260933]\n",
      "2993 [D loss: 0.128994, acc.: 99.22%] [G loss: 3.183863]\n",
      "2994 [D loss: 0.002852, acc.: 100.00%] [G loss: 3.179422]\n",
      "2995 [D loss: 0.129385, acc.: 99.22%] [G loss: 3.342226]\n",
      "2996 [D loss: 0.129250, acc.: 99.22%] [G loss: 3.142233]\n",
      "2997 [D loss: 0.255168, acc.: 98.44%] [G loss: 2.964792]\n",
      "2998 [D loss: 0.129109, acc.: 99.22%] [G loss: 3.137238]\n",
      "2999 [D loss: 0.381708, acc.: 97.66%] [G loss: 3.174460]\n",
      "3000 [D loss: 0.003963, acc.: 100.00%] [G loss: 3.000588]\n",
      "3001 [D loss: 0.393183, acc.: 96.88%] [G loss: 3.335206]\n",
      "3002 [D loss: 0.254384, acc.: 98.44%] [G loss: 3.261576]\n",
      "3003 [D loss: 0.130497, acc.: 99.22%] [G loss: 3.790439]\n",
      "3004 [D loss: 0.127433, acc.: 99.22%] [G loss: 3.942510]\n",
      "3005 [D loss: 0.505872, acc.: 96.88%] [G loss: 3.700874]\n",
      "3006 [D loss: 0.004271, acc.: 100.00%] [G loss: 3.338981]\n",
      "3007 [D loss: 0.381453, acc.: 97.66%] [G loss: 3.298575]\n",
      "3008 [D loss: 0.130539, acc.: 99.22%] [G loss: 3.009568]\n",
      "3009 [D loss: 0.382445, acc.: 97.66%] [G loss: 3.145635]\n",
      "3010 [D loss: 0.255182, acc.: 98.44%] [G loss: 3.096212]\n",
      "3011 [D loss: 0.255049, acc.: 98.44%] [G loss: 3.276653]\n",
      "3012 [D loss: 0.632469, acc.: 96.09%] [G loss: 3.339871]\n",
      "3013 [D loss: 0.253925, acc.: 98.44%] [G loss: 3.452576]\n",
      "3014 [D loss: 0.129575, acc.: 99.22%] [G loss: 3.365020]\n",
      "3015 [D loss: 0.632959, acc.: 96.09%] [G loss: 3.182846]\n",
      "3016 [D loss: 0.129464, acc.: 99.22%] [G loss: 3.198151]\n",
      "3017 [D loss: 0.758285, acc.: 95.31%] [G loss: 3.399300]\n",
      "3018 [D loss: 0.381080, acc.: 97.66%] [G loss: 3.226199]\n",
      "3019 [D loss: 0.506507, acc.: 96.88%] [G loss: 3.345024]\n",
      "3020 [D loss: 0.505632, acc.: 96.88%] [G loss: 3.478868]\n",
      "3021 [D loss: 0.254507, acc.: 98.44%] [G loss: 3.251669]\n",
      "3022 [D loss: 0.255051, acc.: 98.44%] [G loss: 3.303089]\n",
      "3023 [D loss: 0.255545, acc.: 98.44%] [G loss: 3.085052]\n",
      "3024 [D loss: 0.003033, acc.: 100.00%] [G loss: 3.034394]\n",
      "3025 [D loss: 0.002432, acc.: 100.00%] [G loss: 3.231791]\n",
      "3026 [D loss: 0.380325, acc.: 97.66%] [G loss: 3.309837]\n",
      "3027 [D loss: 0.379858, acc.: 97.66%] [G loss: 3.605574]\n",
      "3028 [D loss: 0.380940, acc.: 97.66%] [G loss: 3.379254]\n",
      "3029 [D loss: 0.254125, acc.: 98.44%] [G loss: 3.460819]\n",
      "3030 [D loss: 0.507024, acc.: 96.88%] [G loss: 3.160303]\n",
      "3031 [D loss: 0.381095, acc.: 97.66%] [G loss: 3.168612]\n",
      "3032 [D loss: 0.129241, acc.: 99.22%] [G loss: 3.092976]\n",
      "3033 [D loss: 0.129374, acc.: 99.22%] [G loss: 3.117090]\n",
      "3034 [D loss: 0.129150, acc.: 99.22%] [G loss: 3.223580]\n",
      "3035 [D loss: 0.254364, acc.: 98.44%] [G loss: 3.440562]\n",
      "3036 [D loss: 0.380113, acc.: 97.66%] [G loss: 3.706073]\n",
      "3037 [D loss: 0.129945, acc.: 99.22%] [G loss: 3.511237]\n",
      "3038 [D loss: 0.380796, acc.: 97.66%] [G loss: 3.242537]\n",
      "3039 [D loss: 0.003647, acc.: 100.00%] [G loss: 3.472844]\n",
      "3040 [D loss: 0.254197, acc.: 98.44%] [G loss: 3.400981]\n",
      "3041 [D loss: 0.002934, acc.: 100.00%] [G loss: 3.350911]\n",
      "3042 [D loss: 0.128470, acc.: 99.22%] [G loss: 3.327568]\n",
      "3043 [D loss: 0.381501, acc.: 97.66%] [G loss: 3.171890]\n",
      "3044 [D loss: 0.254235, acc.: 98.44%] [G loss: 3.256979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3045 [D loss: 0.632639, acc.: 96.09%] [G loss: 3.409022]\n",
      "3046 [D loss: 0.381034, acc.: 97.66%] [G loss: 3.557764]\n",
      "3047 [D loss: 0.507426, acc.: 96.88%] [G loss: 3.110640]\n",
      "3048 [D loss: 0.255289, acc.: 98.44%] [G loss: 3.225723]\n",
      "3049 [D loss: 0.128302, acc.: 99.22%] [G loss: 3.606665]\n",
      "3050 [D loss: 0.002664, acc.: 100.00%] [G loss: 3.464699]\n",
      "3051 [D loss: 0.255072, acc.: 98.44%] [G loss: 3.477468]\n",
      "3052 [D loss: 0.255101, acc.: 98.44%] [G loss: 3.237553]\n",
      "3053 [D loss: 0.254949, acc.: 98.44%] [G loss: 3.317204]\n",
      "3054 [D loss: 0.254139, acc.: 98.44%] [G loss: 3.652123]\n",
      "3055 [D loss: 0.253693, acc.: 98.44%] [G loss: 4.096678]\n",
      "3056 [D loss: 0.128072, acc.: 99.22%] [G loss: 3.641603]\n",
      "3057 [D loss: 0.130062, acc.: 99.22%] [G loss: 3.327340]\n",
      "3058 [D loss: 0.255927, acc.: 98.44%] [G loss: 3.154578]\n",
      "3059 [D loss: 0.254631, acc.: 98.44%] [G loss: 3.187774]\n",
      "3060 [D loss: 0.380453, acc.: 97.66%] [G loss: 3.362757]\n",
      "3061 [D loss: 0.002352, acc.: 100.00%] [G loss: 3.361473]\n",
      "3062 [D loss: 0.254563, acc.: 98.44%] [G loss: 3.387364]\n",
      "3063 [D loss: 0.632490, acc.: 96.09%] [G loss: 3.385469]\n",
      "3064 [D loss: 0.128424, acc.: 99.22%] [G loss: 3.434982]\n",
      "3065 [D loss: 0.128315, acc.: 99.22%] [G loss: 3.421376]\n",
      "3066 [D loss: 0.275446, acc.: 97.66%] [G loss: 2.404615]\n",
      "3067 [D loss: 0.039180, acc.: 100.00%] [G loss: 3.401717]\n",
      "3068 [D loss: 0.211057, acc.: 90.62%] [G loss: 4.663005]\n",
      "3069 [D loss: 1.946524, acc.: 69.53%] [G loss: 12.649480]\n",
      "3070 [D loss: 0.703494, acc.: 93.75%] [G loss: 14.142403]\n",
      "3071 [D loss: 0.647902, acc.: 92.97%] [G loss: 13.979392]\n",
      "3072 [D loss: 0.004929, acc.: 100.00%] [G loss: 11.256919]\n",
      "3073 [D loss: 0.158465, acc.: 92.19%] [G loss: 13.162937]\n",
      "3074 [D loss: 0.000013, acc.: 100.00%] [G loss: 14.308095]\n",
      "3075 [D loss: 0.000056, acc.: 100.00%] [G loss: 13.279235]\n",
      "3076 [D loss: 0.000077, acc.: 100.00%] [G loss: 11.810163]\n",
      "3077 [D loss: 0.001488, acc.: 100.00%] [G loss: 11.220428]\n",
      "3078 [D loss: 0.015629, acc.: 100.00%] [G loss: 9.465439]\n",
      "3079 [D loss: 0.041740, acc.: 100.00%] [G loss: 10.319434]\n",
      "3080 [D loss: 0.001624, acc.: 100.00%] [G loss: 9.606657]\n",
      "3081 [D loss: 0.016847, acc.: 100.00%] [G loss: 8.904778]\n",
      "3082 [D loss: 0.034034, acc.: 100.00%] [G loss: 9.723694]\n",
      "3083 [D loss: 0.001080, acc.: 100.00%] [G loss: 9.510358]\n",
      "3084 [D loss: 0.020739, acc.: 100.00%] [G loss: 8.351593]\n",
      "3085 [D loss: 0.171153, acc.: 99.22%] [G loss: 11.387843]\n",
      "3086 [D loss: 0.126492, acc.: 99.22%] [G loss: 11.062586]\n",
      "3087 [D loss: 0.012863, acc.: 100.00%] [G loss: 7.530773]\n",
      "3088 [D loss: 0.236708, acc.: 87.50%] [G loss: 12.793724]\n",
      "3089 [D loss: 0.002426, acc.: 100.00%] [G loss: 12.571758]\n",
      "3090 [D loss: 0.144676, acc.: 93.75%] [G loss: 12.734956]\n",
      "3091 [D loss: 0.468479, acc.: 93.75%] [G loss: 12.844275]\n",
      "3092 [D loss: 0.478931, acc.: 87.50%] [G loss: 14.490624]\n",
      "3093 [D loss: 0.000014, acc.: 100.00%] [G loss: 15.023005]\n",
      "3094 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.735937]\n",
      "3095 [D loss: 0.001763, acc.: 100.00%] [G loss: 10.172132]\n",
      "3096 [D loss: 0.436308, acc.: 87.50%] [G loss: 12.491802]\n",
      "3097 [D loss: 0.009916, acc.: 100.00%] [G loss: 11.688295]\n",
      "3098 [D loss: 0.332702, acc.: 88.28%] [G loss: 14.642569]\n",
      "3099 [D loss: 0.126055, acc.: 99.22%] [G loss: 14.614843]\n",
      "3100 [D loss: 0.054802, acc.: 98.44%] [G loss: 14.368473]\n",
      "3101 [D loss: 0.012126, acc.: 99.22%] [G loss: 13.530455]\n",
      "3102 [D loss: 0.285820, acc.: 92.19%] [G loss: 14.635832]\n",
      "3103 [D loss: 0.125973, acc.: 99.22%] [G loss: 14.652966]\n",
      "3104 [D loss: 0.000913, acc.: 100.00%] [G loss: 14.119858]\n",
      "3105 [D loss: 0.158483, acc.: 98.44%] [G loss: 13.300480]\n",
      "3106 [D loss: 0.283106, acc.: 94.53%] [G loss: 14.667635]\n",
      "3107 [D loss: 0.000006, acc.: 100.00%] [G loss: 14.390957]\n",
      "3108 [D loss: 0.126203, acc.: 99.22%] [G loss: 13.206879]\n",
      "3109 [D loss: 0.063996, acc.: 96.88%] [G loss: 12.348528]\n",
      "3110 [D loss: 0.355878, acc.: 94.53%] [G loss: 12.289951]\n",
      "3111 [D loss: 0.000245, acc.: 100.00%] [G loss: 11.855343]\n",
      "3112 [D loss: 0.165895, acc.: 98.44%] [G loss: 9.905794]\n",
      "3113 [D loss: 0.281285, acc.: 90.62%] [G loss: 11.442223]\n",
      "3114 [D loss: 0.378323, acc.: 97.66%] [G loss: 10.647428]\n",
      "3115 [D loss: 0.424790, acc.: 87.50%] [G loss: 13.536919]\n",
      "3116 [D loss: 0.126038, acc.: 99.22%] [G loss: 13.461815]\n",
      "3117 [D loss: 0.000392, acc.: 100.00%] [G loss: 12.213541]\n",
      "3118 [D loss: 0.134139, acc.: 99.22%] [G loss: 11.889019]\n",
      "3119 [D loss: 0.265021, acc.: 98.44%] [G loss: 10.431984]\n",
      "3120 [D loss: 0.147324, acc.: 99.22%] [G loss: 8.705992]\n",
      "3121 [D loss: 0.320467, acc.: 95.31%] [G loss: 7.948973]\n",
      "3122 [D loss: 0.457315, acc.: 94.53%] [G loss: 8.258207]\n",
      "3123 [D loss: 0.007402, acc.: 100.00%] [G loss: 7.320938]\n",
      "3124 [D loss: 0.152610, acc.: 92.19%] [G loss: 9.086749]\n",
      "3125 [D loss: 0.131440, acc.: 99.22%] [G loss: 9.071615]\n",
      "3126 [D loss: 0.016255, acc.: 100.00%] [G loss: 6.256073]\n",
      "3127 [D loss: 0.419668, acc.: 81.25%] [G loss: 10.680490]\n",
      "3128 [D loss: 0.257050, acc.: 98.44%] [G loss: 11.650703]\n",
      "3129 [D loss: 0.130169, acc.: 99.22%] [G loss: 10.696684]\n",
      "3130 [D loss: 0.385675, acc.: 97.66%] [G loss: 8.863194]\n",
      "3131 [D loss: 0.258007, acc.: 98.44%] [G loss: 6.690430]\n",
      "3132 [D loss: 0.044202, acc.: 99.22%] [G loss: 4.833946]\n",
      "3133 [D loss: 0.041492, acc.: 99.22%] [G loss: 3.998657]\n",
      "3134 [D loss: 0.116697, acc.: 95.31%] [G loss: 7.400377]\n",
      "3135 [D loss: 0.254755, acc.: 98.44%] [G loss: 8.137145]\n",
      "3136 [D loss: 0.129152, acc.: 99.22%] [G loss: 4.979340]\n",
      "3137 [D loss: 0.557743, acc.: 84.38%] [G loss: 8.780126]\n",
      "3138 [D loss: 0.634894, acc.: 96.09%] [G loss: 10.216490]\n",
      "3139 [D loss: 0.387189, acc.: 97.66%] [G loss: 8.232938]\n",
      "3140 [D loss: 0.129988, acc.: 99.22%] [G loss: 5.957077]\n",
      "3141 [D loss: 0.266238, acc.: 98.44%] [G loss: 4.492315]\n",
      "3142 [D loss: 0.179775, acc.: 99.22%] [G loss: 4.021636]\n",
      "3143 [D loss: 0.660413, acc.: 96.09%] [G loss: 4.220703]\n",
      "3144 [D loss: 0.158192, acc.: 99.22%] [G loss: 4.444778]\n",
      "3145 [D loss: 0.023457, acc.: 100.00%] [G loss: 3.999991]\n",
      "3146 [D loss: 0.227953, acc.: 96.09%] [G loss: 6.037237]\n",
      "3147 [D loss: 0.508002, acc.: 96.88%] [G loss: 6.320471]\n",
      "3148 [D loss: 0.256384, acc.: 98.44%] [G loss: 4.461910]\n",
      "3149 [D loss: 0.304045, acc.: 98.44%] [G loss: 5.007966]\n",
      "3150 [D loss: 0.150653, acc.: 98.44%] [G loss: 4.671120]\n",
      "3151 [D loss: 0.018401, acc.: 100.00%] [G loss: 4.179152]\n",
      "3152 [D loss: 0.409933, acc.: 97.66%] [G loss: 4.734456]\n",
      "3153 [D loss: 0.646975, acc.: 96.09%] [G loss: 4.687139]\n",
      "3154 [D loss: 0.649214, acc.: 96.09%] [G loss: 4.517914]\n",
      "3155 [D loss: 0.154848, acc.: 98.44%] [G loss: 4.239571]\n",
      "3156 [D loss: 0.043719, acc.: 98.44%] [G loss: 5.517295]\n",
      "3157 [D loss: 0.253655, acc.: 98.44%] [G loss: 5.000969]\n",
      "3158 [D loss: 0.516558, acc.: 96.88%] [G loss: 4.188019]\n",
      "3159 [D loss: 0.285243, acc.: 97.66%] [G loss: 4.677903]\n",
      "3160 [D loss: 0.298529, acc.: 96.88%] [G loss: 4.755189]\n",
      "3161 [D loss: 0.192481, acc.: 96.09%] [G loss: 6.557199]\n",
      "3162 [D loss: 0.631620, acc.: 96.09%] [G loss: 6.622103]\n",
      "3163 [D loss: 0.003589, acc.: 100.00%] [G loss: 5.328479]\n",
      "3164 [D loss: 0.317284, acc.: 97.66%] [G loss: 4.702637]\n",
      "3165 [D loss: 0.277814, acc.: 98.44%] [G loss: 5.585353]\n",
      "3166 [D loss: 0.006895, acc.: 100.00%] [G loss: 5.397670]\n",
      "3167 [D loss: 0.258072, acc.: 98.44%] [G loss: 5.000813]\n",
      "3168 [D loss: 0.400152, acc.: 97.66%] [G loss: 5.394441]\n",
      "3169 [D loss: 0.256858, acc.: 98.44%] [G loss: 5.007774]\n",
      "3170 [D loss: 0.533418, acc.: 96.88%] [G loss: 5.819033]\n",
      "3171 [D loss: 0.254645, acc.: 98.44%] [G loss: 5.461290]\n",
      "3172 [D loss: 0.650254, acc.: 95.31%] [G loss: 5.336149]\n",
      "3173 [D loss: 0.449298, acc.: 96.88%] [G loss: 5.389598]\n",
      "3174 [D loss: 0.130895, acc.: 99.22%] [G loss: 4.916903]\n",
      "3175 [D loss: 0.284202, acc.: 97.66%] [G loss: 5.651258]\n",
      "3176 [D loss: 0.631030, acc.: 96.09%] [G loss: 5.937180]\n",
      "3177 [D loss: 0.255065, acc.: 98.44%] [G loss: 4.970676]\n",
      "3178 [D loss: 0.018348, acc.: 100.00%] [G loss: 4.838631]\n",
      "3179 [D loss: 0.511175, acc.: 96.88%] [G loss: 4.824098]\n",
      "3180 [D loss: 0.136440, acc.: 99.22%] [G loss: 4.949137]\n",
      "3181 [D loss: 0.635367, acc.: 96.09%] [G loss: 4.720118]\n",
      "3182 [D loss: 0.534992, acc.: 96.88%] [G loss: 6.093985]\n",
      "3183 [D loss: 0.504732, acc.: 96.88%] [G loss: 5.900421]\n",
      "3184 [D loss: 0.253270, acc.: 98.44%] [G loss: 4.654445]\n",
      "3185 [D loss: 0.132370, acc.: 99.22%] [G loss: 4.351667]\n",
      "3186 [D loss: 0.149629, acc.: 99.22%] [G loss: 5.673576]\n",
      "3187 [D loss: 0.379461, acc.: 97.66%] [G loss: 5.250690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188 [D loss: 0.259883, acc.: 98.44%] [G loss: 4.453894]\n",
      "3189 [D loss: 0.012823, acc.: 100.00%] [G loss: 3.767069]\n",
      "3190 [D loss: 0.261727, acc.: 98.44%] [G loss: 3.920403]\n",
      "3191 [D loss: 0.515600, acc.: 96.88%] [G loss: 4.237630]\n",
      "3192 [D loss: 0.509490, acc.: 96.88%] [G loss: 3.715225]\n",
      "3193 [D loss: 0.146909, acc.: 99.22%] [G loss: 4.307649]\n",
      "3194 [D loss: 0.005385, acc.: 100.00%] [G loss: 4.157875]\n",
      "3195 [D loss: 0.259594, acc.: 98.44%] [G loss: 3.616746]\n",
      "3196 [D loss: 0.135939, acc.: 99.22%] [G loss: 3.214133]\n",
      "3197 [D loss: 0.268821, acc.: 98.44%] [G loss: 3.852602]\n",
      "3198 [D loss: 0.506464, acc.: 96.88%] [G loss: 4.152032]\n",
      "3199 [D loss: 0.008086, acc.: 100.00%] [G loss: 4.348123]\n",
      "3200 [D loss: 0.131179, acc.: 99.22%] [G loss: 4.267550]\n",
      "3201 [D loss: 0.138553, acc.: 99.22%] [G loss: 4.112295]\n",
      "3202 [D loss: 0.268890, acc.: 98.44%] [G loss: 3.840128]\n",
      "3203 [D loss: 0.399640, acc.: 97.66%] [G loss: 4.194412]\n",
      "3204 [D loss: 0.255997, acc.: 98.44%] [G loss: 4.417566]\n",
      "3205 [D loss: 0.132274, acc.: 99.22%] [G loss: 4.042097]\n",
      "3206 [D loss: 0.383290, acc.: 97.66%] [G loss: 3.806418]\n",
      "3207 [D loss: 0.384257, acc.: 97.66%] [G loss: 4.119654]\n",
      "3208 [D loss: 0.009425, acc.: 100.00%] [G loss: 4.000248]\n",
      "3209 [D loss: 0.004450, acc.: 100.00%] [G loss: 4.237988]\n",
      "3210 [D loss: 0.131255, acc.: 99.22%] [G loss: 3.936894]\n",
      "3211 [D loss: 0.515180, acc.: 96.88%] [G loss: 3.586912]\n",
      "3212 [D loss: 0.508459, acc.: 96.88%] [G loss: 3.593289]\n",
      "3213 [D loss: 0.005400, acc.: 100.00%] [G loss: 3.453122]\n",
      "3214 [D loss: 0.262244, acc.: 98.44%] [G loss: 3.507995]\n",
      "3215 [D loss: 0.137174, acc.: 99.22%] [G loss: 3.561121]\n",
      "3216 [D loss: 0.256711, acc.: 98.44%] [G loss: 3.502531]\n",
      "3217 [D loss: 0.257114, acc.: 98.44%] [G loss: 3.814964]\n",
      "3218 [D loss: 0.255868, acc.: 98.44%] [G loss: 3.960649]\n",
      "3219 [D loss: 0.007812, acc.: 100.00%] [G loss: 3.622002]\n",
      "3220 [D loss: 0.259974, acc.: 98.44%] [G loss: 3.369310]\n",
      "3221 [D loss: 0.383288, acc.: 97.66%] [G loss: 3.431455]\n",
      "3222 [D loss: 0.384943, acc.: 97.66%] [G loss: 3.472370]\n",
      "3223 [D loss: 0.131351, acc.: 99.22%] [G loss: 3.415318]\n",
      "3224 [D loss: 0.384216, acc.: 97.66%] [G loss: 3.339972]\n",
      "3225 [D loss: 0.759661, acc.: 95.31%] [G loss: 3.222339]\n",
      "3226 [D loss: 0.275756, acc.: 97.66%] [G loss: 2.935118]\n",
      "3227 [D loss: 0.259471, acc.: 98.44%] [G loss: 3.333230]\n",
      "3228 [D loss: 0.005232, acc.: 100.00%] [G loss: 3.261689]\n",
      "3229 [D loss: 0.130753, acc.: 99.22%] [G loss: 3.462910]\n",
      "3230 [D loss: 0.130381, acc.: 99.22%] [G loss: 3.632007]\n",
      "3231 [D loss: 0.387588, acc.: 97.66%] [G loss: 3.511668]\n",
      "3232 [D loss: 0.129435, acc.: 99.22%] [G loss: 3.357613]\n",
      "3233 [D loss: 0.006961, acc.: 100.00%] [G loss: 3.315649]\n",
      "3234 [D loss: 0.131360, acc.: 99.22%] [G loss: 3.552554]\n",
      "3235 [D loss: 0.382044, acc.: 97.66%] [G loss: 3.706311]\n",
      "3236 [D loss: 0.381435, acc.: 97.66%] [G loss: 4.458615]\n",
      "3237 [D loss: 0.129587, acc.: 99.22%] [G loss: 4.553408]\n",
      "3238 [D loss: 0.131987, acc.: 99.22%] [G loss: 3.701680]\n",
      "3239 [D loss: 0.384580, acc.: 97.66%] [G loss: 3.670049]\n",
      "3240 [D loss: 0.135188, acc.: 99.22%] [G loss: 3.877079]\n",
      "3241 [D loss: 0.003048, acc.: 100.00%] [G loss: 3.988970]\n",
      "3242 [D loss: 0.255317, acc.: 98.44%] [G loss: 3.844220]\n",
      "3243 [D loss: 0.129918, acc.: 99.22%] [G loss: 3.812675]\n",
      "3244 [D loss: 0.255491, acc.: 98.44%] [G loss: 3.896156]\n",
      "3245 [D loss: 0.256014, acc.: 98.44%] [G loss: 3.617043]\n",
      "3246 [D loss: 0.509017, acc.: 96.88%] [G loss: 3.555992]\n",
      "3247 [D loss: 0.257897, acc.: 98.44%] [G loss: 3.477455]\n",
      "3248 [D loss: 0.004008, acc.: 100.00%] [G loss: 3.370684]\n",
      "3249 [D loss: 0.382970, acc.: 97.66%] [G loss: 3.324658]\n",
      "3250 [D loss: 0.380732, acc.: 97.66%] [G loss: 3.384500]\n",
      "3251 [D loss: 0.506890, acc.: 96.88%] [G loss: 3.280675]\n",
      "3252 [D loss: 0.255688, acc.: 98.44%] [G loss: 3.428887]\n",
      "3253 [D loss: 0.002630, acc.: 100.00%] [G loss: 3.433683]\n",
      "3254 [D loss: 0.004873, acc.: 100.00%] [G loss: 3.799755]\n",
      "3255 [D loss: 0.379726, acc.: 97.66%] [G loss: 4.047170]\n",
      "3256 [D loss: 0.382248, acc.: 97.66%] [G loss: 3.695274]\n",
      "3257 [D loss: 0.381141, acc.: 97.66%] [G loss: 3.417806]\n",
      "3258 [D loss: 0.381783, acc.: 97.66%] [G loss: 3.888135]\n",
      "3259 [D loss: 0.130473, acc.: 99.22%] [G loss: 3.264609]\n",
      "3260 [D loss: 0.256331, acc.: 98.44%] [G loss: 3.196897]\n",
      "3261 [D loss: 0.004243, acc.: 100.00%] [G loss: 3.279929]\n",
      "3262 [D loss: 0.128802, acc.: 99.22%] [G loss: 3.354076]\n",
      "3263 [D loss: 0.381020, acc.: 97.66%] [G loss: 3.511313]\n",
      "3264 [D loss: 0.632276, acc.: 96.09%] [G loss: 3.668754]\n",
      "3265 [D loss: 0.128913, acc.: 99.22%] [G loss: 3.728433]\n",
      "3266 [D loss: 0.127966, acc.: 99.22%] [G loss: 3.819494]\n",
      "3267 [D loss: 0.632376, acc.: 96.09%] [G loss: 3.991991]\n",
      "3268 [D loss: 0.506106, acc.: 96.88%] [G loss: 3.632259]\n",
      "3269 [D loss: 0.383119, acc.: 97.66%] [G loss: 3.158617]\n",
      "3270 [D loss: 0.005958, acc.: 100.00%] [G loss: 3.453299]\n",
      "3271 [D loss: 0.380364, acc.: 97.66%] [G loss: 3.495193]\n",
      "3272 [D loss: 0.380530, acc.: 97.66%] [G loss: 3.369730]\n",
      "3273 [D loss: 0.002357, acc.: 100.00%] [G loss: 3.513309]\n",
      "3274 [D loss: 0.254533, acc.: 98.44%] [G loss: 3.535681]\n",
      "3275 [D loss: 0.128745, acc.: 99.22%] [G loss: 3.441962]\n",
      "3276 [D loss: 0.380336, acc.: 97.66%] [G loss: 3.474505]\n",
      "3277 [D loss: 0.129464, acc.: 99.22%] [G loss: 3.435068]\n",
      "3278 [D loss: 0.380581, acc.: 97.66%] [G loss: 3.413300]\n",
      "3279 [D loss: 0.380599, acc.: 97.66%] [G loss: 3.190686]\n",
      "3280 [D loss: 0.381451, acc.: 97.66%] [G loss: 3.280316]\n",
      "3281 [D loss: 0.381505, acc.: 97.66%] [G loss: 3.257482]\n",
      "3282 [D loss: 0.129094, acc.: 99.22%] [G loss: 3.551107]\n",
      "3283 [D loss: 0.379860, acc.: 97.66%] [G loss: 3.716331]\n",
      "3284 [D loss: 0.254284, acc.: 98.44%] [G loss: 4.064394]\n",
      "3285 [D loss: 0.631763, acc.: 96.09%] [G loss: 3.910786]\n",
      "3286 [D loss: 0.254539, acc.: 98.44%] [G loss: 3.887463]\n",
      "3287 [D loss: 0.002620, acc.: 100.00%] [G loss: 3.908324]\n",
      "3288 [D loss: 0.255316, acc.: 98.44%] [G loss: 3.416210]\n",
      "3289 [D loss: 0.130215, acc.: 99.22%] [G loss: 3.332787]\n",
      "3290 [D loss: 0.506526, acc.: 96.88%] [G loss: 3.319012]\n",
      "3291 [D loss: 0.380513, acc.: 97.66%] [G loss: 3.749602]\n",
      "3292 [D loss: 0.128544, acc.: 99.22%] [G loss: 3.986093]\n",
      "3293 [D loss: 0.253229, acc.: 98.44%] [G loss: 3.944341]\n",
      "3294 [D loss: 0.255099, acc.: 98.44%] [G loss: 3.392083]\n",
      "3295 [D loss: 0.003236, acc.: 100.00%] [G loss: 3.189563]\n",
      "3296 [D loss: 0.003115, acc.: 100.00%] [G loss: 3.275715]\n",
      "3297 [D loss: 0.002518, acc.: 100.00%] [G loss: 3.574316]\n",
      "3298 [D loss: 0.379432, acc.: 97.66%] [G loss: 3.816088]\n",
      "3299 [D loss: 0.661934, acc.: 92.97%] [G loss: 4.027551]\n",
      "3300 [D loss: 0.399091, acc.: 83.59%] [G loss: 9.394716]\n",
      "3301 [D loss: 2.647898, acc.: 81.25%] [G loss: 10.001713]\n",
      "3302 [D loss: 2.753365, acc.: 82.03%] [G loss: 10.304667]\n",
      "3303 [D loss: 1.594824, acc.: 88.28%] [G loss: 11.728696]\n",
      "3304 [D loss: 1.782383, acc.: 88.28%] [G loss: 12.848391]\n",
      "3305 [D loss: 1.043038, acc.: 92.97%] [G loss: 13.270191]\n",
      "3306 [D loss: 1.127042, acc.: 92.97%] [G loss: 13.896973]\n",
      "3307 [D loss: 0.483216, acc.: 95.31%] [G loss: 15.034281]\n",
      "3308 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.491405]\n",
      "3309 [D loss: 0.159322, acc.: 96.88%] [G loss: 16.118099]\n",
      "3310 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3311 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3312 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3313 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3314 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3315 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3316 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3317 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3318 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118099]\n",
      "3319 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3320 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118099]\n",
      "3321 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3323 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3324 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3325 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3326 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3327 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3328 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3329 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3330 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3331 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3332 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3334 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3335 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3336 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3338 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3339 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3340 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3341 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3342 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3343 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3344 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3345 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3346 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3347 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3348 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3349 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3350 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3351 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3352 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3353 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3354 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3355 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3356 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3357 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3358 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3359 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3360 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3361 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3362 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3363 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3364 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3365 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3366 [D loss: 0.017589, acc.: 99.22%] [G loss: 16.118099]\n",
      "3367 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3368 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3369 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3371 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3372 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3373 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3374 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3375 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3376 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3377 [D loss: 0.000022, acc.: 100.00%] [G loss: 16.118099]\n",
      "3378 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3379 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3380 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3381 [D loss: 0.016442, acc.: 99.22%] [G loss: 16.118099]\n",
      "3382 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3383 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3384 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3385 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3386 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3387 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3388 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3389 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3390 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3391 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3392 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3393 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3394 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3395 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3396 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3397 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3398 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3399 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3401 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3402 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3403 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3404 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3405 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3406 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3407 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3408 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3409 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3410 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3411 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3412 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3413 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3414 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3415 [D loss: 0.001369, acc.: 100.00%] [G loss: 16.118099]\n",
      "3416 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3417 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3418 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3419 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3420 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3421 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3422 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3423 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3424 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3425 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3426 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3427 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3428 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3429 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3430 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3431 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3432 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3433 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3434 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3435 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3436 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3437 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3438 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3439 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3440 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3441 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3442 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3443 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3444 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3445 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3446 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3447 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3448 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3449 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3450 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118099]\n",
      "3451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3452 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3453 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3454 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3455 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3456 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3457 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3458 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3459 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3461 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3462 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3463 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3464 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3465 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3466 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3467 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3468 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3469 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3470 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3471 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3472 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3473 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3474 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3475 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3476 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3477 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3478 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3479 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3480 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3481 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3482 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3483 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3484 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3485 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3486 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3488 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3489 [D loss: 0.015258, acc.: 99.22%] [G loss: 16.118099]\n",
      "3490 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3491 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3492 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3493 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3494 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3495 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3496 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3497 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3498 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3499 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3500 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3501 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3502 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3503 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3504 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3505 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3506 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3507 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3509 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3510 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3511 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3512 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3513 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3514 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3515 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3516 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3517 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3518 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3519 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3521 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3522 [D loss: 0.014177, acc.: 99.22%] [G loss: 16.118099]\n",
      "3523 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3524 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3525 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3526 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3527 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3528 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3529 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3530 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3531 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3532 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3533 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3534 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3535 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3536 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3537 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3538 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3539 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3540 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3541 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3542 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3543 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3544 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3545 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3546 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3547 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3548 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3549 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3550 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3551 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3552 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3553 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3554 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3555 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118099]\n",
      "3556 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3557 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3558 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3559 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3560 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3561 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3562 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3563 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3564 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3565 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3566 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3567 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3568 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3569 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3570 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3571 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3572 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3573 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3574 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3575 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3576 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3577 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3578 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3579 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3580 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3581 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3582 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3583 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3584 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3585 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3586 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3587 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3588 [D loss: 0.001131, acc.: 100.00%] [G loss: 16.118099]\n",
      "3589 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3590 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3591 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3592 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3593 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3594 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3595 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3596 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3597 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118099]\n",
      "3598 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3601 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3602 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3604 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3605 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3607 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3608 [D loss: 0.013088, acc.: 99.22%] [G loss: 16.118099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3609 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3610 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3611 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3612 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3613 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3615 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3616 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3617 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3618 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3620 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3621 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3623 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3624 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3625 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3626 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3627 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3628 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3629 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3630 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3631 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3632 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3633 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3634 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3635 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3637 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3638 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3639 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3640 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3641 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3643 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3644 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3645 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3646 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3647 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3648 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3649 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3650 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3651 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3653 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118099]\n",
      "3654 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3655 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3656 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3657 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3658 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3659 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3660 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3661 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3662 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3663 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3664 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3665 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3666 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3667 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3668 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3669 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3670 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3671 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3672 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3673 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3674 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3675 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3676 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3677 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3678 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3679 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3680 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3681 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3682 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3683 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3684 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3685 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3686 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3687 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3688 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3689 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3690 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3691 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3692 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3693 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3694 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3695 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3696 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3697 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3698 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3699 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3701 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3702 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3703 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3704 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3705 [D loss: 0.001009, acc.: 100.00%] [G loss: 16.118099]\n",
      "3706 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3707 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3708 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3709 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3710 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3711 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3712 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3713 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3714 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3715 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3716 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3717 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3718 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3719 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3720 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3722 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3723 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3724 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118099]\n",
      "3725 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3726 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3727 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3728 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3729 [D loss: 0.000981, acc.: 100.00%] [G loss: 16.118099]\n",
      "3730 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3731 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3732 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3733 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3734 [D loss: 0.011942, acc.: 99.22%] [G loss: 16.118099]\n",
      "3735 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3736 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3737 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3738 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3739 [D loss: 0.000871, acc.: 100.00%] [G loss: 16.118099]\n",
      "3740 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3741 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3742 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3743 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3744 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3745 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3746 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3747 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3748 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3749 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3750 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3751 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3752 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3753 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3754 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3755 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3756 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3757 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3758 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3759 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3760 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3761 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3762 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3763 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3764 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3765 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3766 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3767 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3768 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3769 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3770 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3771 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3772 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3774 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3775 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3776 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3777 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3778 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3779 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3780 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3781 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3782 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3783 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3784 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3786 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3787 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3789 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3790 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3791 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3792 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3793 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3794 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3795 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3796 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3797 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3798 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3799 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3801 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3802 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3803 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3804 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3805 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3806 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3808 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3809 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3810 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3811 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3812 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3813 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3814 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3815 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118099]\n",
      "3816 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3817 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3818 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3819 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3820 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118099]\n",
      "3821 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3822 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3823 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3824 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3825 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3826 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3827 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3828 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3830 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3831 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3832 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3833 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3834 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3835 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118099]\n",
      "3836 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3837 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3838 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3839 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3840 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3841 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3842 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3843 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3844 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3845 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3846 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3847 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3848 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3850 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3851 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3852 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3853 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3854 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3855 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3856 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3857 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3858 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3859 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3860 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3861 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3862 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3863 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3864 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3865 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3866 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3867 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3868 [D loss: 0.000850, acc.: 100.00%] [G loss: 16.118099]\n",
      "3869 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3870 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3871 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3872 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3873 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3874 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3876 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3877 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3878 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3879 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118099]\n",
      "3880 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3881 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3883 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3884 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3885 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3886 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3887 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3888 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3889 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3891 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3892 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3893 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3894 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3895 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3896 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3897 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3898 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3899 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3901 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3902 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3903 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3904 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3905 [D loss: 0.000827, acc.: 100.00%] [G loss: 16.118099]\n",
      "3906 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3907 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3908 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3909 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3911 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3912 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3913 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3914 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3915 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3916 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3917 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3918 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3919 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3920 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3921 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3922 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3923 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3924 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3925 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3926 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3927 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3928 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118099]\n",
      "3929 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3930 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3931 [D loss: 0.000804, acc.: 100.00%] [G loss: 16.118099]\n",
      "3932 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3933 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3934 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3935 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3936 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3937 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3938 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3939 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3940 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118099]\n",
      "3941 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3942 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3943 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3944 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3945 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3946 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3947 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3948 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3949 [D loss: 0.000782, acc.: 100.00%] [G loss: 16.118099]\n",
      "3950 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3951 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3952 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3953 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3954 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3955 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3956 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3957 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3958 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3959 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3960 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3962 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3963 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3964 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3965 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3966 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3967 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3968 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3969 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3970 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3971 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3972 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3973 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3974 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3975 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3976 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3977 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3978 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3979 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3980 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3981 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3982 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3983 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118099]\n",
      "3984 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3985 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3986 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3987 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3988 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3989 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3990 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3992 [D loss: 0.000761, acc.: 100.00%] [G loss: 16.118099]\n",
      "3993 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3994 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118099]\n",
      "3995 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3996 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3998 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n",
      "3999 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118099]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=4000, data=X_test_seq_trunc) # Generate data similar to the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model trained on the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82815, 5260) (82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_seq_trunc.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (8282, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb2, X_valid_emb2, y_train_emb2, y_valid_emb2 = train_test_split(X_test_seq_trunc, y_test, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb2.shape[0] == y_valid_emb2.shape[0]\n",
    "assert X_train_emb2.shape[0] == y_train_emb2.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model2 = Sequential()\n",
    "glove_model2.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model2.add(Flatten())\n",
    "glove_model2.add(Dense(1, activation='sigmoid'))\n",
    "glove_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model2.layers[0].set_weights([emb_matrix])\n",
    "glove_model2.layers[0].trainable = False\n",
    "\n",
    "glove_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74533 samples, validate on 8282 samples\n",
      "Epoch 1/5\n",
      "74533/74533 [==============================] - 304s 4ms/step - loss: 0.4588 - acc: 0.8406 - val_loss: 0.4640 - val_acc: 0.8582\n",
      "Epoch 2/5\n",
      "74533/74533 [==============================] - 293s 4ms/step - loss: 0.3073 - acc: 0.8952 - val_loss: 0.4720 - val_acc: 0.8654\n",
      "Epoch 3/5\n",
      "74533/74533 [==============================] - 286s 4ms/step - loss: 0.2581 - acc: 0.9113 - val_loss: 0.4751 - val_acc: 0.8666\n",
      "Epoch 4/5\n",
      "74533/74533 [==============================] - 292s 4ms/step - loss: 0.2322 - acc: 0.9201 - val_loss: 0.4734 - val_acc: 0.8723\n",
      "Epoch 5/5\n",
      "74533/74533 [==============================] - 279s 4ms/step - loss: 0.2104 - acc: 0.9277 - val_loss: 0.5164 - val_acc: 0.8708\n"
     ]
    }
   ],
   "source": [
    "history2 = glove_model2.fit(X_train_emb2\n",
    "                       , y_train_emb2\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb2, y_valid_emb2)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_model2.predict(X_train_emb[0:1]))\n",
    "# print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model trained on a generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data and labels\n",
    "gen = 100000\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "gen_samp = np.absolute((generator.predict(noise)))\n",
    "prediction = glove_model.predict((gen_samp))\n",
    "prediction = np.round(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(np.round(prediction[0:100]))\n",
    "# print(np.sum(np.round(glove_model.predict(X_train_emb[0:100]))))\n",
    "# print(np.sum(y_train[0:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (30000, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb3, X_valid_emb3, y_train_emb3, y_valid_emb3 = train_test_split(gen_samp, prediction, test_size=0.3, random_state=37)\n",
    "\n",
    "assert X_valid_emb3.shape[0] == y_valid_emb3.shape[0]\n",
    "assert X_train_emb3.shape[0] == y_train_emb3.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model3 = Sequential()\n",
    "glove_model3.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model3.add(Flatten())\n",
    "glove_model3.add(Dense(1, activation='sigmoid'))\n",
    "glove_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model3.layers[0].set_weights([emb_matrix])\n",
    "glove_model3.layers[0].trainable = False\n",
    "\n",
    "glove_model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      "70000/70000 [==============================] - 348s 5ms/step - loss: 0.2424 - acc: 0.9930 - val_loss: 0.1119 - val_acc: 0.9971\n",
      "Epoch 2/5\n",
      "70000/70000 [==============================] - 315s 5ms/step - loss: 0.0714 - acc: 0.9973 - val_loss: 0.0411 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "70000/70000 [==============================] - 307s 4ms/step - loss: 0.0283 - acc: 0.9980 - val_loss: 0.0193 - val_acc: 0.9983\n",
      "Epoch 4/5\n",
      "70000/70000 [==============================] - 318s 5ms/step - loss: 0.0145 - acc: 0.9984 - val_loss: 0.0103 - val_acc: 0.9986\n",
      "Epoch 5/5\n",
      "70000/70000 [==============================] - 333s 5ms/step - loss: 0.0094 - acc: 0.9985 - val_loss: 0.0072 - val_acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "history3 = glove_model3.fit(X_train_emb3\n",
    "                       , y_train_emb3\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb3, y_valid_emb3)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this area I am going to compare their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb3\n",
    "pred = np.round(glove_model2.predict(X_valid_emb3))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[  337  1727]\n",
      " [ 5420 22516]]\n",
      "Accuracy Score : 0.7617666666666667\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.06      0.16      0.09      2064\n",
      "         1.0       0.93      0.81      0.86     27936\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     30000\n",
      "   macro avg       0.49      0.48      0.47     30000\n",
      "weighted avg       0.87      0.76      0.81     30000\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[ 2042    22]\n",
      " [   17 27919]]\n",
      "Accuracy Score : 0.9987\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      2064\n",
      "         1.0       1.00      1.00      1.00     27936\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     30000\n",
      "   macro avg       1.00      0.99      0.99     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb2\n",
    "pred = np.round(glove_model2.predict(X_valid_emb2))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb2))\n",
    "pred3 = np.round(glove_model.predict(X_valid_emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[2112  596]\n",
      " [ 474 5100]]\n",
      "Accuracy Score : 0.8708041535860903\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80      2708\n",
      "         1.0       0.90      0.91      0.91      5574\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      8282\n",
      "   macro avg       0.86      0.85      0.85      8282\n",
      "weighted avg       0.87      0.87      0.87      8282\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[  13 2695]\n",
      " [  21 5553]]\n",
      "Accuracy Score : 0.6720598889157209\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      2708\n",
      "         1.0       0.67      1.00      0.80      5574\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8282\n",
      "   macro avg       0.53      0.50      0.41      8282\n",
      "weighted avg       0.58      0.67      0.54      8282\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[ 661 2047]\n",
      " [ 183 5391]]\n",
      "Accuracy Score : 0.7307413668196088\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.24      0.37      2708\n",
      "         1.0       0.72      0.97      0.83      5574\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      8282\n",
      "   macro avg       0.75      0.61      0.60      8282\n",
      "weighted avg       0.74      0.73      0.68      8282\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated\n",
    "results(pred3, actual) # original trained on the clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
