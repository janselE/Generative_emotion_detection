{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM, Embedding, SpatialDropout1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')\n",
    "df_dataset = pd.read_json('clothing_dataset/renttherunway_final_data.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "df_dataset['text'] = df_dataset['review_summary'] + \" \" + df_dataset['review_text']\n",
    "df_dataset['text'] = df_dataset['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "sub_dataset = df_dataset[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df_idf['text'].append(sub_dataset['text'])\n",
    "all_rating = df_idf['rating'].append(sub_dataset['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_rating\n",
    "y[:len(df_idf)] = y[:len(df_idf)].apply(lambda x: 1 if x > 3.5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y[len(df_idf):] = y[len(df_idf):].apply(lambda x: 1 if x > 5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to use both datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.1, random_state=37)\n",
    "# X_train = df_dataset['text']\n",
    "# y_train = y[len(df_idf):]\n",
    "\n",
    "# X_test = df_idf['text'].to_numpy()\n",
    "# y_test = y[:len(df_idf)]\n",
    "\n",
    "# print('# Train data samples:', X_train.shape)\n",
    "# print('# Test data samples:', X_test.shape)\n",
    "\n",
    "# print('Sample train', X_train[0])\n",
    "# print('\\nSample test', X_test[0])\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# assert X_train.shape[0] == y_train.shape[0]\n",
    "# assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 5260\n",
    "GLOVE_DIM = 300\n",
    "NB_WORDS = 49781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(all_data)\n",
    "\n",
    "all_data_seq = tk.texts_to_sequences(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = all_data_seq[len(df_idf):]\n",
    "X_test_seq = all_data_seq[:len(df_idf)]\n",
    "\n",
    "y_train = y[len(df_idf):]\n",
    "y_test = y[:len(df_idf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    275359.000000\n",
      "mean         63.517557\n",
      "std          69.966379\n",
      "min           1.000000\n",
      "25%          26.000000\n",
      "50%          50.000000\n",
      "75%          82.000000\n",
      "max        5260.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = all_data.apply(lambda x: len(x.split(' ')))\n",
    "print(seq_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82815, 5260)\n",
      "(1, 5260)\n"
     ]
    }
   ],
   "source": [
    "# This is to split the X_test_seq_trunc in two normalize dataset for GANs\n",
    "print(X_test_seq_trunc.shape)\n",
    "\n",
    "positives = X_test_seq_trunc[0:1]\n",
    "negatives = X_test_seq_trunc[0:1]\n",
    "\n",
    "normalized_data = X_test_seq_trunc[0:1]\n",
    "\n",
    "print(positives.shape)\n",
    "\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    if y_test[i] == 1:\n",
    "        positives = np.append(positives, X_test_seq_trunc[i:(i+1)], axis=0)\n",
    "    else:\n",
    "        negatives = np.append(negatives, X_test_seq_trunc[i:(i+1)], axis=0)\n",
    "        \n",
    "iter_ = min(positives.shape[0], negatives.shape[0])\n",
    "for i in range(0, iter_):\n",
    "    normalized_data = np.append(normalized_data, positives[i:(i+1)], axis=0)\n",
    "    normalized_data = np.append(normalized_data, negatives[i:(i+1)], axis=0)\n",
    "    \n",
    "        \n",
    "print(positives.shape, negatives.shape, normalized_data.shape)\n",
    "\n",
    "# for i in range(0, 5000):\n",
    "#     if(positives[0:1, i] != 0):\n",
    "#         print(positives[0:1, i], X_test_seq_trunc[0:1, i], i)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0 ...  61  14 316]\n",
      "(192544, 5260)\n",
      "(82815, 5260)\n",
      "(192544,)\n",
      "(82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq_trunc[10])  # Example of padded sequence\n",
    "print(X_train_seq_trunc.shape)\n",
    "print(X_test_seq_trunc.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (19255, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "\n",
    "print('Shape of validation set:', X_valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This read the embeddings\n",
    "# glove_file = 'glove.42B.' + str(GLOVE_DIM) + 'd.txt'\n",
    "# emb_dict = {}\n",
    "# glove = open(glove_file)\n",
    "# for line in glove:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     vector = np.asarray(values[1:], dtype='float32')\n",
    "#     emb_dict[word] = vector\n",
    "# glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word car in the dictionary\n",
      "Found the word nice in the dictionary\n",
      "Found the word flight in the dictionary\n",
      "Found the word luggage in the dictionary\n"
     ]
    }
   ],
   "source": [
    "airline_words = ['car', 'nice', 'flight', 'luggage']\n",
    "for w in airline_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary'.format(w))\n",
    "# print(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a matrix that represent words and it corresponding emdg\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model.add(Flatten())\n",
    "glove_model.add(Dense(1, activation='sigmoid'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 173289 samples, validate on 19255 samples\n",
      "Epoch 1/5\n",
      "173289/173289 [==============================] - 570s 3ms/step - loss: 0.1126 - acc: 0.9748 - val_loss: 0.1106 - val_acc: 0.9775\n",
      "Epoch 2/5\n",
      "173289/173289 [==============================] - 2681s 15ms/step - loss: 0.0786 - acc: 0.9793 - val_loss: 0.1193 - val_acc: 0.9787\n",
      "Epoch 3/5\n",
      "173289/173289 [==============================] - 1324s 8ms/step - loss: 0.0669 - acc: 0.9813 - val_loss: 0.1276 - val_acc: 0.9747\n",
      "Epoch 4/5\n",
      "173289/173289 [==============================] - 566s 3ms/step - loss: 0.0582 - acc: 0.9837 - val_loss: 0.1401 - val_acc: 0.9671\n",
      "Epoch 5/5\n",
      "173289/173289 [==============================] - 1143s 7ms/step - loss: 0.0520 - acc: 0.9848 - val_loss: 0.1319 - val_acc: 0.9752\n"
     ]
    }
   ],
   "source": [
    "history = glove_model.fit(X_train_emb\n",
    "                       , y_train_emb\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb, y_valid_emb)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999]]\n",
      "(5260,)\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.predict(X_train_emb[0:1]))\n",
    "print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we start building the GANs, this model takes the word embedding and generate new embeddings that are similar to the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(shape):\n",
    "    img_shape = shape\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def results(pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 512)               2693632   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,825,217\n",
      "Trainable params: 2,825,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5260)              5391500   \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 5260)              0         \n",
      "=================================================================\n",
      "Total params: 6,081,420\n",
      "Trainable params: 6,077,836\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows = 1\n",
    "img_cols = X_train_emb[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = np.round(generator.predict(noise))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 7.046728, acc.: 5.47%] [G loss: 0.871567]\n",
      "1 [D loss: 5.264689, acc.: 31.25%] [G loss: 0.668643]\n",
      "2 [D loss: 2.291618, acc.: 47.66%] [G loss: 0.505634]\n",
      "3 [D loss: 0.591841, acc.: 54.69%] [G loss: 0.438582]\n",
      "4 [D loss: 0.715919, acc.: 51.56%] [G loss: 0.385361]\n",
      "5 [D loss: 0.947541, acc.: 50.78%] [G loss: 0.352522]\n",
      "6 [D loss: 0.846476, acc.: 54.69%] [G loss: 0.350906]\n",
      "7 [D loss: 0.820618, acc.: 50.78%] [G loss: 0.292279]\n",
      "8 [D loss: 0.819013, acc.: 53.12%] [G loss: 0.304908]\n",
      "9 [D loss: 0.831732, acc.: 55.47%] [G loss: 0.298858]\n",
      "10 [D loss: 1.168113, acc.: 51.56%] [G loss: 0.319684]\n",
      "11 [D loss: 1.737201, acc.: 44.53%] [G loss: 0.275976]\n",
      "12 [D loss: 1.398799, acc.: 49.22%] [G loss: 0.281492]\n",
      "13 [D loss: 1.400958, acc.: 48.44%] [G loss: 0.274642]\n",
      "14 [D loss: 1.210433, acc.: 48.44%] [G loss: 0.274221]\n",
      "15 [D loss: 1.914791, acc.: 46.09%] [G loss: 0.254683]\n",
      "16 [D loss: 1.041364, acc.: 53.12%] [G loss: 0.292025]\n",
      "17 [D loss: 1.093817, acc.: 51.56%] [G loss: 0.244153]\n",
      "18 [D loss: 1.440656, acc.: 50.78%] [G loss: 0.263657]\n",
      "19 [D loss: 1.333169, acc.: 48.44%] [G loss: 0.220288]\n",
      "20 [D loss: 1.051496, acc.: 53.12%] [G loss: 0.312048]\n",
      "21 [D loss: 1.071376, acc.: 51.56%] [G loss: 0.268647]\n",
      "22 [D loss: 0.992856, acc.: 53.12%] [G loss: 0.306791]\n",
      "23 [D loss: 0.774026, acc.: 55.47%] [G loss: 0.317434]\n",
      "24 [D loss: 0.932147, acc.: 52.34%] [G loss: 0.333334]\n",
      "25 [D loss: 0.880470, acc.: 57.03%] [G loss: 0.374742]\n",
      "26 [D loss: 0.912121, acc.: 52.34%] [G loss: 0.361672]\n",
      "27 [D loss: 0.875809, acc.: 57.03%] [G loss: 0.382498]\n",
      "28 [D loss: 0.771243, acc.: 57.81%] [G loss: 0.363650]\n",
      "29 [D loss: 1.004045, acc.: 56.25%] [G loss: 0.345458]\n",
      "30 [D loss: 0.768450, acc.: 53.12%] [G loss: 0.333131]\n",
      "31 [D loss: 0.795988, acc.: 53.12%] [G loss: 0.293391]\n",
      "32 [D loss: 0.798428, acc.: 53.91%] [G loss: 0.336412]\n",
      "33 [D loss: 0.772955, acc.: 56.25%] [G loss: 0.344389]\n",
      "34 [D loss: 1.335780, acc.: 52.34%] [G loss: 0.299198]\n",
      "35 [D loss: 0.930574, acc.: 56.25%] [G loss: 0.303620]\n",
      "36 [D loss: 0.897693, acc.: 54.69%] [G loss: 0.335520]\n",
      "37 [D loss: 0.934845, acc.: 55.47%] [G loss: 0.340637]\n",
      "38 [D loss: 0.805320, acc.: 53.91%] [G loss: 0.425568]\n",
      "39 [D loss: 0.944343, acc.: 60.94%] [G loss: 0.423867]\n",
      "40 [D loss: 0.830839, acc.: 57.81%] [G loss: 0.411046]\n",
      "41 [D loss: 0.661603, acc.: 60.94%] [G loss: 0.431669]\n",
      "42 [D loss: 0.761989, acc.: 55.47%] [G loss: 0.426293]\n",
      "43 [D loss: 0.904109, acc.: 57.03%] [G loss: 0.485155]\n",
      "44 [D loss: 0.947650, acc.: 59.38%] [G loss: 0.506273]\n",
      "45 [D loss: 0.735798, acc.: 59.38%] [G loss: 0.483838]\n",
      "46 [D loss: 0.983302, acc.: 63.28%] [G loss: 0.457319]\n",
      "47 [D loss: 1.393226, acc.: 50.00%] [G loss: 0.300612]\n",
      "48 [D loss: 0.891360, acc.: 58.59%] [G loss: 0.322915]\n",
      "49 [D loss: 1.012449, acc.: 52.34%] [G loss: 0.332807]\n",
      "50 [D loss: 1.279752, acc.: 53.12%] [G loss: 0.436851]\n",
      "51 [D loss: 0.848477, acc.: 52.34%] [G loss: 0.446305]\n",
      "52 [D loss: 0.873926, acc.: 59.38%] [G loss: 0.470312]\n",
      "53 [D loss: 1.090103, acc.: 57.03%] [G loss: 0.533844]\n",
      "54 [D loss: 1.146248, acc.: 56.25%] [G loss: 0.436075]\n",
      "55 [D loss: 1.043131, acc.: 50.78%] [G loss: 0.394043]\n",
      "56 [D loss: 0.597092, acc.: 62.50%] [G loss: 0.490458]\n",
      "57 [D loss: 0.989739, acc.: 60.16%] [G loss: 0.452865]\n",
      "58 [D loss: 0.781968, acc.: 55.47%] [G loss: 0.333352]\n",
      "59 [D loss: 0.739223, acc.: 55.47%] [G loss: 0.377816]\n",
      "60 [D loss: 0.834654, acc.: 54.69%] [G loss: 0.437389]\n",
      "61 [D loss: 0.797835, acc.: 60.16%] [G loss: 0.474363]\n",
      "62 [D loss: 0.722142, acc.: 60.16%] [G loss: 0.522912]\n",
      "63 [D loss: 0.842108, acc.: 62.50%] [G loss: 0.503442]\n",
      "64 [D loss: 0.497476, acc.: 70.31%] [G loss: 0.570535]\n",
      "65 [D loss: 0.962445, acc.: 64.06%] [G loss: 0.396986]\n",
      "66 [D loss: 0.764062, acc.: 56.25%] [G loss: 0.410230]\n",
      "67 [D loss: 0.806954, acc.: 61.72%] [G loss: 0.398392]\n",
      "68 [D loss: 0.746879, acc.: 59.38%] [G loss: 0.428646]\n",
      "69 [D loss: 0.874998, acc.: 66.41%] [G loss: 0.527386]\n",
      "70 [D loss: 0.806966, acc.: 60.94%] [G loss: 0.556023]\n",
      "71 [D loss: 0.631961, acc.: 69.53%] [G loss: 0.558431]\n",
      "72 [D loss: 0.688009, acc.: 61.72%] [G loss: 0.562806]\n",
      "73 [D loss: 0.510303, acc.: 67.19%] [G loss: 0.586807]\n",
      "74 [D loss: 0.502835, acc.: 71.09%] [G loss: 0.563710]\n",
      "75 [D loss: 0.758555, acc.: 67.97%] [G loss: 0.578310]\n",
      "76 [D loss: 0.750340, acc.: 64.84%] [G loss: 0.567447]\n",
      "77 [D loss: 0.591747, acc.: 73.44%] [G loss: 0.624593]\n",
      "78 [D loss: 0.666811, acc.: 64.06%] [G loss: 0.609452]\n",
      "79 [D loss: 1.042030, acc.: 63.28%] [G loss: 0.614909]\n",
      "80 [D loss: 0.680725, acc.: 65.62%] [G loss: 0.671819]\n",
      "81 [D loss: 0.596291, acc.: 71.88%] [G loss: 0.644661]\n",
      "82 [D loss: 0.752626, acc.: 76.56%] [G loss: 0.650021]\n",
      "83 [D loss: 0.631652, acc.: 67.19%] [G loss: 0.692864]\n",
      "84 [D loss: 0.595706, acc.: 70.31%] [G loss: 0.640767]\n",
      "85 [D loss: 1.366582, acc.: 59.38%] [G loss: 0.477563]\n",
      "86 [D loss: 0.728561, acc.: 64.06%] [G loss: 0.520193]\n",
      "87 [D loss: 0.877237, acc.: 64.06%] [G loss: 0.544631]\n",
      "88 [D loss: 0.642617, acc.: 71.88%] [G loss: 0.593318]\n",
      "89 [D loss: 0.677257, acc.: 67.19%] [G loss: 0.648461]\n",
      "90 [D loss: 0.487613, acc.: 72.66%] [G loss: 0.658544]\n",
      "91 [D loss: 0.642184, acc.: 71.88%] [G loss: 0.723276]\n",
      "92 [D loss: 0.588369, acc.: 70.31%] [G loss: 0.660479]\n",
      "93 [D loss: 1.001591, acc.: 68.75%] [G loss: 0.753571]\n",
      "94 [D loss: 0.715604, acc.: 72.66%] [G loss: 0.715778]\n",
      "95 [D loss: 0.512718, acc.: 75.78%] [G loss: 0.700903]\n",
      "96 [D loss: 0.821954, acc.: 70.31%] [G loss: 0.709666]\n",
      "97 [D loss: 0.612038, acc.: 71.88%] [G loss: 0.757959]\n",
      "98 [D loss: 0.829749, acc.: 69.53%] [G loss: 0.761356]\n",
      "99 [D loss: 0.782620, acc.: 74.22%] [G loss: 0.809723]\n",
      "100 [D loss: 0.872586, acc.: 71.09%] [G loss: 0.799325]\n",
      "101 [D loss: 1.045983, acc.: 71.88%] [G loss: 0.807249]\n",
      "102 [D loss: 0.903911, acc.: 74.22%] [G loss: 0.830987]\n",
      "103 [D loss: 0.884107, acc.: 75.78%] [G loss: 0.815554]\n",
      "104 [D loss: 0.905506, acc.: 78.91%] [G loss: 0.747741]\n",
      "105 [D loss: 1.316141, acc.: 66.41%] [G loss: 0.531007]\n",
      "106 [D loss: 1.602643, acc.: 59.38%] [G loss: 0.591954]\n",
      "107 [D loss: 1.174697, acc.: 64.84%] [G loss: 0.656585]\n",
      "108 [D loss: 0.890449, acc.: 70.31%] [G loss: 0.773625]\n",
      "109 [D loss: 0.750857, acc.: 77.34%] [G loss: 0.765938]\n",
      "110 [D loss: 1.207766, acc.: 66.41%] [G loss: 0.763659]\n",
      "111 [D loss: 0.727421, acc.: 78.12%] [G loss: 0.771777]\n",
      "112 [D loss: 0.976177, acc.: 74.22%] [G loss: 0.764003]\n",
      "113 [D loss: 1.902966, acc.: 68.75%] [G loss: 0.790160]\n",
      "114 [D loss: 1.263503, acc.: 71.88%] [G loss: 0.776042]\n",
      "115 [D loss: 1.590354, acc.: 67.19%] [G loss: 0.824288]\n",
      "116 [D loss: 1.196275, acc.: 79.69%] [G loss: 0.808438]\n",
      "117 [D loss: 1.613386, acc.: 72.66%] [G loss: 0.870938]\n",
      "118 [D loss: 1.613009, acc.: 65.62%] [G loss: 0.907271]\n",
      "119 [D loss: 1.474586, acc.: 67.97%] [G loss: 0.812843]\n",
      "120 [D loss: 1.944924, acc.: 67.97%] [G loss: 0.917257]\n",
      "121 [D loss: 0.933607, acc.: 74.22%] [G loss: 0.914420]\n",
      "122 [D loss: 1.275658, acc.: 73.44%] [G loss: 0.897774]\n",
      "123 [D loss: 1.591999, acc.: 75.78%] [G loss: 0.884572]\n",
      "124 [D loss: 1.640920, acc.: 64.06%] [G loss: 0.925269]\n",
      "125 [D loss: 1.973880, acc.: 67.97%] [G loss: 1.009406]\n",
      "126 [D loss: 1.021050, acc.: 72.66%] [G loss: 1.090719]\n",
      "127 [D loss: 1.917505, acc.: 67.19%] [G loss: 1.119433]\n",
      "128 [D loss: 1.317816, acc.: 76.56%] [G loss: 1.023544]\n",
      "129 [D loss: 2.499448, acc.: 64.84%] [G loss: 1.035959]\n",
      "130 [D loss: 1.991402, acc.: 62.50%] [G loss: 0.675655]\n",
      "131 [D loss: 1.248608, acc.: 63.28%] [G loss: 0.746168]\n",
      "132 [D loss: 0.888585, acc.: 71.88%] [G loss: 0.961202]\n",
      "133 [D loss: 1.219240, acc.: 75.78%] [G loss: 0.963288]\n",
      "134 [D loss: 1.356840, acc.: 75.00%] [G loss: 0.918502]\n",
      "135 [D loss: 1.020706, acc.: 77.34%] [G loss: 0.834783]\n",
      "136 [D loss: 1.232946, acc.: 67.97%] [G loss: 0.884833]\n",
      "137 [D loss: 0.712304, acc.: 73.44%] [G loss: 0.917561]\n",
      "138 [D loss: 1.031475, acc.: 75.00%] [G loss: 1.043360]\n",
      "139 [D loss: 1.138510, acc.: 67.97%] [G loss: 0.692146]\n",
      "140 [D loss: 0.718932, acc.: 69.53%] [G loss: 0.731479]\n",
      "141 [D loss: 0.687652, acc.: 69.53%] [G loss: 0.830120]\n",
      "142 [D loss: 0.708305, acc.: 71.88%] [G loss: 0.941626]\n",
      "143 [D loss: 0.490192, acc.: 79.69%] [G loss: 0.926537]\n",
      "144 [D loss: 0.594936, acc.: 69.53%] [G loss: 0.937681]\n",
      "145 [D loss: 0.601361, acc.: 71.09%] [G loss: 0.968620]\n",
      "146 [D loss: 0.563810, acc.: 74.22%] [G loss: 1.084902]\n",
      "147 [D loss: 0.556177, acc.: 73.44%] [G loss: 1.008301]\n",
      "148 [D loss: 0.525552, acc.: 76.56%] [G loss: 0.930284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 1.037021, acc.: 67.97%] [G loss: 0.851063]\n",
      "150 [D loss: 0.813021, acc.: 68.75%] [G loss: 0.912513]\n",
      "151 [D loss: 0.639808, acc.: 69.53%] [G loss: 0.933183]\n",
      "152 [D loss: 0.500529, acc.: 81.25%] [G loss: 0.946414]\n",
      "153 [D loss: 0.904889, acc.: 67.97%] [G loss: 1.000593]\n",
      "154 [D loss: 0.691594, acc.: 72.66%] [G loss: 0.864655]\n",
      "155 [D loss: 0.760005, acc.: 67.19%] [G loss: 0.747582]\n",
      "156 [D loss: 0.750624, acc.: 71.88%] [G loss: 0.925822]\n",
      "157 [D loss: 0.573179, acc.: 75.00%] [G loss: 0.972421]\n",
      "158 [D loss: 0.938702, acc.: 63.28%] [G loss: 0.911464]\n",
      "159 [D loss: 0.570715, acc.: 72.66%] [G loss: 0.983700]\n",
      "160 [D loss: 0.755350, acc.: 72.66%] [G loss: 0.913544]\n",
      "161 [D loss: 0.944763, acc.: 67.19%] [G loss: 0.947576]\n",
      "162 [D loss: 1.207706, acc.: 61.72%] [G loss: 0.720886]\n",
      "163 [D loss: 0.915006, acc.: 64.06%] [G loss: 0.790592]\n",
      "164 [D loss: 0.795564, acc.: 67.19%] [G loss: 0.850739]\n",
      "165 [D loss: 0.916639, acc.: 65.62%] [G loss: 1.089210]\n",
      "166 [D loss: 1.029608, acc.: 67.19%] [G loss: 1.043500]\n",
      "167 [D loss: 0.839751, acc.: 66.41%] [G loss: 1.094370]\n",
      "168 [D loss: 0.963094, acc.: 71.09%] [G loss: 1.139277]\n",
      "169 [D loss: 1.994743, acc.: 56.25%] [G loss: 0.444076]\n",
      "170 [D loss: 1.158596, acc.: 58.59%] [G loss: 0.558704]\n",
      "171 [D loss: 0.980534, acc.: 64.06%] [G loss: 0.798089]\n",
      "172 [D loss: 0.864214, acc.: 65.62%] [G loss: 1.058817]\n",
      "173 [D loss: 0.871742, acc.: 66.41%] [G loss: 1.213801]\n",
      "174 [D loss: 0.686381, acc.: 74.22%] [G loss: 1.230905]\n",
      "175 [D loss: 0.770488, acc.: 66.41%] [G loss: 1.012603]\n",
      "176 [D loss: 1.209756, acc.: 64.84%] [G loss: 1.051580]\n",
      "177 [D loss: 0.835816, acc.: 67.97%] [G loss: 1.038421]\n",
      "178 [D loss: 0.956521, acc.: 64.06%] [G loss: 0.952459]\n",
      "179 [D loss: 1.162552, acc.: 58.59%] [G loss: 1.034846]\n",
      "180 [D loss: 0.909498, acc.: 67.97%] [G loss: 1.039100]\n",
      "181 [D loss: 0.994716, acc.: 66.41%] [G loss: 1.033925]\n",
      "182 [D loss: 1.292023, acc.: 67.97%] [G loss: 1.112997]\n",
      "183 [D loss: 1.074812, acc.: 69.53%] [G loss: 1.116166]\n",
      "184 [D loss: 1.134853, acc.: 64.06%] [G loss: 1.057517]\n",
      "185 [D loss: 0.853226, acc.: 70.31%] [G loss: 1.079646]\n",
      "186 [D loss: 1.896511, acc.: 59.38%] [G loss: 1.141757]\n",
      "187 [D loss: 1.557604, acc.: 60.94%] [G loss: 0.797474]\n",
      "188 [D loss: 1.312663, acc.: 58.59%] [G loss: 1.117720]\n",
      "189 [D loss: 1.334662, acc.: 68.75%] [G loss: 1.289119]\n",
      "190 [D loss: 1.565654, acc.: 64.84%] [G loss: 1.396419]\n",
      "191 [D loss: 1.905217, acc.: 56.25%] [G loss: 0.625295]\n",
      "192 [D loss: 1.218550, acc.: 58.59%] [G loss: 0.757486]\n",
      "193 [D loss: 0.997943, acc.: 58.59%] [G loss: 1.131953]\n",
      "194 [D loss: 0.840046, acc.: 66.41%] [G loss: 1.458012]\n",
      "195 [D loss: 0.853088, acc.: 69.53%] [G loss: 1.296080]\n",
      "196 [D loss: 1.029519, acc.: 64.84%] [G loss: 1.235939]\n",
      "197 [D loss: 1.257604, acc.: 64.84%] [G loss: 1.297151]\n",
      "198 [D loss: 0.817692, acc.: 69.53%] [G loss: 1.272369]\n",
      "199 [D loss: 1.212372, acc.: 64.84%] [G loss: 1.230482]\n",
      "200 [D loss: 1.147466, acc.: 65.62%] [G loss: 1.232123]\n",
      "201 [D loss: 1.027734, acc.: 63.28%] [G loss: 1.335637]\n",
      "202 [D loss: 1.350682, acc.: 62.50%] [G loss: 1.315953]\n",
      "203 [D loss: 0.924131, acc.: 65.62%] [G loss: 1.351067]\n",
      "204 [D loss: 1.327630, acc.: 68.75%] [G loss: 1.359038]\n",
      "205 [D loss: 1.892313, acc.: 56.25%] [G loss: 1.461980]\n",
      "206 [D loss: 1.598719, acc.: 65.62%] [G loss: 1.626388]\n",
      "207 [D loss: 2.320402, acc.: 57.03%] [G loss: 1.674261]\n",
      "208 [D loss: 2.008932, acc.: 48.44%] [G loss: 0.853805]\n",
      "209 [D loss: 1.426872, acc.: 56.25%] [G loss: 1.179219]\n",
      "210 [D loss: 1.056858, acc.: 65.62%] [G loss: 1.601881]\n",
      "211 [D loss: 0.829415, acc.: 72.66%] [G loss: 1.575208]\n",
      "212 [D loss: 1.247818, acc.: 60.16%] [G loss: 1.642949]\n",
      "213 [D loss: 0.764247, acc.: 73.44%] [G loss: 1.438508]\n",
      "214 [D loss: 1.188618, acc.: 64.84%] [G loss: 1.278597]\n",
      "215 [D loss: 1.042923, acc.: 71.09%] [G loss: 1.237862]\n",
      "216 [D loss: 1.503062, acc.: 59.38%] [G loss: 1.278261]\n",
      "217 [D loss: 1.355281, acc.: 58.59%] [G loss: 1.466734]\n",
      "218 [D loss: 1.057161, acc.: 68.75%] [G loss: 1.622496]\n",
      "219 [D loss: 1.082710, acc.: 67.97%] [G loss: 1.565728]\n",
      "220 [D loss: 1.531578, acc.: 63.28%] [G loss: 1.494109]\n",
      "221 [D loss: 0.604679, acc.: 67.97%] [G loss: 1.387792]\n",
      "222 [D loss: 1.426248, acc.: 68.75%] [G loss: 1.431486]\n",
      "223 [D loss: 1.255341, acc.: 70.31%] [G loss: 1.477498]\n",
      "224 [D loss: 1.397801, acc.: 66.41%] [G loss: 1.493660]\n",
      "225 [D loss: 1.334830, acc.: 70.31%] [G loss: 1.464223]\n",
      "226 [D loss: 1.262151, acc.: 67.19%] [G loss: 1.443464]\n",
      "227 [D loss: 1.306376, acc.: 70.31%] [G loss: 1.539460]\n",
      "228 [D loss: 1.534654, acc.: 67.19%] [G loss: 1.547812]\n",
      "229 [D loss: 1.192511, acc.: 69.53%] [G loss: 1.773152]\n",
      "230 [D loss: 1.984891, acc.: 68.75%] [G loss: 1.813063]\n",
      "231 [D loss: 2.938512, acc.: 44.53%] [G loss: 0.419782]\n",
      "232 [D loss: 1.345634, acc.: 52.34%] [G loss: 0.668851]\n",
      "233 [D loss: 0.625031, acc.: 67.97%] [G loss: 1.460034]\n",
      "234 [D loss: 0.458186, acc.: 75.78%] [G loss: 1.748506]\n",
      "235 [D loss: 0.399215, acc.: 79.69%] [G loss: 1.642404]\n",
      "236 [D loss: 0.678889, acc.: 71.88%] [G loss: 1.357837]\n",
      "237 [D loss: 0.532976, acc.: 77.34%] [G loss: 1.273685]\n",
      "238 [D loss: 0.666428, acc.: 69.53%] [G loss: 1.272471]\n",
      "239 [D loss: 0.975525, acc.: 68.75%] [G loss: 1.216967]\n",
      "240 [D loss: 0.705915, acc.: 68.75%] [G loss: 1.203485]\n",
      "241 [D loss: 0.600021, acc.: 67.19%] [G loss: 1.103379]\n",
      "242 [D loss: 0.695116, acc.: 67.97%] [G loss: 1.314629]\n",
      "243 [D loss: 0.690992, acc.: 75.00%] [G loss: 1.338188]\n",
      "244 [D loss: 0.789598, acc.: 70.31%] [G loss: 1.120410]\n",
      "245 [D loss: 0.558036, acc.: 67.19%] [G loss: 1.181042]\n",
      "246 [D loss: 0.843180, acc.: 70.31%] [G loss: 1.130626]\n",
      "247 [D loss: 1.033115, acc.: 59.38%] [G loss: 1.088185]\n",
      "248 [D loss: 0.571490, acc.: 71.09%] [G loss: 1.096892]\n",
      "249 [D loss: 0.762292, acc.: 69.53%] [G loss: 1.109239]\n",
      "250 [D loss: 1.088613, acc.: 66.41%] [G loss: 1.139612]\n",
      "251 [D loss: 0.862554, acc.: 66.41%] [G loss: 1.311709]\n",
      "252 [D loss: 0.928312, acc.: 71.09%] [G loss: 1.280295]\n",
      "253 [D loss: 0.846246, acc.: 70.31%] [G loss: 1.267690]\n",
      "254 [D loss: 1.378155, acc.: 66.41%] [G loss: 1.405918]\n",
      "255 [D loss: 0.820932, acc.: 67.97%] [G loss: 1.316190]\n",
      "256 [D loss: 1.331462, acc.: 74.22%] [G loss: 1.297977]\n",
      "257 [D loss: 1.774644, acc.: 53.12%] [G loss: 0.813382]\n",
      "258 [D loss: 1.624472, acc.: 56.25%] [G loss: 0.957410]\n",
      "259 [D loss: 0.916517, acc.: 70.31%] [G loss: 1.304019]\n",
      "260 [D loss: 0.551179, acc.: 76.56%] [G loss: 1.424806]\n",
      "261 [D loss: 0.714596, acc.: 75.00%] [G loss: 1.414053]\n",
      "262 [D loss: 0.833741, acc.: 74.22%] [G loss: 1.356566]\n",
      "263 [D loss: 1.529947, acc.: 63.28%] [G loss: 1.314621]\n",
      "264 [D loss: 0.724082, acc.: 73.44%] [G loss: 1.536620]\n",
      "265 [D loss: 1.341073, acc.: 63.28%] [G loss: 1.583431]\n",
      "266 [D loss: 1.196739, acc.: 75.00%] [G loss: 1.428668]\n",
      "267 [D loss: 1.423839, acc.: 66.41%] [G loss: 1.353269]\n",
      "268 [D loss: 1.673274, acc.: 68.75%] [G loss: 1.367918]\n",
      "269 [D loss: 2.189538, acc.: 58.59%] [G loss: 1.390933]\n",
      "270 [D loss: 1.777015, acc.: 59.38%] [G loss: 0.936619]\n",
      "271 [D loss: 1.028703, acc.: 60.94%] [G loss: 1.140358]\n",
      "272 [D loss: 1.073073, acc.: 72.66%] [G loss: 1.495666]\n",
      "273 [D loss: 0.835236, acc.: 78.91%] [G loss: 1.586859]\n",
      "274 [D loss: 0.903469, acc.: 73.44%] [G loss: 1.624905]\n",
      "275 [D loss: 1.543802, acc.: 71.09%] [G loss: 1.656639]\n",
      "276 [D loss: 0.631471, acc.: 78.91%] [G loss: 1.604041]\n",
      "277 [D loss: 1.738787, acc.: 73.44%] [G loss: 1.597071]\n",
      "278 [D loss: 1.200485, acc.: 77.34%] [G loss: 1.428508]\n",
      "279 [D loss: 1.566549, acc.: 67.97%] [G loss: 1.532741]\n",
      "280 [D loss: 1.738869, acc.: 73.44%] [G loss: 1.644085]\n",
      "281 [D loss: 0.946207, acc.: 78.12%] [G loss: 1.584345]\n",
      "282 [D loss: 1.611216, acc.: 70.31%] [G loss: 1.566047]\n",
      "283 [D loss: 1.072159, acc.: 76.56%] [G loss: 1.607153]\n",
      "284 [D loss: 1.367463, acc.: 73.44%] [G loss: 1.592308]\n",
      "285 [D loss: 2.172715, acc.: 62.50%] [G loss: 1.867262]\n",
      "286 [D loss: 1.597756, acc.: 67.97%] [G loss: 1.158056]\n",
      "287 [D loss: 1.575307, acc.: 63.28%] [G loss: 1.336377]\n",
      "288 [D loss: 0.990275, acc.: 79.69%] [G loss: 1.473977]\n",
      "289 [D loss: 0.777392, acc.: 75.78%] [G loss: 1.827749]\n",
      "290 [D loss: 0.641856, acc.: 85.16%] [G loss: 1.803872]\n",
      "291 [D loss: 0.866959, acc.: 74.22%] [G loss: 1.620617]\n",
      "292 [D loss: 1.041962, acc.: 77.34%] [G loss: 1.533284]\n",
      "293 [D loss: 1.106053, acc.: 74.22%] [G loss: 1.511301]\n",
      "294 [D loss: 0.826836, acc.: 82.03%] [G loss: 1.426960]\n",
      "295 [D loss: 1.355127, acc.: 72.66%] [G loss: 1.513773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.994799, acc.: 77.34%] [G loss: 1.535928]\n",
      "297 [D loss: 1.159605, acc.: 78.12%] [G loss: 1.478069]\n",
      "298 [D loss: 1.188054, acc.: 76.56%] [G loss: 1.467888]\n",
      "299 [D loss: 0.993398, acc.: 84.38%] [G loss: 1.488040]\n",
      "300 [D loss: 0.526023, acc.: 85.16%] [G loss: 1.472012]\n",
      "301 [D loss: 1.054389, acc.: 82.03%] [G loss: 1.457672]\n",
      "302 [D loss: 1.621373, acc.: 68.75%] [G loss: 1.394174]\n",
      "303 [D loss: 1.425780, acc.: 77.34%] [G loss: 1.482499]\n",
      "304 [D loss: 1.288463, acc.: 78.91%] [G loss: 1.545128]\n",
      "305 [D loss: 1.023916, acc.: 82.81%] [G loss: 1.476797]\n",
      "306 [D loss: 1.106613, acc.: 82.81%] [G loss: 1.497591]\n",
      "307 [D loss: 0.888516, acc.: 82.03%] [G loss: 1.440814]\n",
      "308 [D loss: 0.766526, acc.: 82.81%] [G loss: 1.427128]\n",
      "309 [D loss: 1.150405, acc.: 82.81%] [G loss: 1.473580]\n",
      "310 [D loss: 1.647521, acc.: 79.69%] [G loss: 1.530922]\n",
      "311 [D loss: 1.114856, acc.: 83.59%] [G loss: 1.465862]\n",
      "312 [D loss: 1.523160, acc.: 76.56%] [G loss: 1.515203]\n",
      "313 [D loss: 1.369241, acc.: 82.81%] [G loss: 1.488616]\n",
      "314 [D loss: 1.109183, acc.: 83.59%] [G loss: 1.430636]\n",
      "315 [D loss: 1.722038, acc.: 82.03%] [G loss: 1.383911]\n",
      "316 [D loss: 1.148202, acc.: 80.47%] [G loss: 1.326226]\n",
      "317 [D loss: 1.289085, acc.: 79.69%] [G loss: 1.357949]\n",
      "318 [D loss: 1.605994, acc.: 81.25%] [G loss: 1.470737]\n",
      "319 [D loss: 2.362513, acc.: 79.69%] [G loss: 1.578925]\n",
      "320 [D loss: 2.150698, acc.: 60.94%] [G loss: 1.042430]\n",
      "321 [D loss: 1.346283, acc.: 75.00%] [G loss: 1.138113]\n",
      "322 [D loss: 0.812788, acc.: 76.56%] [G loss: 1.460121]\n",
      "323 [D loss: 0.610155, acc.: 87.50%] [G loss: 1.603862]\n",
      "324 [D loss: 0.993619, acc.: 79.69%] [G loss: 1.401605]\n",
      "325 [D loss: 1.186047, acc.: 80.47%] [G loss: 1.452048]\n",
      "326 [D loss: 1.053281, acc.: 81.25%] [G loss: 1.471115]\n",
      "327 [D loss: 1.747276, acc.: 76.56%] [G loss: 1.363814]\n",
      "328 [D loss: 0.901863, acc.: 82.03%] [G loss: 1.445030]\n",
      "329 [D loss: 1.180089, acc.: 81.25%] [G loss: 1.512790]\n",
      "330 [D loss: 0.735480, acc.: 88.28%] [G loss: 1.511967]\n",
      "331 [D loss: 1.327268, acc.: 85.16%] [G loss: 1.465088]\n",
      "332 [D loss: 1.160419, acc.: 82.03%] [G loss: 1.340094]\n",
      "333 [D loss: 1.474787, acc.: 78.91%] [G loss: 1.377331]\n",
      "334 [D loss: 0.531508, acc.: 83.59%] [G loss: 1.311087]\n",
      "335 [D loss: 1.373944, acc.: 80.47%] [G loss: 1.369013]\n",
      "336 [D loss: 1.498572, acc.: 83.59%] [G loss: 1.318655]\n",
      "337 [D loss: 0.874807, acc.: 82.81%] [G loss: 1.352753]\n",
      "338 [D loss: 1.169676, acc.: 75.78%] [G loss: 1.384234]\n",
      "339 [D loss: 1.394813, acc.: 77.34%] [G loss: 1.275305]\n",
      "340 [D loss: 1.716638, acc.: 64.84%] [G loss: 1.047055]\n",
      "341 [D loss: 0.696078, acc.: 82.81%] [G loss: 1.260473]\n",
      "342 [D loss: 1.029936, acc.: 81.25%] [G loss: 1.401488]\n",
      "343 [D loss: 1.348243, acc.: 88.28%] [G loss: 1.457091]\n",
      "344 [D loss: 1.370483, acc.: 79.69%] [G loss: 1.380450]\n",
      "345 [D loss: 1.370270, acc.: 76.56%] [G loss: 1.396809]\n",
      "346 [D loss: 1.381924, acc.: 81.25%] [G loss: 1.378648]\n",
      "347 [D loss: 1.134522, acc.: 82.81%] [G loss: 1.241512]\n",
      "348 [D loss: 0.996270, acc.: 85.94%] [G loss: 1.262357]\n",
      "349 [D loss: 1.951776, acc.: 72.66%] [G loss: 1.314485]\n",
      "350 [D loss: 1.247434, acc.: 68.75%] [G loss: 0.962226]\n",
      "351 [D loss: 1.578995, acc.: 64.84%] [G loss: 1.143119]\n",
      "352 [D loss: 1.238942, acc.: 71.88%] [G loss: 1.379812]\n",
      "353 [D loss: 0.663404, acc.: 82.03%] [G loss: 1.490650]\n",
      "354 [D loss: 0.799351, acc.: 82.03%] [G loss: 1.552408]\n",
      "355 [D loss: 0.644754, acc.: 85.16%] [G loss: 1.486031]\n",
      "356 [D loss: 1.028493, acc.: 79.69%] [G loss: 1.423275]\n",
      "357 [D loss: 1.059208, acc.: 78.12%] [G loss: 1.392277]\n",
      "358 [D loss: 1.525135, acc.: 78.12%] [G loss: 1.410838]\n",
      "359 [D loss: 0.998617, acc.: 83.59%] [G loss: 1.355060]\n",
      "360 [D loss: 1.764941, acc.: 67.97%] [G loss: 0.949611]\n",
      "361 [D loss: 0.787620, acc.: 73.44%] [G loss: 1.053234]\n",
      "362 [D loss: 0.835371, acc.: 78.12%] [G loss: 1.125938]\n",
      "363 [D loss: 0.665654, acc.: 83.59%] [G loss: 1.247537]\n",
      "364 [D loss: 0.940696, acc.: 79.69%] [G loss: 1.238634]\n",
      "365 [D loss: 0.713525, acc.: 78.91%] [G loss: 1.424876]\n",
      "366 [D loss: 0.762866, acc.: 85.16%] [G loss: 1.394744]\n",
      "367 [D loss: 1.082861, acc.: 76.56%] [G loss: 1.231102]\n",
      "368 [D loss: 0.884351, acc.: 82.81%] [G loss: 1.344455]\n",
      "369 [D loss: 0.555321, acc.: 82.03%] [G loss: 1.197252]\n",
      "370 [D loss: 1.057093, acc.: 78.12%] [G loss: 1.260124]\n",
      "371 [D loss: 1.227232, acc.: 75.00%] [G loss: 1.281169]\n",
      "372 [D loss: 1.236768, acc.: 74.22%] [G loss: 1.319801]\n",
      "373 [D loss: 0.967753, acc.: 78.12%] [G loss: 1.509930]\n",
      "374 [D loss: 0.811314, acc.: 77.34%] [G loss: 1.450490]\n",
      "375 [D loss: 0.800614, acc.: 81.25%] [G loss: 1.485779]\n",
      "376 [D loss: 0.776015, acc.: 82.81%] [G loss: 1.393647]\n",
      "377 [D loss: 0.450889, acc.: 82.03%] [G loss: 1.396042]\n",
      "378 [D loss: 1.431061, acc.: 75.00%] [G loss: 1.386294]\n",
      "379 [D loss: 1.313175, acc.: 81.25%] [G loss: 1.376994]\n",
      "380 [D loss: 1.072413, acc.: 78.91%] [G loss: 1.360817]\n",
      "381 [D loss: 1.442986, acc.: 77.34%] [G loss: 1.327573]\n",
      "382 [D loss: 1.457334, acc.: 77.34%] [G loss: 1.370490]\n",
      "383 [D loss: 0.819271, acc.: 80.47%] [G loss: 1.369880]\n",
      "384 [D loss: 1.287338, acc.: 78.91%] [G loss: 1.417412]\n",
      "385 [D loss: 1.127446, acc.: 84.38%] [G loss: 1.448390]\n",
      "386 [D loss: 0.933105, acc.: 82.03%] [G loss: 1.425855]\n",
      "387 [D loss: 1.139922, acc.: 81.25%] [G loss: 1.506141]\n",
      "388 [D loss: 2.244677, acc.: 54.69%] [G loss: 0.759887]\n",
      "389 [D loss: 0.559858, acc.: 72.66%] [G loss: 0.966055]\n",
      "390 [D loss: 0.662944, acc.: 79.69%] [G loss: 1.189656]\n",
      "391 [D loss: 0.562303, acc.: 80.47%] [G loss: 1.347704]\n",
      "392 [D loss: 0.286035, acc.: 85.94%] [G loss: 1.470787]\n",
      "393 [D loss: 0.391133, acc.: 82.81%] [G loss: 1.404902]\n",
      "394 [D loss: 0.643950, acc.: 76.56%] [G loss: 1.482018]\n",
      "395 [D loss: 0.721587, acc.: 67.97%] [G loss: 0.963021]\n",
      "396 [D loss: 0.495367, acc.: 69.53%] [G loss: 1.024239]\n",
      "397 [D loss: 0.376888, acc.: 82.81%] [G loss: 1.299144]\n",
      "398 [D loss: 0.485719, acc.: 75.78%] [G loss: 1.320872]\n",
      "399 [D loss: 0.756101, acc.: 78.12%] [G loss: 1.412675]\n",
      "400 [D loss: 0.355203, acc.: 83.59%] [G loss: 1.365330]\n",
      "401 [D loss: 0.615745, acc.: 77.34%] [G loss: 1.397210]\n",
      "402 [D loss: 0.406539, acc.: 79.69%] [G loss: 1.363057]\n",
      "403 [D loss: 0.436808, acc.: 80.47%] [G loss: 1.286428]\n",
      "404 [D loss: 0.540545, acc.: 74.22%] [G loss: 1.267480]\n",
      "405 [D loss: 0.542715, acc.: 77.34%] [G loss: 1.236829]\n",
      "406 [D loss: 0.433295, acc.: 84.38%] [G loss: 1.227375]\n",
      "407 [D loss: 0.691454, acc.: 80.47%] [G loss: 1.174547]\n",
      "408 [D loss: 0.648699, acc.: 81.25%] [G loss: 1.154045]\n",
      "409 [D loss: 0.616938, acc.: 71.88%] [G loss: 1.221869]\n",
      "410 [D loss: 0.448063, acc.: 76.56%] [G loss: 1.303567]\n",
      "411 [D loss: 0.602059, acc.: 77.34%] [G loss: 1.527949]\n",
      "412 [D loss: 0.822366, acc.: 79.69%] [G loss: 1.452206]\n",
      "413 [D loss: 0.437627, acc.: 80.47%] [G loss: 1.338194]\n",
      "414 [D loss: 0.614664, acc.: 81.25%] [G loss: 1.328547]\n",
      "415 [D loss: 1.113819, acc.: 73.44%] [G loss: 1.354641]\n",
      "416 [D loss: 0.686388, acc.: 80.47%] [G loss: 1.434464]\n",
      "417 [D loss: 0.852306, acc.: 79.69%] [G loss: 1.344270]\n",
      "418 [D loss: 0.955592, acc.: 81.25%] [G loss: 1.375245]\n",
      "419 [D loss: 0.594894, acc.: 80.47%] [G loss: 1.438393]\n",
      "420 [D loss: 0.885212, acc.: 82.03%] [G loss: 1.412105]\n",
      "421 [D loss: 0.988037, acc.: 75.00%] [G loss: 1.486251]\n",
      "422 [D loss: 0.519495, acc.: 84.38%] [G loss: 1.473925]\n",
      "423 [D loss: 0.770251, acc.: 78.12%] [G loss: 1.552382]\n",
      "424 [D loss: 0.386467, acc.: 88.28%] [G loss: 1.564233]\n",
      "425 [D loss: 0.797793, acc.: 81.25%] [G loss: 1.619579]\n",
      "426 [D loss: 1.351061, acc.: 82.03%] [G loss: 1.539873]\n",
      "427 [D loss: 0.895067, acc.: 84.38%] [G loss: 1.456424]\n",
      "428 [D loss: 1.141976, acc.: 80.47%] [G loss: 1.556453]\n",
      "429 [D loss: 0.752861, acc.: 85.94%] [G loss: 1.551638]\n",
      "430 [D loss: 1.024294, acc.: 82.03%] [G loss: 1.638306]\n",
      "431 [D loss: 0.838464, acc.: 86.72%] [G loss: 1.650071]\n",
      "432 [D loss: 0.524636, acc.: 83.59%] [G loss: 1.627234]\n",
      "433 [D loss: 0.580406, acc.: 88.28%] [G loss: 1.552227]\n",
      "434 [D loss: 1.461337, acc.: 82.81%] [G loss: 1.601750]\n",
      "435 [D loss: 1.097890, acc.: 83.59%] [G loss: 1.462712]\n",
      "436 [D loss: 1.111064, acc.: 85.94%] [G loss: 1.532247]\n",
      "437 [D loss: 0.759228, acc.: 83.59%] [G loss: 1.658993]\n",
      "438 [D loss: 2.265035, acc.: 73.44%] [G loss: 1.878311]\n",
      "439 [D loss: 1.627951, acc.: 80.47%] [G loss: 1.966400]\n",
      "440 [D loss: 1.161308, acc.: 90.62%] [G loss: 1.876378]\n",
      "441 [D loss: 1.823628, acc.: 82.03%] [G loss: 1.757930]\n",
      "442 [D loss: 2.189391, acc.: 71.88%] [G loss: 1.819531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 1.121637, acc.: 83.59%] [G loss: 1.953333]\n",
      "444 [D loss: 1.555476, acc.: 85.16%] [G loss: 1.649715]\n",
      "445 [D loss: 1.502000, acc.: 79.69%] [G loss: 1.744407]\n",
      "446 [D loss: 1.439832, acc.: 85.16%] [G loss: 1.913359]\n",
      "447 [D loss: 2.327365, acc.: 77.34%] [G loss: 1.733206]\n",
      "448 [D loss: 1.179938, acc.: 89.06%] [G loss: 1.686034]\n",
      "449 [D loss: 1.600764, acc.: 80.47%] [G loss: 1.757928]\n",
      "450 [D loss: 1.885378, acc.: 79.69%] [G loss: 1.873641]\n",
      "451 [D loss: 1.053177, acc.: 85.16%] [G loss: 1.852402]\n",
      "452 [D loss: 1.027224, acc.: 89.84%] [G loss: 1.715515]\n",
      "453 [D loss: 1.831561, acc.: 81.25%] [G loss: 1.687727]\n",
      "454 [D loss: 1.070896, acc.: 84.38%] [G loss: 1.576992]\n",
      "455 [D loss: 1.841679, acc.: 81.25%] [G loss: 1.647445]\n",
      "456 [D loss: 1.544781, acc.: 82.81%] [G loss: 1.773588]\n",
      "457 [D loss: 1.415264, acc.: 85.16%] [G loss: 1.717303]\n",
      "458 [D loss: 1.062562, acc.: 85.94%] [G loss: 1.824416]\n",
      "459 [D loss: 1.684075, acc.: 86.72%] [G loss: 1.761310]\n",
      "460 [D loss: 1.780554, acc.: 84.38%] [G loss: 1.838181]\n",
      "461 [D loss: 1.324321, acc.: 83.59%] [G loss: 1.762144]\n",
      "462 [D loss: 1.749529, acc.: 80.47%] [G loss: 1.299023]\n",
      "463 [D loss: 1.150274, acc.: 82.03%] [G loss: 1.752449]\n",
      "464 [D loss: 1.028732, acc.: 78.91%] [G loss: 1.820194]\n",
      "465 [D loss: 0.663001, acc.: 90.62%] [G loss: 1.984967]\n",
      "466 [D loss: 1.218004, acc.: 83.59%] [G loss: 1.670263]\n",
      "467 [D loss: 0.976894, acc.: 82.81%] [G loss: 1.792733]\n",
      "468 [D loss: 0.765689, acc.: 82.03%] [G loss: 2.123553]\n",
      "469 [D loss: 0.939036, acc.: 89.84%] [G loss: 2.096748]\n",
      "470 [D loss: 0.736842, acc.: 92.19%] [G loss: 1.892058]\n",
      "471 [D loss: 0.793133, acc.: 87.50%] [G loss: 1.667205]\n",
      "472 [D loss: 0.371523, acc.: 89.06%] [G loss: 1.662036]\n",
      "473 [D loss: 1.158918, acc.: 82.03%] [G loss: 1.708981]\n",
      "474 [D loss: 0.326155, acc.: 89.84%] [G loss: 1.847520]\n",
      "475 [D loss: 0.920430, acc.: 87.50%] [G loss: 1.847599]\n",
      "476 [D loss: 0.512979, acc.: 94.53%] [G loss: 1.565014]\n",
      "477 [D loss: 0.701634, acc.: 77.34%] [G loss: 1.888551]\n",
      "478 [D loss: 0.512882, acc.: 82.81%] [G loss: 1.758519]\n",
      "479 [D loss: 1.561862, acc.: 82.81%] [G loss: 1.675404]\n",
      "480 [D loss: 1.283193, acc.: 88.28%] [G loss: 1.538085]\n",
      "481 [D loss: 0.713693, acc.: 86.72%] [G loss: 1.515608]\n",
      "482 [D loss: 0.927530, acc.: 89.06%] [G loss: 1.472421]\n",
      "483 [D loss: 0.527066, acc.: 82.81%] [G loss: 1.628162]\n",
      "484 [D loss: 1.510044, acc.: 79.69%] [G loss: 1.697350]\n",
      "485 [D loss: 0.568146, acc.: 89.06%] [G loss: 1.680902]\n",
      "486 [D loss: 0.833049, acc.: 85.94%] [G loss: 1.612611]\n",
      "487 [D loss: 0.983411, acc.: 85.16%] [G loss: 1.608719]\n",
      "488 [D loss: 0.965824, acc.: 85.16%] [G loss: 1.555643]\n",
      "489 [D loss: 0.681697, acc.: 89.06%] [G loss: 1.581506]\n",
      "490 [D loss: 0.927750, acc.: 89.06%] [G loss: 1.645214]\n",
      "491 [D loss: 1.061648, acc.: 86.72%] [G loss: 1.517750]\n",
      "492 [D loss: 0.939438, acc.: 88.28%] [G loss: 1.544225]\n",
      "493 [D loss: 1.099790, acc.: 85.16%] [G loss: 1.664239]\n",
      "494 [D loss: 0.772014, acc.: 91.41%] [G loss: 1.628092]\n",
      "495 [D loss: 0.916650, acc.: 92.19%] [G loss: 1.576183]\n",
      "496 [D loss: 1.493027, acc.: 81.25%] [G loss: 1.457122]\n",
      "497 [D loss: 1.028155, acc.: 80.47%] [G loss: 1.875616]\n",
      "498 [D loss: 1.518524, acc.: 85.16%] [G loss: 1.739693]\n",
      "499 [D loss: 0.884743, acc.: 92.19%] [G loss: 1.613541]\n",
      "500 [D loss: 0.946463, acc.: 87.50%] [G loss: 1.557703]\n",
      "501 [D loss: 1.579611, acc.: 82.81%] [G loss: 1.658695]\n",
      "502 [D loss: 0.811008, acc.: 89.84%] [G loss: 1.753637]\n",
      "503 [D loss: 1.177996, acc.: 88.28%] [G loss: 1.733401]\n",
      "504 [D loss: 0.822778, acc.: 87.50%] [G loss: 1.637088]\n",
      "505 [D loss: 1.622954, acc.: 79.69%] [G loss: 1.719741]\n",
      "506 [D loss: 0.907289, acc.: 90.62%] [G loss: 1.628343]\n",
      "507 [D loss: 0.899874, acc.: 91.41%] [G loss: 1.656710]\n",
      "508 [D loss: 1.199704, acc.: 85.94%] [G loss: 1.606001]\n",
      "509 [D loss: 1.347260, acc.: 82.81%] [G loss: 1.797276]\n",
      "510 [D loss: 0.932167, acc.: 89.06%] [G loss: 1.688764]\n",
      "511 [D loss: 1.037826, acc.: 89.06%] [G loss: 1.754764]\n",
      "512 [D loss: 1.340415, acc.: 80.47%] [G loss: 1.791272]\n",
      "513 [D loss: 1.047433, acc.: 85.94%] [G loss: 1.741173]\n",
      "514 [D loss: 1.509385, acc.: 87.50%] [G loss: 1.765433]\n",
      "515 [D loss: 2.310690, acc.: 79.69%] [G loss: 1.638236]\n",
      "516 [D loss: 2.299271, acc.: 61.72%] [G loss: 0.888454]\n",
      "517 [D loss: 0.941531, acc.: 75.00%] [G loss: 1.520487]\n",
      "518 [D loss: 0.595041, acc.: 87.50%] [G loss: 1.676394]\n",
      "519 [D loss: 0.950552, acc.: 87.50%] [G loss: 1.708165]\n",
      "520 [D loss: 0.699076, acc.: 85.94%] [G loss: 1.893609]\n",
      "521 [D loss: 0.569318, acc.: 89.06%] [G loss: 1.864228]\n",
      "522 [D loss: 0.547752, acc.: 85.16%] [G loss: 1.912981]\n",
      "523 [D loss: 0.661110, acc.: 91.41%] [G loss: 1.946386]\n",
      "524 [D loss: 1.413693, acc.: 85.94%] [G loss: 1.749522]\n",
      "525 [D loss: 0.609399, acc.: 85.94%] [G loss: 1.629755]\n",
      "526 [D loss: 0.821580, acc.: 85.16%] [G loss: 1.728525]\n",
      "527 [D loss: 0.761453, acc.: 83.59%] [G loss: 2.003721]\n",
      "528 [D loss: 0.661463, acc.: 92.97%] [G loss: 1.744941]\n",
      "529 [D loss: 0.641970, acc.: 84.38%] [G loss: 1.759207]\n",
      "530 [D loss: 0.511397, acc.: 92.19%] [G loss: 1.710614]\n",
      "531 [D loss: 0.934896, acc.: 88.28%] [G loss: 1.752730]\n",
      "532 [D loss: 0.863635, acc.: 84.38%] [G loss: 1.785025]\n",
      "533 [D loss: 0.760625, acc.: 81.25%] [G loss: 1.791730]\n",
      "534 [D loss: 0.883240, acc.: 78.12%] [G loss: 1.988829]\n",
      "535 [D loss: 0.676649, acc.: 90.62%] [G loss: 2.002496]\n",
      "536 [D loss: 1.129726, acc.: 73.44%] [G loss: 0.883597]\n",
      "537 [D loss: 0.490602, acc.: 71.09%] [G loss: 1.183793]\n",
      "538 [D loss: 0.421253, acc.: 80.47%] [G loss: 1.735037]\n",
      "539 [D loss: 0.282622, acc.: 83.59%] [G loss: 1.948529]\n",
      "540 [D loss: 0.513451, acc.: 81.25%] [G loss: 2.001276]\n",
      "541 [D loss: 0.278983, acc.: 85.94%] [G loss: 1.973942]\n",
      "542 [D loss: 0.338216, acc.: 90.62%] [G loss: 1.782989]\n",
      "543 [D loss: 0.410911, acc.: 81.25%] [G loss: 1.803856]\n",
      "544 [D loss: 0.516070, acc.: 81.25%] [G loss: 1.794418]\n",
      "545 [D loss: 0.395889, acc.: 78.91%] [G loss: 1.856668]\n",
      "546 [D loss: 0.606086, acc.: 80.47%] [G loss: 1.835601]\n",
      "547 [D loss: 0.370700, acc.: 89.06%] [G loss: 1.680224]\n",
      "548 [D loss: 0.293485, acc.: 84.38%] [G loss: 1.562185]\n",
      "549 [D loss: 0.442974, acc.: 82.81%] [G loss: 1.397376]\n",
      "550 [D loss: 0.321051, acc.: 85.16%] [G loss: 1.412430]\n",
      "551 [D loss: 0.514457, acc.: 83.59%] [G loss: 1.398860]\n",
      "552 [D loss: 0.373708, acc.: 87.50%] [G loss: 1.476036]\n",
      "553 [D loss: 0.269833, acc.: 85.94%] [G loss: 1.528331]\n",
      "554 [D loss: 0.394598, acc.: 81.25%] [G loss: 1.628353]\n",
      "555 [D loss: 0.506537, acc.: 83.59%] [G loss: 1.674690]\n",
      "556 [D loss: 0.314276, acc.: 82.81%] [G loss: 1.505550]\n",
      "557 [D loss: 0.543164, acc.: 82.81%] [G loss: 1.151110]\n",
      "558 [D loss: 0.590689, acc.: 76.56%] [G loss: 1.234329]\n",
      "559 [D loss: 0.327736, acc.: 82.03%] [G loss: 1.381853]\n",
      "560 [D loss: 0.366187, acc.: 89.06%] [G loss: 1.341874]\n",
      "561 [D loss: 0.358871, acc.: 79.69%] [G loss: 1.395681]\n",
      "562 [D loss: 0.535610, acc.: 82.03%] [G loss: 1.411242]\n",
      "563 [D loss: 0.390950, acc.: 78.91%] [G loss: 1.450900]\n",
      "564 [D loss: 0.375550, acc.: 78.12%] [G loss: 1.642970]\n",
      "565 [D loss: 0.497078, acc.: 79.69%] [G loss: 1.726472]\n",
      "566 [D loss: 0.380376, acc.: 86.72%] [G loss: 1.704110]\n",
      "567 [D loss: 0.695203, acc.: 82.81%] [G loss: 1.489231]\n",
      "568 [D loss: 0.377646, acc.: 82.81%] [G loss: 1.497954]\n",
      "569 [D loss: 0.587020, acc.: 82.03%] [G loss: 1.474155]\n",
      "570 [D loss: 0.556973, acc.: 79.69%] [G loss: 1.470571]\n",
      "571 [D loss: 0.619882, acc.: 78.12%] [G loss: 1.579497]\n",
      "572 [D loss: 0.529177, acc.: 85.16%] [G loss: 1.524963]\n",
      "573 [D loss: 0.569435, acc.: 80.47%] [G loss: 1.706372]\n",
      "574 [D loss: 0.546200, acc.: 81.25%] [G loss: 1.696323]\n",
      "575 [D loss: 0.516860, acc.: 84.38%] [G loss: 1.633296]\n",
      "576 [D loss: 0.904447, acc.: 81.25%] [G loss: 1.495721]\n",
      "577 [D loss: 0.521430, acc.: 85.16%] [G loss: 1.498545]\n",
      "578 [D loss: 1.311180, acc.: 60.94%] [G loss: 0.565814]\n",
      "579 [D loss: 1.026886, acc.: 62.50%] [G loss: 0.927268]\n",
      "580 [D loss: 0.483138, acc.: 76.56%] [G loss: 1.657827]\n",
      "581 [D loss: 0.222146, acc.: 89.06%] [G loss: 1.831611]\n",
      "582 [D loss: 0.354525, acc.: 80.47%] [G loss: 1.713033]\n",
      "583 [D loss: 0.266275, acc.: 85.16%] [G loss: 1.564836]\n",
      "584 [D loss: 0.323900, acc.: 83.59%] [G loss: 1.518265]\n",
      "585 [D loss: 0.467336, acc.: 82.03%] [G loss: 1.442136]\n",
      "586 [D loss: 0.538426, acc.: 78.91%] [G loss: 1.519929]\n",
      "587 [D loss: 0.473126, acc.: 78.12%] [G loss: 1.465219]\n",
      "588 [D loss: 0.328740, acc.: 85.16%] [G loss: 1.505802]\n",
      "589 [D loss: 0.660344, acc.: 75.78%] [G loss: 1.737287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.646942, acc.: 79.69%] [G loss: 1.590503]\n",
      "591 [D loss: 0.611926, acc.: 76.56%] [G loss: 1.456531]\n",
      "592 [D loss: 0.574012, acc.: 78.91%] [G loss: 1.604154]\n",
      "593 [D loss: 0.567382, acc.: 73.44%] [G loss: 1.688564]\n",
      "594 [D loss: 0.921788, acc.: 75.00%] [G loss: 2.038002]\n",
      "595 [D loss: 0.319896, acc.: 84.38%] [G loss: 2.129745]\n",
      "596 [D loss: 0.332532, acc.: 89.06%] [G loss: 1.718575]\n",
      "597 [D loss: 0.346247, acc.: 82.03%] [G loss: 1.488289]\n",
      "598 [D loss: 0.673642, acc.: 78.12%] [G loss: 1.476811]\n",
      "599 [D loss: 0.402523, acc.: 81.25%] [G loss: 1.546272]\n",
      "600 [D loss: 0.441125, acc.: 78.12%] [G loss: 1.493518]\n",
      "601 [D loss: 0.311801, acc.: 85.16%] [G loss: 1.583756]\n",
      "602 [D loss: 0.668707, acc.: 78.12%] [G loss: 1.630574]\n",
      "603 [D loss: 0.603634, acc.: 78.91%] [G loss: 1.770874]\n",
      "604 [D loss: 0.552058, acc.: 82.81%] [G loss: 1.678870]\n",
      "605 [D loss: 0.921376, acc.: 75.00%] [G loss: 2.003044]\n",
      "606 [D loss: 0.442968, acc.: 83.59%] [G loss: 1.985069]\n",
      "607 [D loss: 0.790567, acc.: 81.25%] [G loss: 1.888539]\n",
      "608 [D loss: 0.517297, acc.: 85.94%] [G loss: 1.649391]\n",
      "609 [D loss: 0.632316, acc.: 78.91%] [G loss: 1.789696]\n",
      "610 [D loss: 0.734605, acc.: 76.56%] [G loss: 1.270977]\n",
      "611 [D loss: 1.127321, acc.: 71.09%] [G loss: 1.764477]\n",
      "612 [D loss: 0.647870, acc.: 77.34%] [G loss: 2.139523]\n",
      "613 [D loss: 0.219259, acc.: 90.62%] [G loss: 2.370142]\n",
      "614 [D loss: 1.104880, acc.: 80.47%] [G loss: 2.152388]\n",
      "615 [D loss: 0.412086, acc.: 85.94%] [G loss: 2.060600]\n",
      "616 [D loss: 0.838988, acc.: 79.69%] [G loss: 2.043384]\n",
      "617 [D loss: 1.079742, acc.: 73.44%] [G loss: 2.311851]\n",
      "618 [D loss: 1.189200, acc.: 76.56%] [G loss: 2.481696]\n",
      "619 [D loss: 0.904602, acc.: 82.03%] [G loss: 2.220008]\n",
      "620 [D loss: 1.594311, acc.: 79.69%] [G loss: 2.099763]\n",
      "621 [D loss: 0.814326, acc.: 79.69%] [G loss: 1.905099]\n",
      "622 [D loss: 0.921273, acc.: 77.34%] [G loss: 2.222016]\n",
      "623 [D loss: 1.397255, acc.: 81.25%] [G loss: 2.234089]\n",
      "624 [D loss: 0.658411, acc.: 82.81%] [G loss: 2.257678]\n",
      "625 [D loss: 1.224858, acc.: 71.88%] [G loss: 1.681283]\n",
      "626 [D loss: 0.612012, acc.: 75.78%] [G loss: 2.031839]\n",
      "627 [D loss: 1.196298, acc.: 78.12%] [G loss: 2.579230]\n",
      "628 [D loss: 0.890818, acc.: 84.38%] [G loss: 2.367001]\n",
      "629 [D loss: 1.008339, acc.: 82.81%] [G loss: 2.554582]\n",
      "630 [D loss: 1.646072, acc.: 75.78%] [G loss: 2.929923]\n",
      "631 [D loss: 1.069723, acc.: 85.16%] [G loss: 2.597344]\n",
      "632 [D loss: 0.642664, acc.: 85.16%] [G loss: 2.258263]\n",
      "633 [D loss: 1.011374, acc.: 80.47%] [G loss: 2.364598]\n",
      "634 [D loss: 1.745481, acc.: 80.47%] [G loss: 2.211474]\n",
      "635 [D loss: 0.654728, acc.: 82.03%] [G loss: 2.222292]\n",
      "636 [D loss: 1.036402, acc.: 86.72%] [G loss: 2.287078]\n",
      "637 [D loss: 0.679049, acc.: 88.28%] [G loss: 2.175430]\n",
      "638 [D loss: 1.182598, acc.: 87.50%] [G loss: 1.977136]\n",
      "639 [D loss: 1.245753, acc.: 78.12%] [G loss: 2.843225]\n",
      "640 [D loss: 0.795733, acc.: 89.06%] [G loss: 3.072456]\n",
      "641 [D loss: 0.796337, acc.: 89.84%] [G loss: 2.516565]\n",
      "642 [D loss: 1.477520, acc.: 79.69%] [G loss: 2.978832]\n",
      "643 [D loss: 0.775538, acc.: 91.41%] [G loss: 2.317311]\n",
      "644 [D loss: 0.970379, acc.: 70.31%] [G loss: 2.744082]\n",
      "645 [D loss: 0.506213, acc.: 92.19%] [G loss: 2.786229]\n",
      "646 [D loss: 0.887169, acc.: 90.62%] [G loss: 2.436063]\n",
      "647 [D loss: 0.680158, acc.: 89.06%] [G loss: 2.283153]\n",
      "648 [D loss: 0.846165, acc.: 85.94%] [G loss: 2.345895]\n",
      "649 [D loss: 1.012596, acc.: 84.38%] [G loss: 2.329984]\n",
      "650 [D loss: 0.800708, acc.: 90.62%] [G loss: 2.383687]\n",
      "651 [D loss: 1.165853, acc.: 88.28%] [G loss: 2.373510]\n",
      "652 [D loss: 0.287249, acc.: 94.53%] [G loss: 2.372417]\n",
      "653 [D loss: 0.837030, acc.: 85.16%] [G loss: 2.384865]\n",
      "654 [D loss: 0.955115, acc.: 85.94%] [G loss: 2.619948]\n",
      "655 [D loss: 0.750121, acc.: 91.41%] [G loss: 2.491123]\n",
      "656 [D loss: 1.216644, acc.: 85.94%] [G loss: 2.401529]\n",
      "657 [D loss: 1.273073, acc.: 86.72%] [G loss: 2.236738]\n",
      "658 [D loss: 0.862877, acc.: 84.38%] [G loss: 2.366254]\n",
      "659 [D loss: 1.015713, acc.: 89.06%] [G loss: 2.613073]\n",
      "660 [D loss: 1.707540, acc.: 81.25%] [G loss: 2.453575]\n",
      "661 [D loss: 0.888182, acc.: 88.28%] [G loss: 2.453038]\n",
      "662 [D loss: 1.367170, acc.: 89.84%] [G loss: 2.543496]\n",
      "663 [D loss: 1.821756, acc.: 60.94%] [G loss: 1.249210]\n",
      "664 [D loss: 0.733085, acc.: 72.66%] [G loss: 3.869389]\n",
      "665 [D loss: 0.457800, acc.: 82.03%] [G loss: 4.600492]\n",
      "666 [D loss: 0.399040, acc.: 77.34%] [G loss: 5.228621]\n",
      "667 [D loss: 0.586330, acc.: 92.97%] [G loss: 4.427636]\n",
      "668 [D loss: 0.417028, acc.: 89.84%] [G loss: 3.356934]\n",
      "669 [D loss: 0.772180, acc.: 89.06%] [G loss: 2.954429]\n",
      "670 [D loss: 0.344847, acc.: 90.62%] [G loss: 2.511804]\n",
      "671 [D loss: 0.313621, acc.: 88.28%] [G loss: 2.166100]\n",
      "672 [D loss: 0.598714, acc.: 88.28%] [G loss: 2.235473]\n",
      "673 [D loss: 0.425564, acc.: 82.03%] [G loss: 2.623432]\n",
      "674 [D loss: 0.453481, acc.: 86.72%] [G loss: 2.206893]\n",
      "675 [D loss: 0.857845, acc.: 81.25%] [G loss: 2.341144]\n",
      "676 [D loss: 0.195804, acc.: 89.06%] [G loss: 2.355358]\n",
      "677 [D loss: 0.592940, acc.: 87.50%] [G loss: 2.333534]\n",
      "678 [D loss: 0.644291, acc.: 90.62%] [G loss: 2.625912]\n",
      "679 [D loss: 0.462490, acc.: 83.59%] [G loss: 2.728727]\n",
      "680 [D loss: 0.454398, acc.: 89.84%] [G loss: 2.570875]\n",
      "681 [D loss: 0.787511, acc.: 85.16%] [G loss: 2.900271]\n",
      "682 [D loss: 0.368475, acc.: 86.72%] [G loss: 2.652920]\n",
      "683 [D loss: 0.699268, acc.: 78.91%] [G loss: 3.044801]\n",
      "684 [D loss: 0.571710, acc.: 90.62%] [G loss: 2.686036]\n",
      "685 [D loss: 0.442671, acc.: 89.84%] [G loss: 2.467870]\n",
      "686 [D loss: 0.602434, acc.: 84.38%] [G loss: 2.384438]\n",
      "687 [D loss: 0.303145, acc.: 90.62%] [G loss: 2.306061]\n",
      "688 [D loss: 0.571330, acc.: 85.16%] [G loss: 2.496802]\n",
      "689 [D loss: 0.535147, acc.: 85.94%] [G loss: 2.795092]\n",
      "690 [D loss: 0.367779, acc.: 85.16%] [G loss: 2.614202]\n",
      "691 [D loss: 0.874191, acc.: 82.81%] [G loss: 2.792565]\n",
      "692 [D loss: 0.720748, acc.: 84.38%] [G loss: 2.747102]\n",
      "693 [D loss: 0.369787, acc.: 94.53%] [G loss: 2.349779]\n",
      "694 [D loss: 0.787064, acc.: 89.84%] [G loss: 2.214786]\n",
      "695 [D loss: 0.468970, acc.: 85.94%] [G loss: 2.191221]\n",
      "696 [D loss: 0.372112, acc.: 93.75%] [G loss: 2.097230]\n",
      "697 [D loss: 0.681434, acc.: 85.94%] [G loss: 2.090306]\n",
      "698 [D loss: 0.674730, acc.: 89.84%] [G loss: 2.279861]\n",
      "699 [D loss: 0.736634, acc.: 85.94%] [G loss: 2.489562]\n",
      "700 [D loss: 0.632565, acc.: 93.75%] [G loss: 2.174400]\n",
      "701 [D loss: 0.811314, acc.: 85.16%] [G loss: 2.090131]\n",
      "702 [D loss: 1.030096, acc.: 88.28%] [G loss: 2.052270]\n",
      "703 [D loss: 0.803953, acc.: 90.62%] [G loss: 2.208393]\n",
      "704 [D loss: 1.117671, acc.: 89.84%] [G loss: 2.216810]\n",
      "705 [D loss: 0.597199, acc.: 94.53%] [G loss: 2.503865]\n",
      "706 [D loss: 0.343026, acc.: 95.31%] [G loss: 2.282688]\n",
      "707 [D loss: 0.472643, acc.: 89.84%] [G loss: 2.343985]\n",
      "708 [D loss: 1.070730, acc.: 66.41%] [G loss: 1.418377]\n",
      "709 [D loss: 0.490919, acc.: 76.56%] [G loss: 2.111187]\n",
      "710 [D loss: 0.415193, acc.: 93.75%] [G loss: 2.765840]\n",
      "711 [D loss: 0.133844, acc.: 95.31%] [G loss: 2.081639]\n",
      "712 [D loss: 0.253712, acc.: 88.28%] [G loss: 2.194470]\n",
      "713 [D loss: 0.405973, acc.: 93.75%] [G loss: 2.432565]\n",
      "714 [D loss: 0.131415, acc.: 95.31%] [G loss: 2.070067]\n",
      "715 [D loss: 0.265527, acc.: 89.84%] [G loss: 2.713048]\n",
      "716 [D loss: 0.341319, acc.: 86.72%] [G loss: 2.466132]\n",
      "717 [D loss: 0.283624, acc.: 92.19%] [G loss: 2.403601]\n",
      "718 [D loss: 0.426636, acc.: 90.62%] [G loss: 2.213846]\n",
      "719 [D loss: 0.219981, acc.: 90.62%] [G loss: 2.173765]\n",
      "720 [D loss: 0.303693, acc.: 89.06%] [G loss: 2.047837]\n",
      "721 [D loss: 0.531130, acc.: 92.19%] [G loss: 2.037680]\n",
      "722 [D loss: 0.445348, acc.: 89.84%] [G loss: 1.998880]\n",
      "723 [D loss: 0.447160, acc.: 89.06%] [G loss: 1.982142]\n",
      "724 [D loss: 0.184107, acc.: 92.19%] [G loss: 1.984839]\n",
      "725 [D loss: 0.231597, acc.: 89.84%] [G loss: 2.227143]\n",
      "726 [D loss: 0.334858, acc.: 91.41%] [G loss: 2.277379]\n",
      "727 [D loss: 0.193397, acc.: 89.84%] [G loss: 2.210698]\n",
      "728 [D loss: 0.350184, acc.: 88.28%] [G loss: 2.235404]\n",
      "729 [D loss: 0.125213, acc.: 95.31%] [G loss: 2.198466]\n",
      "730 [D loss: 0.518785, acc.: 91.41%] [G loss: 2.000741]\n",
      "731 [D loss: 0.207204, acc.: 92.19%] [G loss: 2.139960]\n",
      "732 [D loss: 0.374224, acc.: 94.53%] [G loss: 2.101962]\n",
      "733 [D loss: 0.569897, acc.: 89.06%] [G loss: 2.077064]\n",
      "734 [D loss: 0.636440, acc.: 83.59%] [G loss: 2.156764]\n",
      "735 [D loss: 0.619270, acc.: 86.72%] [G loss: 2.054613]\n",
      "736 [D loss: 0.279550, acc.: 90.62%] [G loss: 2.231403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 [D loss: 0.679441, acc.: 89.06%] [G loss: 2.131097]\n",
      "738 [D loss: 0.313754, acc.: 91.41%] [G loss: 2.298464]\n",
      "739 [D loss: 0.450011, acc.: 90.62%] [G loss: 2.189749]\n",
      "740 [D loss: 0.436732, acc.: 90.62%] [G loss: 2.278893]\n",
      "741 [D loss: 0.768967, acc.: 92.19%] [G loss: 2.238693]\n",
      "742 [D loss: 0.473536, acc.: 96.88%] [G loss: 2.088122]\n",
      "743 [D loss: 0.351644, acc.: 85.94%] [G loss: 2.416426]\n",
      "744 [D loss: 0.800055, acc.: 88.28%] [G loss: 2.071672]\n",
      "745 [D loss: 0.925847, acc.: 81.25%] [G loss: 2.657916]\n",
      "746 [D loss: 0.573418, acc.: 96.88%] [G loss: 2.456502]\n",
      "747 [D loss: 1.398777, acc.: 66.41%] [G loss: 0.812974]\n",
      "748 [D loss: 0.690413, acc.: 66.41%] [G loss: 1.781232]\n",
      "749 [D loss: 0.239110, acc.: 87.50%] [G loss: 2.684623]\n",
      "750 [D loss: 0.551250, acc.: 78.12%] [G loss: 4.301718]\n",
      "751 [D loss: 0.433363, acc.: 82.03%] [G loss: 3.691965]\n",
      "752 [D loss: 0.668663, acc.: 80.47%] [G loss: 2.762340]\n",
      "753 [D loss: 0.517317, acc.: 75.78%] [G loss: 2.260278]\n",
      "754 [D loss: 0.526655, acc.: 75.78%] [G loss: 1.950852]\n",
      "755 [D loss: 0.640124, acc.: 77.34%] [G loss: 1.720951]\n",
      "756 [D loss: 0.580777, acc.: 74.22%] [G loss: 1.874108]\n",
      "757 [D loss: 0.783922, acc.: 67.97%] [G loss: 2.430472]\n",
      "758 [D loss: 0.709475, acc.: 77.34%] [G loss: 2.668296]\n",
      "759 [D loss: 0.884670, acc.: 74.22%] [G loss: 2.635653]\n",
      "760 [D loss: 0.808423, acc.: 71.88%] [G loss: 2.673493]\n",
      "761 [D loss: 0.806827, acc.: 77.34%] [G loss: 2.519773]\n",
      "762 [D loss: 0.393037, acc.: 82.03%] [G loss: 1.899423]\n",
      "763 [D loss: 0.561031, acc.: 78.91%] [G loss: 1.439923]\n",
      "764 [D loss: 0.808740, acc.: 71.88%] [G loss: 1.633783]\n",
      "765 [D loss: 1.392856, acc.: 70.31%] [G loss: 1.740939]\n",
      "766 [D loss: 0.903798, acc.: 76.56%] [G loss: 1.975399]\n",
      "767 [D loss: 1.233614, acc.: 65.62%] [G loss: 2.383177]\n",
      "768 [D loss: 0.724700, acc.: 73.44%] [G loss: 2.645284]\n",
      "769 [D loss: 0.637262, acc.: 79.69%] [G loss: 2.261978]\n",
      "770 [D loss: 1.161538, acc.: 73.44%] [G loss: 1.976418]\n",
      "771 [D loss: 1.380746, acc.: 64.84%] [G loss: 0.723448]\n",
      "772 [D loss: 1.958042, acc.: 60.94%] [G loss: 1.092385]\n",
      "773 [D loss: 1.592742, acc.: 78.91%] [G loss: 2.229090]\n",
      "774 [D loss: 1.838864, acc.: 71.88%] [G loss: 1.297668]\n",
      "775 [D loss: 1.475387, acc.: 68.75%] [G loss: 1.941193]\n",
      "776 [D loss: 0.874178, acc.: 71.88%] [G loss: 2.570470]\n",
      "777 [D loss: 0.813731, acc.: 76.56%] [G loss: 2.621521]\n",
      "778 [D loss: 0.771773, acc.: 77.34%] [G loss: 2.458597]\n",
      "779 [D loss: 1.386566, acc.: 70.31%] [G loss: 2.873090]\n",
      "780 [D loss: 0.821060, acc.: 76.56%] [G loss: 2.889207]\n",
      "781 [D loss: 0.936872, acc.: 75.00%] [G loss: 2.341951]\n",
      "782 [D loss: 1.442830, acc.: 65.62%] [G loss: 2.058977]\n",
      "783 [D loss: 1.871211, acc.: 60.16%] [G loss: 2.021717]\n",
      "784 [D loss: 1.698766, acc.: 63.28%] [G loss: 2.323580]\n",
      "785 [D loss: 1.641999, acc.: 68.75%] [G loss: 2.503758]\n",
      "786 [D loss: 1.151468, acc.: 71.09%] [G loss: 3.022172]\n",
      "787 [D loss: 1.670489, acc.: 65.62%] [G loss: 3.800617]\n",
      "788 [D loss: 1.699150, acc.: 70.31%] [G loss: 3.863790]\n",
      "789 [D loss: 2.261771, acc.: 66.41%] [G loss: 4.094078]\n",
      "790 [D loss: 1.375541, acc.: 77.34%] [G loss: 3.689722]\n",
      "791 [D loss: 1.903942, acc.: 69.53%] [G loss: 3.554203]\n",
      "792 [D loss: 1.857066, acc.: 74.22%] [G loss: 3.375888]\n",
      "793 [D loss: 1.730288, acc.: 68.75%] [G loss: 3.286807]\n",
      "794 [D loss: 2.036223, acc.: 67.97%] [G loss: 3.727904]\n",
      "795 [D loss: 2.712104, acc.: 64.84%] [G loss: 4.005549]\n",
      "796 [D loss: 2.515694, acc.: 70.31%] [G loss: 3.622490]\n",
      "797 [D loss: 1.618629, acc.: 72.66%] [G loss: 4.171493]\n",
      "798 [D loss: 1.922108, acc.: 67.19%] [G loss: 6.785107]\n",
      "799 [D loss: 2.313331, acc.: 71.88%] [G loss: 6.590406]\n",
      "800 [D loss: 1.419903, acc.: 81.25%] [G loss: 5.876462]\n",
      "801 [D loss: 2.133857, acc.: 72.66%] [G loss: 5.278462]\n",
      "802 [D loss: 1.847713, acc.: 75.00%] [G loss: 4.756578]\n",
      "803 [D loss: 2.041114, acc.: 69.53%] [G loss: 3.693611]\n",
      "804 [D loss: 1.687244, acc.: 78.12%] [G loss: 3.755348]\n",
      "805 [D loss: 1.470625, acc.: 75.78%] [G loss: 4.600572]\n",
      "806 [D loss: 0.882779, acc.: 85.16%] [G loss: 4.820872]\n",
      "807 [D loss: 1.620689, acc.: 81.25%] [G loss: 4.758143]\n",
      "808 [D loss: 1.680214, acc.: 82.03%] [G loss: 4.287413]\n",
      "809 [D loss: 1.285312, acc.: 78.12%] [G loss: 4.173857]\n",
      "810 [D loss: 1.453771, acc.: 82.81%] [G loss: 4.292497]\n",
      "811 [D loss: 1.421329, acc.: 85.16%] [G loss: 4.170424]\n",
      "812 [D loss: 1.574777, acc.: 75.00%] [G loss: 5.648532]\n",
      "813 [D loss: 2.712972, acc.: 58.59%] [G loss: 7.567717]\n",
      "814 [D loss: 0.986937, acc.: 85.16%] [G loss: 8.598194]\n",
      "815 [D loss: 0.861151, acc.: 91.41%] [G loss: 8.570126]\n",
      "816 [D loss: 0.684306, acc.: 88.28%] [G loss: 7.730055]\n",
      "817 [D loss: 0.198572, acc.: 96.09%] [G loss: 6.048771]\n",
      "818 [D loss: 0.799782, acc.: 93.75%] [G loss: 4.106801]\n",
      "819 [D loss: 1.331683, acc.: 82.81%] [G loss: 4.015738]\n",
      "820 [D loss: 1.495663, acc.: 85.16%] [G loss: 3.376380]\n",
      "821 [D loss: 0.679493, acc.: 88.28%] [G loss: 3.081262]\n",
      "822 [D loss: 1.402211, acc.: 87.50%] [G loss: 3.597976]\n",
      "823 [D loss: 0.594669, acc.: 94.53%] [G loss: 2.978127]\n",
      "824 [D loss: 1.251045, acc.: 88.28%] [G loss: 3.033603]\n",
      "825 [D loss: 1.135506, acc.: 80.47%] [G loss: 3.885144]\n",
      "826 [D loss: 1.517493, acc.: 71.88%] [G loss: 5.505222]\n",
      "827 [D loss: 1.740656, acc.: 71.09%] [G loss: 9.734176]\n",
      "828 [D loss: 1.302275, acc.: 86.72%] [G loss: 8.707943]\n",
      "829 [D loss: 0.971699, acc.: 85.94%] [G loss: 8.758677]\n",
      "830 [D loss: 0.560556, acc.: 93.75%] [G loss: 8.921434]\n",
      "831 [D loss: 0.492512, acc.: 91.41%] [G loss: 8.797030]\n",
      "832 [D loss: 1.054323, acc.: 87.50%] [G loss: 8.914405]\n",
      "833 [D loss: 0.902610, acc.: 94.53%] [G loss: 8.715304]\n",
      "834 [D loss: 0.783177, acc.: 88.28%] [G loss: 7.719059]\n",
      "835 [D loss: 1.688622, acc.: 88.28%] [G loss: 7.912403]\n",
      "836 [D loss: 1.080253, acc.: 89.84%] [G loss: 6.206006]\n",
      "837 [D loss: 1.195057, acc.: 85.16%] [G loss: 6.158806]\n",
      "838 [D loss: 0.665735, acc.: 96.09%] [G loss: 5.385987]\n",
      "839 [D loss: 1.350736, acc.: 89.06%] [G loss: 5.398463]\n",
      "840 [D loss: 0.545786, acc.: 96.09%] [G loss: 4.642519]\n",
      "841 [D loss: 0.688151, acc.: 95.31%] [G loss: 4.508649]\n",
      "842 [D loss: 1.932616, acc.: 84.38%] [G loss: 2.309826]\n",
      "843 [D loss: 0.433431, acc.: 79.69%] [G loss: 2.362196]\n",
      "844 [D loss: 0.462767, acc.: 86.72%] [G loss: 5.474431]\n",
      "845 [D loss: 1.138155, acc.: 71.88%] [G loss: 6.608897]\n",
      "846 [D loss: 0.676247, acc.: 80.47%] [G loss: 9.233129]\n",
      "847 [D loss: 0.538838, acc.: 96.88%] [G loss: 7.872167]\n",
      "848 [D loss: 0.687085, acc.: 80.47%] [G loss: 8.178961]\n",
      "849 [D loss: 0.384413, acc.: 89.84%] [G loss: 9.479927]\n",
      "850 [D loss: 0.175047, acc.: 96.09%] [G loss: 7.265748]\n",
      "851 [D loss: 0.436504, acc.: 93.75%] [G loss: 5.513624]\n",
      "852 [D loss: 0.568407, acc.: 92.19%] [G loss: 4.108763]\n",
      "853 [D loss: 0.430795, acc.: 96.88%] [G loss: 3.337630]\n",
      "854 [D loss: 0.974951, acc.: 90.62%] [G loss: 2.790824]\n",
      "855 [D loss: 0.513655, acc.: 92.19%] [G loss: 2.857055]\n",
      "856 [D loss: 0.982890, acc.: 91.41%] [G loss: 3.197745]\n",
      "857 [D loss: 0.696238, acc.: 96.09%] [G loss: 2.937536]\n",
      "858 [D loss: 0.866265, acc.: 92.19%] [G loss: 2.713093]\n",
      "859 [D loss: 0.284631, acc.: 92.97%] [G loss: 2.832675]\n",
      "860 [D loss: 0.593811, acc.: 96.09%] [G loss: 2.778817]\n",
      "861 [D loss: 0.343214, acc.: 96.09%] [G loss: 2.833606]\n",
      "862 [D loss: 0.093398, acc.: 97.66%] [G loss: 2.874257]\n",
      "863 [D loss: 0.461032, acc.: 96.09%] [G loss: 3.306727]\n",
      "864 [D loss: 0.482319, acc.: 94.53%] [G loss: 3.421776]\n",
      "865 [D loss: 0.808638, acc.: 95.31%] [G loss: 2.826526]\n",
      "866 [D loss: 0.873515, acc.: 90.62%] [G loss: 2.445899]\n",
      "867 [D loss: 0.606052, acc.: 93.75%] [G loss: 2.564225]\n",
      "868 [D loss: 0.477250, acc.: 94.53%] [G loss: 2.847338]\n",
      "869 [D loss: 0.177141, acc.: 90.62%] [G loss: 3.733281]\n",
      "870 [D loss: 0.660089, acc.: 81.25%] [G loss: 4.699019]\n",
      "871 [D loss: 0.933329, acc.: 85.94%] [G loss: 5.187363]\n",
      "872 [D loss: 0.525546, acc.: 96.88%] [G loss: 4.995482]\n",
      "873 [D loss: 0.667199, acc.: 95.31%] [G loss: 4.195917]\n",
      "874 [D loss: 0.678010, acc.: 95.31%] [G loss: 3.134720]\n",
      "875 [D loss: 0.370388, acc.: 92.19%] [G loss: 2.901911]\n",
      "876 [D loss: 0.810388, acc.: 93.75%] [G loss: 2.757785]\n",
      "877 [D loss: 0.736073, acc.: 92.97%] [G loss: 2.632426]\n",
      "878 [D loss: 0.489599, acc.: 94.53%] [G loss: 2.797747]\n",
      "879 [D loss: 0.809118, acc.: 93.75%] [G loss: 2.814249]\n",
      "880 [D loss: 0.562305, acc.: 95.31%] [G loss: 2.902019]\n",
      "881 [D loss: 0.336687, acc.: 94.53%] [G loss: 2.950592]\n",
      "882 [D loss: 0.313804, acc.: 96.09%] [G loss: 3.387905]\n",
      "883 [D loss: 0.561313, acc.: 89.06%] [G loss: 3.631857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 [D loss: 0.560634, acc.: 90.62%] [G loss: 3.565961]\n",
      "885 [D loss: 1.251844, acc.: 80.47%] [G loss: 2.603446]\n",
      "886 [D loss: 0.540021, acc.: 81.25%] [G loss: 3.647580]\n",
      "887 [D loss: 0.364780, acc.: 85.94%] [G loss: 4.305170]\n",
      "888 [D loss: 0.059309, acc.: 97.66%] [G loss: 3.888512]\n",
      "889 [D loss: 0.072226, acc.: 99.22%] [G loss: 3.179628]\n",
      "890 [D loss: 0.212004, acc.: 96.88%] [G loss: 2.690200]\n",
      "891 [D loss: 0.082936, acc.: 98.44%] [G loss: 2.401049]\n",
      "892 [D loss: 0.260264, acc.: 94.53%] [G loss: 2.651033]\n",
      "893 [D loss: 0.083805, acc.: 96.09%] [G loss: 2.749444]\n",
      "894 [D loss: 0.289788, acc.: 92.97%] [G loss: 2.093229]\n",
      "895 [D loss: 0.344699, acc.: 80.47%] [G loss: 3.625991]\n",
      "896 [D loss: 0.656256, acc.: 82.81%] [G loss: 4.030784]\n",
      "897 [D loss: 0.160574, acc.: 93.75%] [G loss: 4.591647]\n",
      "898 [D loss: 0.038834, acc.: 99.22%] [G loss: 3.617492]\n",
      "899 [D loss: 0.088108, acc.: 96.88%] [G loss: 2.963572]\n",
      "900 [D loss: 0.403793, acc.: 92.19%] [G loss: 2.548535]\n",
      "901 [D loss: 0.188014, acc.: 90.62%] [G loss: 2.366331]\n",
      "902 [D loss: 0.250364, acc.: 87.50%] [G loss: 2.927525]\n",
      "903 [D loss: 0.228544, acc.: 87.50%] [G loss: 3.188972]\n",
      "904 [D loss: 0.174662, acc.: 91.41%] [G loss: 3.230772]\n",
      "905 [D loss: 0.214134, acc.: 96.09%] [G loss: 2.922335]\n",
      "906 [D loss: 0.302567, acc.: 90.62%] [G loss: 3.229820]\n",
      "907 [D loss: 0.431017, acc.: 75.00%] [G loss: 2.767727]\n",
      "908 [D loss: 0.279951, acc.: 85.16%] [G loss: 3.319971]\n",
      "909 [D loss: 0.107354, acc.: 96.88%] [G loss: 3.331413]\n",
      "910 [D loss: 0.348237, acc.: 96.88%] [G loss: 2.562311]\n",
      "911 [D loss: 0.180989, acc.: 90.62%] [G loss: 2.261400]\n",
      "912 [D loss: 0.137070, acc.: 94.53%] [G loss: 2.052083]\n",
      "913 [D loss: 0.449643, acc.: 89.84%] [G loss: 2.223207]\n",
      "914 [D loss: 0.566417, acc.: 88.28%] [G loss: 2.272628]\n",
      "915 [D loss: 0.424088, acc.: 92.19%] [G loss: 2.847795]\n",
      "916 [D loss: 0.306575, acc.: 88.28%] [G loss: 2.900324]\n",
      "917 [D loss: 0.362966, acc.: 84.38%] [G loss: 3.314535]\n",
      "918 [D loss: 0.751945, acc.: 89.84%] [G loss: 3.943235]\n",
      "919 [D loss: 0.365315, acc.: 93.75%] [G loss: 2.996254]\n",
      "920 [D loss: 0.499791, acc.: 92.97%] [G loss: 2.654535]\n",
      "921 [D loss: 0.282683, acc.: 94.53%] [G loss: 2.506732]\n",
      "922 [D loss: 0.398981, acc.: 92.97%] [G loss: 2.165885]\n",
      "923 [D loss: 0.490665, acc.: 93.75%] [G loss: 2.083890]\n",
      "924 [D loss: 0.786645, acc.: 89.06%] [G loss: 2.205996]\n",
      "925 [D loss: 0.336704, acc.: 96.88%] [G loss: 2.196707]\n",
      "926 [D loss: 0.534094, acc.: 91.41%] [G loss: 2.685408]\n",
      "927 [D loss: 0.496309, acc.: 84.38%] [G loss: 3.460797]\n",
      "928 [D loss: 0.981424, acc.: 71.88%] [G loss: 4.263093]\n",
      "929 [D loss: 0.531292, acc.: 79.69%] [G loss: 6.212345]\n",
      "930 [D loss: 0.307909, acc.: 96.88%] [G loss: 5.674430]\n",
      "931 [D loss: 0.431612, acc.: 96.09%] [G loss: 4.478162]\n",
      "932 [D loss: 0.387514, acc.: 92.97%] [G loss: 3.689987]\n",
      "933 [D loss: 0.125161, acc.: 94.53%] [G loss: 3.186411]\n",
      "934 [D loss: 0.738966, acc.: 93.75%] [G loss: 2.523681]\n",
      "935 [D loss: 0.416204, acc.: 89.06%] [G loss: 2.350484]\n",
      "936 [D loss: 0.911262, acc.: 87.50%] [G loss: 2.478378]\n",
      "937 [D loss: 0.319052, acc.: 97.66%] [G loss: 2.330308]\n",
      "938 [D loss: 0.901891, acc.: 87.50%] [G loss: 2.404717]\n",
      "939 [D loss: 1.280221, acc.: 87.50%] [G loss: 2.434688]\n",
      "940 [D loss: 0.728077, acc.: 92.97%] [G loss: 2.478945]\n",
      "941 [D loss: 0.846766, acc.: 93.75%] [G loss: 2.729803]\n",
      "942 [D loss: 0.310352, acc.: 98.44%] [G loss: 3.222513]\n",
      "943 [D loss: 0.972748, acc.: 92.97%] [G loss: 2.698744]\n",
      "944 [D loss: 0.907547, acc.: 91.41%] [G loss: 2.403289]\n",
      "945 [D loss: 0.894382, acc.: 91.41%] [G loss: 2.007514]\n",
      "946 [D loss: 1.034381, acc.: 88.28%] [G loss: 2.470237]\n",
      "947 [D loss: 0.383356, acc.: 92.19%] [G loss: 3.279868]\n",
      "948 [D loss: 0.834164, acc.: 92.19%] [G loss: 3.444710]\n",
      "949 [D loss: 0.167496, acc.: 99.22%] [G loss: 3.047079]\n",
      "950 [D loss: 1.332408, acc.: 91.41%] [G loss: 2.563629]\n",
      "951 [D loss: 0.963995, acc.: 93.75%] [G loss: 2.535976]\n",
      "952 [D loss: 1.094822, acc.: 91.41%] [G loss: 2.383298]\n",
      "953 [D loss: 1.144755, acc.: 86.72%] [G loss: 3.037473]\n",
      "954 [D loss: 0.909859, acc.: 80.47%] [G loss: 4.221267]\n",
      "955 [D loss: 1.122311, acc.: 72.66%] [G loss: 5.893722]\n",
      "956 [D loss: 0.858016, acc.: 83.59%] [G loss: 7.138232]\n",
      "957 [D loss: 0.788530, acc.: 94.53%] [G loss: 6.882746]\n",
      "958 [D loss: 0.294616, acc.: 97.66%] [G loss: 5.336874]\n",
      "959 [D loss: 0.668219, acc.: 96.09%] [G loss: 4.415531]\n",
      "960 [D loss: 0.792324, acc.: 95.31%] [G loss: 3.640743]\n",
      "961 [D loss: 0.109873, acc.: 97.66%] [G loss: 2.928917]\n",
      "962 [D loss: 0.307292, acc.: 98.44%] [G loss: 2.975655]\n",
      "963 [D loss: 0.449767, acc.: 96.88%] [G loss: 2.900110]\n",
      "964 [D loss: 0.691232, acc.: 92.97%] [G loss: 2.479358]\n",
      "965 [D loss: 0.391012, acc.: 96.09%] [G loss: 2.476110]\n",
      "966 [D loss: 0.466933, acc.: 93.75%] [G loss: 2.606477]\n",
      "967 [D loss: 0.712586, acc.: 93.75%] [G loss: 2.805409]\n",
      "968 [D loss: 0.321246, acc.: 97.66%] [G loss: 1.943094]\n",
      "969 [D loss: 0.463465, acc.: 96.09%] [G loss: 2.198599]\n",
      "970 [D loss: 0.212312, acc.: 98.44%] [G loss: 1.877503]\n",
      "971 [D loss: 0.140689, acc.: 94.53%] [G loss: 2.474911]\n",
      "972 [D loss: 0.238215, acc.: 95.31%] [G loss: 3.442143]\n",
      "973 [D loss: 0.372851, acc.: 86.72%] [G loss: 4.398174]\n",
      "974 [D loss: 0.483081, acc.: 83.59%] [G loss: 4.850847]\n",
      "975 [D loss: 0.065748, acc.: 99.22%] [G loss: 5.041991]\n",
      "976 [D loss: 0.150916, acc.: 98.44%] [G loss: 3.942740]\n",
      "977 [D loss: 0.055586, acc.: 99.22%] [G loss: 3.124547]\n",
      "978 [D loss: 0.436964, acc.: 96.88%] [G loss: 2.584752]\n",
      "979 [D loss: 0.211613, acc.: 96.09%] [G loss: 2.212458]\n",
      "980 [D loss: 0.110493, acc.: 96.88%] [G loss: 2.397451]\n",
      "981 [D loss: 0.350022, acc.: 95.31%] [G loss: 2.942093]\n",
      "982 [D loss: 0.212329, acc.: 95.31%] [G loss: 2.608546]\n",
      "983 [D loss: 0.233365, acc.: 96.09%] [G loss: 2.555389]\n",
      "984 [D loss: 0.381035, acc.: 92.19%] [G loss: 2.600471]\n",
      "985 [D loss: 0.919647, acc.: 79.69%] [G loss: 2.935620]\n",
      "986 [D loss: 0.422059, acc.: 89.84%] [G loss: 3.711112]\n",
      "987 [D loss: 0.286988, acc.: 89.84%] [G loss: 3.594617]\n",
      "988 [D loss: 0.427860, acc.: 96.88%] [G loss: 3.218039]\n",
      "989 [D loss: 0.655382, acc.: 91.41%] [G loss: 3.048808]\n",
      "990 [D loss: 0.043238, acc.: 100.00%] [G loss: 2.533063]\n",
      "991 [D loss: 0.198226, acc.: 98.44%] [G loss: 2.335037]\n",
      "992 [D loss: 0.110196, acc.: 96.09%] [G loss: 2.375336]\n",
      "993 [D loss: 0.340724, acc.: 98.44%] [G loss: 2.629662]\n",
      "994 [D loss: 0.444854, acc.: 96.09%] [G loss: 2.359368]\n",
      "995 [D loss: 0.244716, acc.: 96.09%] [G loss: 1.993124]\n",
      "996 [D loss: 0.443301, acc.: 89.84%] [G loss: 2.645985]\n",
      "997 [D loss: 0.838735, acc.: 78.12%] [G loss: 3.709975]\n",
      "998 [D loss: 0.406092, acc.: 76.56%] [G loss: 5.842306]\n",
      "999 [D loss: 0.019214, acc.: 100.00%] [G loss: 5.727178]\n",
      "1000 [D loss: 0.661031, acc.: 96.09%] [G loss: 4.166444]\n",
      "1001 [D loss: 0.291024, acc.: 96.09%] [G loss: 3.439436]\n",
      "1002 [D loss: 0.561701, acc.: 96.88%] [G loss: 2.694813]\n",
      "1003 [D loss: 0.700895, acc.: 95.31%] [G loss: 2.438224]\n",
      "1004 [D loss: 0.730161, acc.: 94.53%] [G loss: 2.114872]\n",
      "1005 [D loss: 0.209075, acc.: 98.44%] [G loss: 1.929969]\n",
      "1006 [D loss: 0.316883, acc.: 88.28%] [G loss: 2.495019]\n",
      "1007 [D loss: 1.098131, acc.: 82.03%] [G loss: 3.265094]\n",
      "1008 [D loss: 0.421830, acc.: 89.06%] [G loss: 3.890153]\n",
      "1009 [D loss: 0.334353, acc.: 97.66%] [G loss: 3.299565]\n",
      "1010 [D loss: 0.941049, acc.: 93.75%] [G loss: 2.427372]\n",
      "1011 [D loss: 0.602840, acc.: 93.75%] [G loss: 2.329995]\n",
      "1012 [D loss: 0.442225, acc.: 97.66%] [G loss: 2.131151]\n",
      "1013 [D loss: 0.835302, acc.: 93.75%] [G loss: 1.945314]\n",
      "1014 [D loss: 0.347786, acc.: 96.88%] [G loss: 2.122083]\n",
      "1015 [D loss: 0.231979, acc.: 97.66%] [G loss: 2.314250]\n",
      "1016 [D loss: 0.396491, acc.: 92.19%] [G loss: 2.787629]\n",
      "1017 [D loss: 0.577965, acc.: 92.97%] [G loss: 3.190913]\n",
      "1018 [D loss: 0.441153, acc.: 96.88%] [G loss: 3.749758]\n",
      "1019 [D loss: 0.182403, acc.: 97.66%] [G loss: 2.902523]\n",
      "1020 [D loss: 0.449078, acc.: 96.09%] [G loss: 2.209368]\n",
      "1021 [D loss: 0.343588, acc.: 96.88%] [G loss: 2.134606]\n",
      "1022 [D loss: 0.483211, acc.: 94.53%] [G loss: 2.556218]\n",
      "1023 [D loss: 0.317451, acc.: 96.88%] [G loss: 2.892461]\n",
      "1024 [D loss: 0.221158, acc.: 96.09%] [G loss: 2.544919]\n",
      "1025 [D loss: 0.459674, acc.: 96.88%] [G loss: 2.503635]\n",
      "1026 [D loss: 0.954455, acc.: 93.75%] [G loss: 2.092131]\n",
      "1027 [D loss: 0.478354, acc.: 79.69%] [G loss: 1.793094]\n",
      "1028 [D loss: 0.506503, acc.: 81.25%] [G loss: 3.154413]\n",
      "1029 [D loss: 1.020979, acc.: 71.88%] [G loss: 5.934619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 [D loss: 1.057207, acc.: 79.69%] [G loss: 6.381966]\n",
      "1031 [D loss: 0.034188, acc.: 97.66%] [G loss: 5.562993]\n",
      "1032 [D loss: 0.125600, acc.: 94.53%] [G loss: 3.776755]\n",
      "1033 [D loss: 0.084727, acc.: 97.66%] [G loss: 3.505814]\n",
      "1034 [D loss: 0.091148, acc.: 96.88%] [G loss: 3.005872]\n",
      "1035 [D loss: 0.073026, acc.: 99.22%] [G loss: 2.169245]\n",
      "1036 [D loss: 0.250170, acc.: 85.16%] [G loss: 2.328502]\n",
      "1037 [D loss: 0.327256, acc.: 88.28%] [G loss: 2.419247]\n",
      "1038 [D loss: 0.142491, acc.: 95.31%] [G loss: 2.211694]\n",
      "1039 [D loss: 0.298866, acc.: 93.75%] [G loss: 2.237524]\n",
      "1040 [D loss: 0.212674, acc.: 86.72%] [G loss: 2.152113]\n",
      "1041 [D loss: 0.150914, acc.: 95.31%] [G loss: 2.338754]\n",
      "1042 [D loss: 0.122255, acc.: 95.31%] [G loss: 1.975609]\n",
      "1043 [D loss: 0.214127, acc.: 91.41%] [G loss: 1.992070]\n",
      "1044 [D loss: 0.303315, acc.: 91.41%] [G loss: 2.068012]\n",
      "1045 [D loss: 0.199082, acc.: 89.84%] [G loss: 2.048314]\n",
      "1046 [D loss: 0.165008, acc.: 94.53%] [G loss: 1.997338]\n",
      "1047 [D loss: 0.196363, acc.: 91.41%] [G loss: 2.476890]\n",
      "1048 [D loss: 0.176087, acc.: 91.41%] [G loss: 2.323810]\n",
      "1049 [D loss: 0.272989, acc.: 83.59%] [G loss: 2.387273]\n",
      "1050 [D loss: 0.346134, acc.: 81.25%] [G loss: 3.080407]\n",
      "1051 [D loss: 0.364903, acc.: 92.19%] [G loss: 2.719799]\n",
      "1052 [D loss: 0.313833, acc.: 91.41%] [G loss: 2.392982]\n",
      "1053 [D loss: 0.500131, acc.: 93.75%] [G loss: 2.285985]\n",
      "1054 [D loss: 0.287817, acc.: 92.97%] [G loss: 2.392581]\n",
      "1055 [D loss: 0.310390, acc.: 92.19%] [G loss: 2.449101]\n",
      "1056 [D loss: 0.508084, acc.: 92.97%] [G loss: 2.359582]\n",
      "1057 [D loss: 0.275052, acc.: 92.19%] [G loss: 2.214506]\n",
      "1058 [D loss: 0.213728, acc.: 90.62%] [G loss: 2.199939]\n",
      "1059 [D loss: 0.146622, acc.: 94.53%] [G loss: 2.189555]\n",
      "1060 [D loss: 0.270355, acc.: 90.62%] [G loss: 2.249283]\n",
      "1061 [D loss: 0.241589, acc.: 96.09%] [G loss: 2.157734]\n",
      "1062 [D loss: 0.205294, acc.: 89.84%] [G loss: 2.314559]\n",
      "1063 [D loss: 0.358407, acc.: 95.31%] [G loss: 2.310380]\n",
      "1064 [D loss: 0.452779, acc.: 89.06%] [G loss: 2.753300]\n",
      "1065 [D loss: 0.393013, acc.: 90.62%] [G loss: 3.062846]\n",
      "1066 [D loss: 0.600336, acc.: 94.53%] [G loss: 3.207187]\n",
      "1067 [D loss: 0.684278, acc.: 95.31%] [G loss: 3.273585]\n",
      "1068 [D loss: 0.265172, acc.: 93.75%] [G loss: 2.685883]\n",
      "1069 [D loss: 0.690600, acc.: 89.84%] [G loss: 2.290152]\n",
      "1070 [D loss: 0.307074, acc.: 91.41%] [G loss: 2.202950]\n",
      "1071 [D loss: 0.644609, acc.: 90.62%] [G loss: 2.630790]\n",
      "1072 [D loss: 0.716979, acc.: 95.31%] [G loss: 2.715641]\n",
      "1073 [D loss: 0.781624, acc.: 89.84%] [G loss: 3.034786]\n",
      "1074 [D loss: 0.230754, acc.: 95.31%] [G loss: 2.893036]\n",
      "1075 [D loss: 0.252512, acc.: 93.75%] [G loss: 2.673113]\n",
      "1076 [D loss: 0.475667, acc.: 95.31%] [G loss: 1.989103]\n",
      "1077 [D loss: 0.275729, acc.: 92.97%] [G loss: 2.703893]\n",
      "1078 [D loss: 0.610039, acc.: 92.19%] [G loss: 3.194491]\n",
      "1079 [D loss: 0.857902, acc.: 90.62%] [G loss: 3.080491]\n",
      "1080 [D loss: 0.420548, acc.: 97.66%] [G loss: 2.805061]\n",
      "1081 [D loss: 0.715456, acc.: 92.19%] [G loss: 2.478752]\n",
      "1082 [D loss: 0.630884, acc.: 93.75%] [G loss: 2.686083]\n",
      "1083 [D loss: 0.676932, acc.: 96.09%] [G loss: 2.599726]\n",
      "1084 [D loss: 0.818742, acc.: 94.53%] [G loss: 2.790368]\n",
      "1085 [D loss: 0.087322, acc.: 96.88%] [G loss: 2.934024]\n",
      "1086 [D loss: 0.458593, acc.: 96.09%] [G loss: 2.946974]\n",
      "1087 [D loss: 0.114723, acc.: 97.66%] [G loss: 2.578056]\n",
      "1088 [D loss: 0.597079, acc.: 95.31%] [G loss: 2.735533]\n",
      "1089 [D loss: 0.364102, acc.: 95.31%] [G loss: 2.388418]\n",
      "1090 [D loss: 0.364641, acc.: 96.88%] [G loss: 2.493112]\n",
      "1091 [D loss: 0.344363, acc.: 96.88%] [G loss: 2.945018]\n",
      "1092 [D loss: 1.180630, acc.: 92.19%] [G loss: 2.841494]\n",
      "1093 [D loss: 0.332269, acc.: 97.66%] [G loss: 2.189005]\n",
      "1094 [D loss: 0.386111, acc.: 92.19%] [G loss: 2.857587]\n",
      "1095 [D loss: 0.471965, acc.: 96.09%] [G loss: 2.737281]\n",
      "1096 [D loss: 0.502270, acc.: 92.97%] [G loss: 3.234821]\n",
      "1097 [D loss: 0.560134, acc.: 85.94%] [G loss: 2.914326]\n",
      "1098 [D loss: 0.654670, acc.: 71.88%] [G loss: 5.191478]\n",
      "1099 [D loss: 0.757900, acc.: 82.81%] [G loss: 7.059241]\n",
      "1100 [D loss: 0.383754, acc.: 97.66%] [G loss: 6.174893]\n",
      "1101 [D loss: 0.304454, acc.: 96.09%] [G loss: 5.314915]\n",
      "1102 [D loss: 0.176092, acc.: 97.66%] [G loss: 4.457705]\n",
      "1103 [D loss: 0.923022, acc.: 93.75%] [G loss: 3.379917]\n",
      "1104 [D loss: 0.589254, acc.: 94.53%] [G loss: 3.087586]\n",
      "1105 [D loss: 0.437423, acc.: 96.88%] [G loss: 2.514456]\n",
      "1106 [D loss: 0.342349, acc.: 96.88%] [G loss: 2.546824]\n",
      "1107 [D loss: 0.447901, acc.: 96.88%] [G loss: 2.671723]\n",
      "1108 [D loss: 0.318144, acc.: 96.88%] [G loss: 2.546572]\n",
      "1109 [D loss: 0.298615, acc.: 98.44%] [G loss: 2.434766]\n",
      "1110 [D loss: 0.585184, acc.: 95.31%] [G loss: 2.174894]\n",
      "1111 [D loss: 0.197940, acc.: 98.44%] [G loss: 2.171983]\n",
      "1112 [D loss: 0.605519, acc.: 94.53%] [G loss: 2.199805]\n",
      "1113 [D loss: 0.295979, acc.: 90.62%] [G loss: 2.709340]\n",
      "1114 [D loss: 0.523562, acc.: 92.19%] [G loss: 2.984401]\n",
      "1115 [D loss: 0.947828, acc.: 92.97%] [G loss: 2.837250]\n",
      "1116 [D loss: 0.535360, acc.: 96.09%] [G loss: 2.520818]\n",
      "1117 [D loss: 0.800559, acc.: 80.47%] [G loss: 1.289906]\n",
      "1118 [D loss: 0.326976, acc.: 82.81%] [G loss: 3.037795]\n",
      "1119 [D loss: 0.601135, acc.: 87.50%] [G loss: 3.589019]\n",
      "1120 [D loss: 0.292597, acc.: 90.62%] [G loss: 3.827730]\n",
      "1121 [D loss: 0.127747, acc.: 94.53%] [G loss: 3.206921]\n",
      "1122 [D loss: 0.140408, acc.: 89.84%] [G loss: 3.468206]\n",
      "1123 [D loss: 0.082200, acc.: 97.66%] [G loss: 2.978356]\n",
      "1124 [D loss: 0.107928, acc.: 96.09%] [G loss: 2.385819]\n",
      "1125 [D loss: 0.397082, acc.: 82.03%] [G loss: 2.681380]\n",
      "1126 [D loss: 0.619748, acc.: 78.91%] [G loss: 3.418818]\n",
      "1127 [D loss: 0.663736, acc.: 78.12%] [G loss: 4.209787]\n",
      "1128 [D loss: 0.345560, acc.: 87.50%] [G loss: 5.043636]\n",
      "1129 [D loss: 0.061120, acc.: 97.66%] [G loss: 3.926244]\n",
      "1130 [D loss: 0.090240, acc.: 98.44%] [G loss: 3.368677]\n",
      "1131 [D loss: 0.226283, acc.: 97.66%] [G loss: 2.106333]\n",
      "1132 [D loss: 0.229744, acc.: 87.50%] [G loss: 1.964978]\n",
      "1133 [D loss: 0.303408, acc.: 91.41%] [G loss: 2.162861]\n",
      "1134 [D loss: 0.132547, acc.: 95.31%] [G loss: 1.893609]\n",
      "1135 [D loss: 0.414874, acc.: 86.72%] [G loss: 1.866943]\n",
      "1136 [D loss: 0.353862, acc.: 80.47%] [G loss: 2.694489]\n",
      "1137 [D loss: 0.604723, acc.: 78.12%] [G loss: 3.360389]\n",
      "1138 [D loss: 0.336740, acc.: 78.91%] [G loss: 3.866854]\n",
      "1139 [D loss: 0.241384, acc.: 93.75%] [G loss: 3.330577]\n",
      "1140 [D loss: 0.229714, acc.: 96.88%] [G loss: 2.637905]\n",
      "1141 [D loss: 0.171846, acc.: 89.84%] [G loss: 2.153165]\n",
      "1142 [D loss: 0.459067, acc.: 90.62%] [G loss: 1.649172]\n",
      "1143 [D loss: 0.295136, acc.: 86.72%] [G loss: 1.945025]\n",
      "1144 [D loss: 0.158208, acc.: 94.53%] [G loss: 2.190306]\n",
      "1145 [D loss: 0.305990, acc.: 89.84%] [G loss: 2.217604]\n",
      "1146 [D loss: 0.370556, acc.: 90.62%] [G loss: 2.393896]\n",
      "1147 [D loss: 0.298659, acc.: 84.38%] [G loss: 2.767325]\n",
      "1148 [D loss: 0.678480, acc.: 84.38%] [G loss: 2.836525]\n",
      "1149 [D loss: 0.276540, acc.: 85.94%] [G loss: 2.853782]\n",
      "1150 [D loss: 0.251932, acc.: 85.94%] [G loss: 3.080431]\n",
      "1151 [D loss: 0.231744, acc.: 95.31%] [G loss: 2.771834]\n",
      "1152 [D loss: 0.583337, acc.: 86.72%] [G loss: 2.216444]\n",
      "1153 [D loss: 0.358604, acc.: 87.50%] [G loss: 2.549161]\n",
      "1154 [D loss: 0.981547, acc.: 85.94%] [G loss: 2.445990]\n",
      "1155 [D loss: 0.475644, acc.: 80.47%] [G loss: 2.825094]\n",
      "1156 [D loss: 0.233651, acc.: 88.28%] [G loss: 3.262218]\n",
      "1157 [D loss: 0.235345, acc.: 94.53%] [G loss: 2.902063]\n",
      "1158 [D loss: 0.254205, acc.: 94.53%] [G loss: 2.588233]\n",
      "1159 [D loss: 0.747849, acc.: 92.19%] [G loss: 2.471082]\n",
      "1160 [D loss: 0.418964, acc.: 92.97%] [G loss: 2.465304]\n",
      "1161 [D loss: 0.273746, acc.: 94.53%] [G loss: 2.391326]\n",
      "1162 [D loss: 0.235459, acc.: 87.50%] [G loss: 2.788130]\n",
      "1163 [D loss: 0.487164, acc.: 85.94%] [G loss: 3.373410]\n",
      "1164 [D loss: 0.332200, acc.: 95.31%] [G loss: 3.716973]\n",
      "1165 [D loss: 0.556759, acc.: 96.88%] [G loss: 2.868009]\n",
      "1166 [D loss: 0.385895, acc.: 92.97%] [G loss: 2.842638]\n",
      "1167 [D loss: 0.739308, acc.: 76.56%] [G loss: 1.683359]\n",
      "1168 [D loss: 0.552894, acc.: 69.53%] [G loss: 3.328935]\n",
      "1169 [D loss: 0.269161, acc.: 89.06%] [G loss: 3.693600]\n",
      "1170 [D loss: 0.602001, acc.: 82.03%] [G loss: 4.086512]\n",
      "1171 [D loss: 0.410753, acc.: 90.62%] [G loss: 4.511586]\n",
      "1172 [D loss: 0.040817, acc.: 100.00%] [G loss: 3.248056]\n",
      "1173 [D loss: 0.098237, acc.: 96.09%] [G loss: 2.257255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174 [D loss: 0.449280, acc.: 91.41%] [G loss: 1.934534]\n",
      "1175 [D loss: 0.387669, acc.: 89.84%] [G loss: 2.357688]\n",
      "1176 [D loss: 0.325035, acc.: 89.06%] [G loss: 2.509999]\n",
      "1177 [D loss: 0.285525, acc.: 90.62%] [G loss: 2.524761]\n",
      "1178 [D loss: 0.471883, acc.: 89.06%] [G loss: 2.710022]\n",
      "1179 [D loss: 0.456777, acc.: 86.72%] [G loss: 2.929973]\n",
      "1180 [D loss: 0.565590, acc.: 81.25%] [G loss: 3.222234]\n",
      "1181 [D loss: 0.131717, acc.: 94.53%] [G loss: 3.205006]\n",
      "1182 [D loss: 0.106682, acc.: 96.09%] [G loss: 2.529502]\n",
      "1183 [D loss: 0.481781, acc.: 94.53%] [G loss: 2.412292]\n",
      "1184 [D loss: 0.663019, acc.: 89.06%] [G loss: 2.219291]\n",
      "1185 [D loss: 0.398114, acc.: 92.97%] [G loss: 2.295351]\n",
      "1186 [D loss: 0.440597, acc.: 92.19%] [G loss: 2.099003]\n",
      "1187 [D loss: 0.157195, acc.: 92.19%] [G loss: 2.536289]\n",
      "1188 [D loss: 0.473463, acc.: 93.75%] [G loss: 2.208386]\n",
      "1189 [D loss: 0.137892, acc.: 92.19%] [G loss: 2.450654]\n",
      "1190 [D loss: 0.603188, acc.: 87.50%] [G loss: 2.496435]\n",
      "1191 [D loss: 0.365728, acc.: 85.94%] [G loss: 2.926076]\n",
      "1192 [D loss: 0.391556, acc.: 92.97%] [G loss: 3.133956]\n",
      "1193 [D loss: 0.227174, acc.: 95.31%] [G loss: 2.974941]\n",
      "1194 [D loss: 0.238968, acc.: 95.31%] [G loss: 2.941281]\n",
      "1195 [D loss: 0.078624, acc.: 97.66%] [G loss: 2.540725]\n",
      "1196 [D loss: 0.275107, acc.: 95.31%] [G loss: 2.508030]\n",
      "1197 [D loss: 0.719025, acc.: 85.94%] [G loss: 2.324980]\n",
      "1198 [D loss: 0.418991, acc.: 84.38%] [G loss: 2.870841]\n",
      "1199 [D loss: 0.334181, acc.: 87.50%] [G loss: 3.653071]\n",
      "1200 [D loss: 0.249313, acc.: 83.59%] [G loss: 3.862267]\n",
      "1201 [D loss: 0.060746, acc.: 99.22%] [G loss: 3.508516]\n",
      "1202 [D loss: 0.569574, acc.: 95.31%] [G loss: 2.653215]\n",
      "1203 [D loss: 0.468566, acc.: 94.53%] [G loss: 2.414586]\n",
      "1204 [D loss: 0.374631, acc.: 94.53%] [G loss: 2.280009]\n",
      "1205 [D loss: 0.233843, acc.: 94.53%] [G loss: 2.151031]\n",
      "1206 [D loss: 0.432854, acc.: 89.84%] [G loss: 2.122909]\n",
      "1207 [D loss: 0.542417, acc.: 81.25%] [G loss: 2.754954]\n",
      "1208 [D loss: 1.049153, acc.: 71.88%] [G loss: 4.121827]\n",
      "1209 [D loss: 0.796689, acc.: 80.47%] [G loss: 4.713105]\n",
      "1210 [D loss: 0.169986, acc.: 98.44%] [G loss: 4.318436]\n",
      "1211 [D loss: 0.695616, acc.: 95.31%] [G loss: 3.862250]\n",
      "1212 [D loss: 0.469592, acc.: 94.53%] [G loss: 3.410853]\n",
      "1213 [D loss: 0.304814, acc.: 96.09%] [G loss: 3.259642]\n",
      "1214 [D loss: 0.342753, acc.: 94.53%] [G loss: 2.934262]\n",
      "1215 [D loss: 1.595365, acc.: 88.28%] [G loss: 2.809587]\n",
      "1216 [D loss: 0.743695, acc.: 92.19%] [G loss: 2.791535]\n",
      "1217 [D loss: 0.230733, acc.: 96.09%] [G loss: 2.571459]\n",
      "1218 [D loss: 0.664656, acc.: 92.97%] [G loss: 2.596390]\n",
      "1219 [D loss: 0.493007, acc.: 93.75%] [G loss: 2.299128]\n",
      "1220 [D loss: 0.235393, acc.: 96.09%] [G loss: 2.528621]\n",
      "1221 [D loss: 0.214042, acc.: 97.66%] [G loss: 2.864594]\n",
      "1222 [D loss: 0.791425, acc.: 95.31%] [G loss: 2.773157]\n",
      "1223 [D loss: 0.342476, acc.: 96.09%] [G loss: 2.698340]\n",
      "1224 [D loss: 0.677052, acc.: 88.28%] [G loss: 2.761841]\n",
      "1225 [D loss: 0.287823, acc.: 93.75%] [G loss: 2.852136]\n",
      "1226 [D loss: 0.559445, acc.: 84.38%] [G loss: 3.368673]\n",
      "1227 [D loss: 0.394714, acc.: 96.88%] [G loss: 3.381508]\n",
      "1228 [D loss: 0.822950, acc.: 93.75%] [G loss: 2.685375]\n",
      "1229 [D loss: 0.699367, acc.: 95.31%] [G loss: 2.357800]\n",
      "1230 [D loss: 0.341615, acc.: 96.09%] [G loss: 2.299654]\n",
      "1231 [D loss: 0.554098, acc.: 96.88%] [G loss: 2.244296]\n",
      "1232 [D loss: 0.592582, acc.: 96.09%] [G loss: 2.236953]\n",
      "1233 [D loss: 1.221638, acc.: 92.19%] [G loss: 2.452040]\n",
      "1234 [D loss: 0.293876, acc.: 98.44%] [G loss: 3.114196]\n",
      "1235 [D loss: 0.527412, acc.: 96.88%] [G loss: 3.396676]\n",
      "1236 [D loss: 0.552687, acc.: 96.88%] [G loss: 3.725716]\n",
      "1237 [D loss: 0.455965, acc.: 95.31%] [G loss: 2.867745]\n",
      "1238 [D loss: 0.624035, acc.: 94.53%] [G loss: 2.270774]\n",
      "1239 [D loss: 1.052701, acc.: 89.06%] [G loss: 3.153469]\n",
      "1240 [D loss: 1.023674, acc.: 78.12%] [G loss: 4.926322]\n",
      "1241 [D loss: 0.479032, acc.: 89.84%] [G loss: 5.037046]\n",
      "1242 [D loss: 0.133051, acc.: 99.22%] [G loss: 4.645697]\n",
      "1243 [D loss: 0.540784, acc.: 96.88%] [G loss: 3.787494]\n",
      "1244 [D loss: 1.043778, acc.: 92.97%] [G loss: 3.293467]\n",
      "1245 [D loss: 0.412660, acc.: 97.66%] [G loss: 3.157604]\n",
      "1246 [D loss: 0.658654, acc.: 96.09%] [G loss: 2.816439]\n",
      "1247 [D loss: 0.285201, acc.: 98.44%] [G loss: 2.657341]\n",
      "1248 [D loss: 0.540900, acc.: 96.88%] [G loss: 2.705071]\n",
      "1249 [D loss: 0.995188, acc.: 93.75%] [G loss: 2.346209]\n",
      "1250 [D loss: 0.605389, acc.: 92.97%] [G loss: 2.192680]\n",
      "1251 [D loss: 0.514408, acc.: 92.19%] [G loss: 1.947771]\n",
      "1252 [D loss: 0.445403, acc.: 90.62%] [G loss: 3.389822]\n",
      "1253 [D loss: 0.736018, acc.: 81.25%] [G loss: 4.421445]\n",
      "1254 [D loss: 0.573575, acc.: 93.75%] [G loss: 4.920053]\n",
      "1255 [D loss: 0.135400, acc.: 99.22%] [G loss: 3.854836]\n",
      "1256 [D loss: 0.269029, acc.: 98.44%] [G loss: 3.285412]\n",
      "1257 [D loss: 0.405681, acc.: 97.66%] [G loss: 2.920834]\n",
      "1258 [D loss: 0.408964, acc.: 97.66%] [G loss: 2.988659]\n",
      "1259 [D loss: 0.289324, acc.: 98.44%] [G loss: 2.639631]\n",
      "1260 [D loss: 0.821725, acc.: 92.97%] [G loss: 2.177842]\n",
      "1261 [D loss: 0.320595, acc.: 97.66%] [G loss: 2.218001]\n",
      "1262 [D loss: 0.424774, acc.: 97.66%] [G loss: 2.123814]\n",
      "1263 [D loss: 0.550971, acc.: 96.88%] [G loss: 2.164857]\n",
      "1264 [D loss: 0.173811, acc.: 99.22%] [G loss: 2.109161]\n",
      "1265 [D loss: 0.182962, acc.: 98.44%] [G loss: 2.431956]\n",
      "1266 [D loss: 0.428622, acc.: 96.88%] [G loss: 3.032956]\n",
      "1267 [D loss: 0.140110, acc.: 99.22%] [G loss: 3.272062]\n",
      "1268 [D loss: 0.361750, acc.: 92.97%] [G loss: 2.826085]\n",
      "1269 [D loss: 0.797739, acc.: 77.34%] [G loss: 3.369285]\n",
      "1270 [D loss: 0.279521, acc.: 92.19%] [G loss: 3.852390]\n",
      "1271 [D loss: 0.359932, acc.: 92.97%] [G loss: 3.843493]\n",
      "1272 [D loss: 0.301173, acc.: 98.44%] [G loss: 3.684876]\n",
      "1273 [D loss: 0.776178, acc.: 95.31%] [G loss: 3.331877]\n",
      "1274 [D loss: 0.270692, acc.: 98.44%] [G loss: 2.775141]\n",
      "1275 [D loss: 0.654857, acc.: 96.09%] [G loss: 2.558493]\n",
      "1276 [D loss: 0.420711, acc.: 97.66%] [G loss: 2.590393]\n",
      "1277 [D loss: 0.534613, acc.: 96.88%] [G loss: 2.834102]\n",
      "1278 [D loss: 0.395348, acc.: 97.66%] [G loss: 3.288096]\n",
      "1279 [D loss: 0.526769, acc.: 96.88%] [G loss: 3.108807]\n",
      "1280 [D loss: 0.833407, acc.: 92.97%] [G loss: 2.587324]\n",
      "1281 [D loss: 0.579705, acc.: 96.88%] [G loss: 2.013008]\n",
      "1282 [D loss: 0.550780, acc.: 96.88%] [G loss: 2.452441]\n",
      "1283 [D loss: 0.307297, acc.: 97.66%] [G loss: 2.696465]\n",
      "1284 [D loss: 0.664527, acc.: 96.09%] [G loss: 2.273225]\n",
      "1285 [D loss: 0.806286, acc.: 94.53%] [G loss: 2.007056]\n",
      "1286 [D loss: 0.808790, acc.: 95.31%] [G loss: 2.229672]\n",
      "1287 [D loss: 0.283591, acc.: 98.44%] [G loss: 2.167510]\n",
      "1288 [D loss: 1.040807, acc.: 93.75%] [G loss: 2.106238]\n",
      "1289 [D loss: 0.429241, acc.: 97.66%] [G loss: 2.203429]\n",
      "1290 [D loss: 0.297387, acc.: 97.66%] [G loss: 2.262039]\n",
      "1291 [D loss: 0.687088, acc.: 96.09%] [G loss: 2.487958]\n",
      "1292 [D loss: 0.595458, acc.: 93.75%] [G loss: 3.367176]\n",
      "1293 [D loss: 0.527828, acc.: 96.88%] [G loss: 3.940068]\n",
      "1294 [D loss: 0.663435, acc.: 96.09%] [G loss: 3.155092]\n",
      "1295 [D loss: 0.412488, acc.: 97.66%] [G loss: 2.650801]\n",
      "1296 [D loss: 0.530715, acc.: 96.88%] [G loss: 2.404092]\n",
      "1297 [D loss: 0.419868, acc.: 97.66%] [G loss: 2.054525]\n",
      "1298 [D loss: 0.790609, acc.: 95.31%] [G loss: 2.216733]\n",
      "1299 [D loss: 0.407019, acc.: 97.66%] [G loss: 2.179990]\n",
      "1300 [D loss: 0.290501, acc.: 97.66%] [G loss: 2.153496]\n",
      "1301 [D loss: 1.434307, acc.: 91.41%] [G loss: 2.694709]\n",
      "1302 [D loss: 0.788110, acc.: 95.31%] [G loss: 3.299239]\n",
      "1303 [D loss: 0.680008, acc.: 94.53%] [G loss: 3.452914]\n",
      "1304 [D loss: 0.536790, acc.: 89.84%] [G loss: 3.222283]\n",
      "1305 [D loss: 0.385906, acc.: 82.03%] [G loss: 4.300547]\n",
      "1306 [D loss: 1.048576, acc.: 87.50%] [G loss: 5.457920]\n",
      "1307 [D loss: 0.773420, acc.: 95.31%] [G loss: 5.134820]\n",
      "1308 [D loss: 1.037785, acc.: 93.75%] [G loss: 4.178796]\n",
      "1309 [D loss: 0.522245, acc.: 96.09%] [G loss: 3.886106]\n",
      "1310 [D loss: 0.518801, acc.: 96.88%] [G loss: 3.312067]\n",
      "1311 [D loss: 0.780867, acc.: 95.31%] [G loss: 3.002825]\n",
      "1312 [D loss: 0.909217, acc.: 94.53%] [G loss: 2.658773]\n",
      "1313 [D loss: 0.781984, acc.: 95.31%] [G loss: 2.607415]\n",
      "1314 [D loss: 0.658929, acc.: 96.09%] [G loss: 2.795870]\n",
      "1315 [D loss: 0.672443, acc.: 93.75%] [G loss: 3.370594]\n",
      "1316 [D loss: 0.572332, acc.: 96.88%] [G loss: 3.288880]\n",
      "1317 [D loss: 0.645409, acc.: 96.09%] [G loss: 2.903078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318 [D loss: 0.306457, acc.: 96.09%] [G loss: 2.265678]\n",
      "1319 [D loss: 0.706143, acc.: 96.09%] [G loss: 2.701141]\n",
      "1320 [D loss: 0.411049, acc.: 96.88%] [G loss: 3.035279]\n",
      "1321 [D loss: 0.526697, acc.: 96.88%] [G loss: 2.759592]\n",
      "1322 [D loss: 0.656712, acc.: 96.09%] [G loss: 2.663887]\n",
      "1323 [D loss: 0.532746, acc.: 96.88%] [G loss: 2.559332]\n",
      "1324 [D loss: 0.543667, acc.: 96.88%] [G loss: 2.555196]\n",
      "1325 [D loss: 0.538899, acc.: 96.09%] [G loss: 2.457390]\n",
      "1326 [D loss: 1.051067, acc.: 93.75%] [G loss: 2.418341]\n",
      "1327 [D loss: 0.537883, acc.: 96.88%] [G loss: 2.362848]\n",
      "1328 [D loss: 0.680093, acc.: 96.09%] [G loss: 2.455827]\n",
      "1329 [D loss: 0.788184, acc.: 95.31%] [G loss: 2.477871]\n",
      "1330 [D loss: 0.405626, acc.: 97.66%] [G loss: 2.419411]\n",
      "1331 [D loss: 0.274673, acc.: 98.44%] [G loss: 2.675148]\n",
      "1332 [D loss: 0.520144, acc.: 96.88%] [G loss: 3.034246]\n",
      "1333 [D loss: 1.152559, acc.: 92.97%] [G loss: 2.655110]\n",
      "1334 [D loss: 0.535195, acc.: 96.88%] [G loss: 2.324469]\n",
      "1335 [D loss: 0.952355, acc.: 92.19%] [G loss: 2.376751]\n",
      "1336 [D loss: 0.573263, acc.: 94.53%] [G loss: 2.092019]\n",
      "1337 [D loss: 0.674199, acc.: 95.31%] [G loss: 2.575888]\n",
      "1338 [D loss: 0.393565, acc.: 97.66%] [G loss: 2.635301]\n",
      "1339 [D loss: 0.275134, acc.: 98.44%] [G loss: 2.460483]\n",
      "1340 [D loss: 1.152445, acc.: 92.97%] [G loss: 2.528538]\n",
      "1341 [D loss: 0.644799, acc.: 96.09%] [G loss: 2.873883]\n",
      "1342 [D loss: 0.265584, acc.: 98.44%] [G loss: 2.831385]\n",
      "1343 [D loss: 0.020358, acc.: 100.00%] [G loss: 2.635132]\n",
      "1344 [D loss: 0.537906, acc.: 96.88%] [G loss: 2.119371]\n",
      "1345 [D loss: 0.430435, acc.: 97.66%] [G loss: 2.098408]\n",
      "1346 [D loss: 0.552036, acc.: 95.31%] [G loss: 2.211336]\n",
      "1347 [D loss: 0.411577, acc.: 97.66%] [G loss: 2.321951]\n",
      "1348 [D loss: 0.150640, acc.: 99.22%] [G loss: 2.304622]\n",
      "1349 [D loss: 0.278483, acc.: 97.66%] [G loss: 2.585306]\n",
      "1350 [D loss: 0.391394, acc.: 97.66%] [G loss: 3.214942]\n",
      "1351 [D loss: 0.514007, acc.: 96.88%] [G loss: 3.307216]\n",
      "1352 [D loss: 0.300088, acc.: 97.66%] [G loss: 2.659928]\n",
      "1353 [D loss: 0.557125, acc.: 96.88%] [G loss: 2.127386]\n",
      "1354 [D loss: 0.605793, acc.: 96.09%] [G loss: 3.130421]\n",
      "1355 [D loss: 0.341126, acc.: 93.75%] [G loss: 3.489225]\n",
      "1356 [D loss: 0.512671, acc.: 96.88%] [G loss: 3.119068]\n",
      "1357 [D loss: 0.644455, acc.: 96.09%] [G loss: 2.704576]\n",
      "1358 [D loss: 0.517904, acc.: 96.88%] [G loss: 2.460987]\n",
      "1359 [D loss: 0.510179, acc.: 96.88%] [G loss: 1.748645]\n",
      "1360 [D loss: 0.059420, acc.: 100.00%] [G loss: 1.696047]\n",
      "1361 [D loss: 0.438341, acc.: 97.66%] [G loss: 1.828589]\n",
      "1362 [D loss: 0.593058, acc.: 96.88%] [G loss: 2.691164]\n",
      "1363 [D loss: 0.275566, acc.: 92.97%] [G loss: 3.405705]\n",
      "1364 [D loss: 0.394464, acc.: 81.25%] [G loss: 4.138005]\n",
      "1365 [D loss: 0.191489, acc.: 87.50%] [G loss: 5.300313]\n",
      "1366 [D loss: 0.132892, acc.: 99.22%] [G loss: 5.576077]\n",
      "1367 [D loss: 0.395101, acc.: 97.66%] [G loss: 4.953008]\n",
      "1368 [D loss: 0.389945, acc.: 97.66%] [G loss: 3.946699]\n",
      "1369 [D loss: 0.010451, acc.: 100.00%] [G loss: 3.255857]\n",
      "1370 [D loss: 0.143704, acc.: 99.22%] [G loss: 2.764999]\n",
      "1371 [D loss: 0.150956, acc.: 99.22%] [G loss: 2.518367]\n",
      "1372 [D loss: 0.033916, acc.: 100.00%] [G loss: 2.120687]\n",
      "1373 [D loss: 0.164765, acc.: 99.22%] [G loss: 2.051115]\n",
      "1374 [D loss: 0.556688, acc.: 96.88%] [G loss: 1.740875]\n",
      "1375 [D loss: 0.198305, acc.: 99.22%] [G loss: 2.072452]\n",
      "1376 [D loss: 0.557956, acc.: 96.88%] [G loss: 2.038471]\n",
      "1377 [D loss: 0.404843, acc.: 97.66%] [G loss: 2.183878]\n",
      "1378 [D loss: 0.415772, acc.: 97.66%] [G loss: 2.068806]\n",
      "1379 [D loss: 0.298301, acc.: 98.44%] [G loss: 2.064220]\n",
      "1380 [D loss: 0.073946, acc.: 100.00%] [G loss: 2.498088]\n",
      "1381 [D loss: 0.311308, acc.: 94.53%] [G loss: 2.574543]\n",
      "1382 [D loss: 0.075344, acc.: 100.00%] [G loss: 2.669462]\n",
      "1383 [D loss: 0.304627, acc.: 98.44%] [G loss: 2.668481]\n",
      "1384 [D loss: 0.171459, acc.: 99.22%] [G loss: 2.338366]\n",
      "1385 [D loss: 0.167730, acc.: 99.22%] [G loss: 2.354951]\n",
      "1386 [D loss: 0.159067, acc.: 99.22%] [G loss: 2.088343]\n",
      "1387 [D loss: 0.419697, acc.: 97.66%] [G loss: 1.988400]\n",
      "1388 [D loss: 0.431540, acc.: 97.66%] [G loss: 2.066099]\n",
      "1389 [D loss: 0.295683, acc.: 97.66%] [G loss: 2.283258]\n",
      "1390 [D loss: 0.917707, acc.: 94.53%] [G loss: 2.424492]\n",
      "1391 [D loss: 0.015838, acc.: 100.00%] [G loss: 2.830810]\n",
      "1392 [D loss: 0.168499, acc.: 99.22%] [G loss: 2.410755]\n",
      "1393 [D loss: 0.454364, acc.: 97.66%] [G loss: 2.537152]\n",
      "1394 [D loss: 0.319041, acc.: 96.09%] [G loss: 2.689176]\n",
      "1395 [D loss: 0.184586, acc.: 99.22%] [G loss: 2.591240]\n",
      "1396 [D loss: 0.304745, acc.: 97.66%] [G loss: 2.617595]\n",
      "1397 [D loss: 0.530665, acc.: 96.88%] [G loss: 2.789864]\n",
      "1398 [D loss: 0.394563, acc.: 97.66%] [G loss: 2.655001]\n",
      "1399 [D loss: 0.404127, acc.: 97.66%] [G loss: 2.589505]\n",
      "1400 [D loss: 0.401405, acc.: 97.66%] [G loss: 2.623633]\n",
      "1401 [D loss: 0.026812, acc.: 100.00%] [G loss: 2.453094]\n",
      "1402 [D loss: 0.293654, acc.: 98.44%] [G loss: 2.183619]\n",
      "1403 [D loss: 0.612044, acc.: 92.97%] [G loss: 2.339319]\n",
      "1404 [D loss: 0.560060, acc.: 85.16%] [G loss: 3.044204]\n",
      "1405 [D loss: 0.331872, acc.: 93.75%] [G loss: 3.696792]\n",
      "1406 [D loss: 0.138235, acc.: 99.22%] [G loss: 2.939915]\n",
      "1407 [D loss: 0.269584, acc.: 98.44%] [G loss: 2.706134]\n",
      "1408 [D loss: 0.522161, acc.: 96.88%] [G loss: 2.444106]\n",
      "1409 [D loss: 0.523265, acc.: 96.88%] [G loss: 2.408878]\n",
      "1410 [D loss: 0.152309, acc.: 99.22%] [G loss: 2.445852]\n",
      "1411 [D loss: 0.029694, acc.: 100.00%] [G loss: 2.263018]\n",
      "1412 [D loss: 0.156006, acc.: 99.22%] [G loss: 2.158059]\n",
      "1413 [D loss: 0.540956, acc.: 96.88%] [G loss: 1.444195]\n",
      "1414 [D loss: 0.257322, acc.: 95.31%] [G loss: 1.613900]\n",
      "1415 [D loss: 0.519721, acc.: 85.16%] [G loss: 2.673480]\n",
      "1416 [D loss: 0.978128, acc.: 77.34%] [G loss: 4.496059]\n",
      "1417 [D loss: 0.226293, acc.: 88.28%] [G loss: 7.080384]\n",
      "1418 [D loss: 0.259478, acc.: 98.44%] [G loss: 6.187807]\n",
      "1419 [D loss: 0.144641, acc.: 98.44%] [G loss: 4.927144]\n",
      "1420 [D loss: 0.262352, acc.: 98.44%] [G loss: 3.651888]\n",
      "1421 [D loss: 0.019044, acc.: 100.00%] [G loss: 2.864213]\n",
      "1422 [D loss: 0.151866, acc.: 99.22%] [G loss: 2.484754]\n",
      "1423 [D loss: 0.184478, acc.: 98.44%] [G loss: 2.288437]\n",
      "1424 [D loss: 0.176497, acc.: 99.22%] [G loss: 2.231557]\n",
      "1425 [D loss: 0.200437, acc.: 96.88%] [G loss: 1.877548]\n",
      "1426 [D loss: 0.560737, acc.: 96.88%] [G loss: 1.854924]\n",
      "1427 [D loss: 0.201716, acc.: 99.22%] [G loss: 2.274362]\n",
      "1428 [D loss: 0.380747, acc.: 94.53%] [G loss: 2.330512]\n",
      "1429 [D loss: 0.318985, acc.: 97.66%] [G loss: 2.433728]\n",
      "1430 [D loss: 0.292315, acc.: 98.44%] [G loss: 2.372428]\n",
      "1431 [D loss: 0.168166, acc.: 99.22%] [G loss: 2.102670]\n",
      "1432 [D loss: 0.047256, acc.: 100.00%] [G loss: 2.423828]\n",
      "1433 [D loss: 0.038086, acc.: 99.22%] [G loss: 2.061103]\n",
      "1434 [D loss: 0.055899, acc.: 100.00%] [G loss: 2.315369]\n",
      "1435 [D loss: 0.304443, acc.: 97.66%] [G loss: 2.402086]\n",
      "1436 [D loss: 0.179125, acc.: 99.22%] [G loss: 2.223119]\n",
      "1437 [D loss: 0.841300, acc.: 92.19%] [G loss: 2.024803]\n",
      "1438 [D loss: 0.402900, acc.: 92.97%] [G loss: 2.162506]\n",
      "1439 [D loss: 0.423196, acc.: 96.88%] [G loss: 2.536342]\n",
      "1440 [D loss: 0.164143, acc.: 99.22%] [G loss: 2.331983]\n",
      "1441 [D loss: 0.166612, acc.: 99.22%] [G loss: 2.191456]\n",
      "1442 [D loss: 0.027589, acc.: 100.00%] [G loss: 2.046677]\n",
      "1443 [D loss: 0.310700, acc.: 98.44%] [G loss: 2.380788]\n",
      "1444 [D loss: 0.546739, acc.: 96.09%] [G loss: 2.996558]\n",
      "1445 [D loss: 0.407809, acc.: 97.66%] [G loss: 3.132742]\n",
      "1446 [D loss: 0.168378, acc.: 99.22%] [G loss: 2.339907]\n",
      "1447 [D loss: 0.566128, acc.: 95.31%] [G loss: 1.862020]\n",
      "1448 [D loss: 0.068025, acc.: 99.22%] [G loss: 2.139262]\n",
      "1449 [D loss: 0.435392, acc.: 96.88%] [G loss: 2.313830]\n",
      "1450 [D loss: 0.553925, acc.: 96.09%] [G loss: 2.352683]\n",
      "1451 [D loss: 0.530721, acc.: 96.88%] [G loss: 2.446898]\n",
      "1452 [D loss: 0.280946, acc.: 98.44%] [G loss: 2.592764]\n",
      "1453 [D loss: 0.399535, acc.: 97.66%] [G loss: 2.681707]\n",
      "1454 [D loss: 0.334802, acc.: 95.31%] [G loss: 2.184292]\n",
      "1455 [D loss: 0.397414, acc.: 94.53%] [G loss: 3.215701]\n",
      "1456 [D loss: 0.355922, acc.: 91.41%] [G loss: 3.843906]\n",
      "1457 [D loss: 0.750549, acc.: 92.19%] [G loss: 3.115435]\n",
      "1458 [D loss: 0.567603, acc.: 96.09%] [G loss: 3.054755]\n",
      "1459 [D loss: 0.525219, acc.: 96.88%] [G loss: 2.997644]\n",
      "1460 [D loss: 0.393114, acc.: 97.66%] [G loss: 2.684719]\n",
      "1461 [D loss: 0.018031, acc.: 100.00%] [G loss: 2.445776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462 [D loss: 0.529895, acc.: 96.88%] [G loss: 2.273670]\n",
      "1463 [D loss: 0.274710, acc.: 98.44%] [G loss: 2.378859]\n",
      "1464 [D loss: 0.905121, acc.: 94.53%] [G loss: 2.759509]\n",
      "1465 [D loss: 0.146509, acc.: 99.22%] [G loss: 2.401378]\n",
      "1466 [D loss: 0.175577, acc.: 98.44%] [G loss: 1.986899]\n",
      "1467 [D loss: 0.178748, acc.: 98.44%] [G loss: 1.762707]\n",
      "1468 [D loss: 0.299425, acc.: 98.44%] [G loss: 1.916091]\n",
      "1469 [D loss: 0.424367, acc.: 97.66%] [G loss: 2.159440]\n",
      "1470 [D loss: 0.282654, acc.: 98.44%] [G loss: 2.642831]\n",
      "1471 [D loss: 0.520044, acc.: 96.88%] [G loss: 2.932089]\n",
      "1472 [D loss: 0.521857, acc.: 96.88%] [G loss: 2.829624]\n",
      "1473 [D loss: 0.169797, acc.: 99.22%] [G loss: 1.996019]\n",
      "1474 [D loss: 0.438676, acc.: 97.66%] [G loss: 2.088204]\n",
      "1475 [D loss: 0.695155, acc.: 93.75%] [G loss: 2.047385]\n",
      "1476 [D loss: 0.202417, acc.: 99.22%] [G loss: 2.575163]\n",
      "1477 [D loss: 0.265127, acc.: 98.44%] [G loss: 2.862114]\n",
      "1478 [D loss: 0.519817, acc.: 96.88%] [G loss: 2.728096]\n",
      "1479 [D loss: 0.513751, acc.: 96.88%] [G loss: 1.949912]\n",
      "1480 [D loss: 0.218102, acc.: 96.88%] [G loss: 1.886057]\n",
      "1481 [D loss: 0.172524, acc.: 93.75%] [G loss: 2.764810]\n",
      "1482 [D loss: 0.403870, acc.: 88.28%] [G loss: 3.536168]\n",
      "1483 [D loss: 0.706616, acc.: 80.47%] [G loss: 5.044436]\n",
      "1484 [D loss: 0.301103, acc.: 96.88%] [G loss: 6.043066]\n",
      "1485 [D loss: 0.018333, acc.: 99.22%] [G loss: 4.909480]\n",
      "1486 [D loss: 0.011870, acc.: 100.00%] [G loss: 4.098505]\n",
      "1487 [D loss: 0.267657, acc.: 98.44%] [G loss: 3.434878]\n",
      "1488 [D loss: 0.143657, acc.: 99.22%] [G loss: 2.897958]\n",
      "1489 [D loss: 0.160486, acc.: 99.22%] [G loss: 2.568964]\n",
      "1490 [D loss: 0.293851, acc.: 98.44%] [G loss: 2.096316]\n",
      "1491 [D loss: 0.315457, acc.: 98.44%] [G loss: 2.021556]\n",
      "1492 [D loss: 0.424575, acc.: 97.66%] [G loss: 2.189275]\n",
      "1493 [D loss: 0.434915, acc.: 97.66%] [G loss: 1.938865]\n",
      "1494 [D loss: 0.188653, acc.: 99.22%] [G loss: 2.107798]\n",
      "1495 [D loss: 0.160085, acc.: 99.22%] [G loss: 2.090650]\n",
      "1496 [D loss: 0.301272, acc.: 98.44%] [G loss: 2.065984]\n",
      "1497 [D loss: 0.313890, acc.: 98.44%] [G loss: 2.009025]\n",
      "1498 [D loss: 0.204810, acc.: 99.22%] [G loss: 2.226222]\n",
      "1499 [D loss: 0.341556, acc.: 95.31%] [G loss: 2.268712]\n",
      "1500 [D loss: 0.344542, acc.: 98.44%] [G loss: 2.048428]\n",
      "1501 [D loss: 0.427450, acc.: 97.66%] [G loss: 2.310588]\n",
      "1502 [D loss: 0.285046, acc.: 98.44%] [G loss: 2.161180]\n",
      "1503 [D loss: 0.424917, acc.: 97.66%] [G loss: 2.193168]\n",
      "1504 [D loss: 0.315991, acc.: 97.66%] [G loss: 2.009472]\n",
      "1505 [D loss: 0.063937, acc.: 97.66%] [G loss: 2.085101]\n",
      "1506 [D loss: 0.182546, acc.: 99.22%] [G loss: 2.225722]\n",
      "1507 [D loss: 0.163841, acc.: 99.22%] [G loss: 2.467102]\n",
      "1508 [D loss: 0.031675, acc.: 100.00%] [G loss: 2.519315]\n",
      "1509 [D loss: 0.296872, acc.: 98.44%] [G loss: 2.084212]\n",
      "1510 [D loss: 0.462571, acc.: 94.53%] [G loss: 1.684917]\n",
      "1511 [D loss: 0.238829, acc.: 97.66%] [G loss: 2.434881]\n",
      "1512 [D loss: 0.486705, acc.: 92.97%] [G loss: 2.526042]\n",
      "1513 [D loss: 0.470594, acc.: 96.09%] [G loss: 2.910325]\n",
      "1514 [D loss: 0.032204, acc.: 100.00%] [G loss: 2.940589]\n",
      "1515 [D loss: 0.284763, acc.: 98.44%] [G loss: 2.630756]\n",
      "1516 [D loss: 0.018550, acc.: 100.00%] [G loss: 2.412802]\n",
      "1517 [D loss: 0.404429, acc.: 97.66%] [G loss: 2.409712]\n",
      "1518 [D loss: 0.163006, acc.: 99.22%] [G loss: 2.101405]\n",
      "1519 [D loss: 0.682410, acc.: 95.31%] [G loss: 1.886823]\n",
      "1520 [D loss: 0.296350, acc.: 98.44%] [G loss: 1.946191]\n",
      "1521 [D loss: 0.163657, acc.: 98.44%] [G loss: 1.872735]\n",
      "1522 [D loss: 0.043229, acc.: 100.00%] [G loss: 1.945044]\n",
      "1523 [D loss: 0.605625, acc.: 92.97%] [G loss: 1.511369]\n",
      "1524 [D loss: 0.424721, acc.: 85.16%] [G loss: 2.381421]\n",
      "1525 [D loss: 0.683210, acc.: 84.38%] [G loss: 3.415801]\n",
      "1526 [D loss: 0.311283, acc.: 96.09%] [G loss: 4.411319]\n",
      "1527 [D loss: 0.229972, acc.: 92.97%] [G loss: 3.919406]\n",
      "1528 [D loss: 0.553456, acc.: 96.09%] [G loss: 3.665814]\n",
      "1529 [D loss: 0.027172, acc.: 100.00%] [G loss: 2.930854]\n",
      "1530 [D loss: 0.294167, acc.: 98.44%] [G loss: 2.281991]\n",
      "1531 [D loss: 0.042009, acc.: 100.00%] [G loss: 2.136674]\n",
      "1532 [D loss: 0.187309, acc.: 99.22%] [G loss: 2.188802]\n",
      "1533 [D loss: 0.297607, acc.: 98.44%] [G loss: 1.915400]\n",
      "1534 [D loss: 0.060563, acc.: 100.00%] [G loss: 1.940218]\n",
      "1535 [D loss: 0.319402, acc.: 96.88%] [G loss: 1.735045]\n",
      "1536 [D loss: 0.088491, acc.: 98.44%] [G loss: 1.869361]\n",
      "1537 [D loss: 0.074411, acc.: 99.22%] [G loss: 1.991934]\n",
      "1538 [D loss: 0.060370, acc.: 100.00%] [G loss: 2.139720]\n",
      "1539 [D loss: 0.063665, acc.: 98.44%] [G loss: 2.251022]\n",
      "1540 [D loss: 0.046623, acc.: 100.00%] [G loss: 2.300224]\n",
      "1541 [D loss: 0.185049, acc.: 98.44%] [G loss: 2.244193]\n",
      "1542 [D loss: 0.321339, acc.: 96.09%] [G loss: 1.963779]\n",
      "1543 [D loss: 0.393360, acc.: 93.75%] [G loss: 2.377032]\n",
      "1544 [D loss: 0.362147, acc.: 86.72%] [G loss: 3.251466]\n",
      "1545 [D loss: 0.148116, acc.: 99.22%] [G loss: 3.271887]\n",
      "1546 [D loss: 0.149733, acc.: 99.22%] [G loss: 2.761612]\n",
      "1547 [D loss: 0.289245, acc.: 98.44%] [G loss: 2.497690]\n",
      "1548 [D loss: 0.177122, acc.: 99.22%] [G loss: 2.275297]\n",
      "1549 [D loss: 0.183578, acc.: 99.22%] [G loss: 2.105817]\n",
      "1550 [D loss: 0.191899, acc.: 96.88%] [G loss: 1.897787]\n",
      "1551 [D loss: 0.451659, acc.: 97.66%] [G loss: 1.883354]\n",
      "1552 [D loss: 0.048710, acc.: 100.00%] [G loss: 1.978680]\n",
      "1553 [D loss: 0.443068, acc.: 96.09%] [G loss: 2.239555]\n",
      "1554 [D loss: 0.074394, acc.: 100.00%] [G loss: 2.759558]\n",
      "1555 [D loss: 0.156095, acc.: 99.22%] [G loss: 2.908223]\n",
      "1556 [D loss: 0.046045, acc.: 99.22%] [G loss: 2.436879]\n",
      "1557 [D loss: 0.300114, acc.: 98.44%] [G loss: 2.017013]\n",
      "1558 [D loss: 0.193891, acc.: 97.66%] [G loss: 1.951565]\n",
      "1559 [D loss: 0.179754, acc.: 99.22%] [G loss: 1.937214]\n",
      "1560 [D loss: 1.174873, acc.: 92.97%] [G loss: 1.980629]\n",
      "1561 [D loss: 0.299963, acc.: 97.66%] [G loss: 1.831166]\n",
      "1562 [D loss: 0.178935, acc.: 99.22%] [G loss: 1.780714]\n",
      "1563 [D loss: 0.567390, acc.: 96.88%] [G loss: 2.104472]\n",
      "1564 [D loss: 0.413756, acc.: 97.66%] [G loss: 2.159924]\n",
      "1565 [D loss: 0.161131, acc.: 98.44%] [G loss: 1.973346]\n",
      "1566 [D loss: 0.206919, acc.: 99.22%] [G loss: 2.231095]\n",
      "1567 [D loss: 0.198214, acc.: 97.66%] [G loss: 2.345461]\n",
      "1568 [D loss: 0.534749, acc.: 96.88%] [G loss: 2.294762]\n",
      "1569 [D loss: 0.530320, acc.: 96.88%] [G loss: 2.387501]\n",
      "1570 [D loss: 0.152659, acc.: 99.22%] [G loss: 2.496940]\n",
      "1571 [D loss: 0.159772, acc.: 99.22%] [G loss: 2.447503]\n",
      "1572 [D loss: 0.913528, acc.: 93.75%] [G loss: 2.093003]\n",
      "1573 [D loss: 0.172456, acc.: 99.22%] [G loss: 1.895777]\n",
      "1574 [D loss: 0.045633, acc.: 99.22%] [G loss: 2.344964]\n",
      "1575 [D loss: 0.158529, acc.: 99.22%] [G loss: 2.891991]\n",
      "1576 [D loss: 0.276846, acc.: 98.44%] [G loss: 2.738352]\n",
      "1577 [D loss: 0.543753, acc.: 96.09%] [G loss: 2.219223]\n",
      "1578 [D loss: 0.669065, acc.: 96.09%] [G loss: 1.868324]\n",
      "1579 [D loss: 0.808011, acc.: 92.97%] [G loss: 1.895776]\n",
      "1580 [D loss: 0.317075, acc.: 97.66%] [G loss: 2.166221]\n",
      "1581 [D loss: 0.404315, acc.: 97.66%] [G loss: 2.371073]\n",
      "1582 [D loss: 0.271236, acc.: 98.44%] [G loss: 2.497482]\n",
      "1583 [D loss: 0.395171, acc.: 97.66%] [G loss: 2.398643]\n",
      "1584 [D loss: 0.146338, acc.: 99.22%] [G loss: 2.663238]\n",
      "1585 [D loss: 0.279762, acc.: 98.44%] [G loss: 2.572428]\n",
      "1586 [D loss: 0.297417, acc.: 96.88%] [G loss: 2.169012]\n",
      "1587 [D loss: 0.806249, acc.: 95.31%] [G loss: 2.176669]\n",
      "1588 [D loss: 0.053367, acc.: 98.44%] [G loss: 2.263703]\n",
      "1589 [D loss: 0.667261, acc.: 96.09%] [G loss: 2.180130]\n",
      "1590 [D loss: 0.283050, acc.: 98.44%] [G loss: 2.496387]\n",
      "1591 [D loss: 0.392514, acc.: 97.66%] [G loss: 2.794002]\n",
      "1592 [D loss: 0.396125, acc.: 97.66%] [G loss: 2.718946]\n",
      "1593 [D loss: 0.273467, acc.: 98.44%] [G loss: 2.425913]\n",
      "1594 [D loss: 0.282381, acc.: 98.44%] [G loss: 2.150178]\n",
      "1595 [D loss: 0.034498, acc.: 100.00%] [G loss: 2.120828]\n",
      "1596 [D loss: 0.541703, acc.: 96.88%] [G loss: 2.055277]\n",
      "1597 [D loss: 0.414272, acc.: 97.66%] [G loss: 2.118855]\n",
      "1598 [D loss: 0.659939, acc.: 96.09%] [G loss: 2.268583]\n",
      "1599 [D loss: 0.406131, acc.: 97.66%] [G loss: 2.158638]\n",
      "1600 [D loss: 0.397611, acc.: 97.66%] [G loss: 2.398144]\n",
      "1601 [D loss: 0.652575, acc.: 96.09%] [G loss: 2.281089]\n",
      "1602 [D loss: 0.147318, acc.: 99.22%] [G loss: 2.557520]\n",
      "1603 [D loss: 0.273336, acc.: 98.44%] [G loss: 2.465506]\n",
      "1604 [D loss: 0.291687, acc.: 97.66%] [G loss: 2.542370]\n",
      "1605 [D loss: 0.412243, acc.: 97.66%] [G loss: 2.305383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606 [D loss: 0.419592, acc.: 97.66%] [G loss: 2.192326]\n",
      "1607 [D loss: 0.174893, acc.: 97.66%] [G loss: 2.174645]\n",
      "1608 [D loss: 0.413877, acc.: 97.66%] [G loss: 2.391772]\n",
      "1609 [D loss: 0.774594, acc.: 95.31%] [G loss: 2.756195]\n",
      "1610 [D loss: 0.269194, acc.: 98.44%] [G loss: 2.681375]\n",
      "1611 [D loss: 0.277330, acc.: 97.66%] [G loss: 2.363171]\n",
      "1612 [D loss: 0.293857, acc.: 96.88%] [G loss: 2.320290]\n",
      "1613 [D loss: 0.357303, acc.: 94.53%] [G loss: 3.166112]\n",
      "1614 [D loss: 0.530477, acc.: 92.19%] [G loss: 3.946634]\n",
      "1615 [D loss: 0.129715, acc.: 99.22%] [G loss: 3.946002]\n",
      "1616 [D loss: 0.508758, acc.: 96.88%] [G loss: 3.500035]\n",
      "1617 [D loss: 0.386290, acc.: 97.66%] [G loss: 3.255261]\n",
      "1618 [D loss: 0.135147, acc.: 99.22%] [G loss: 3.022760]\n",
      "1619 [D loss: 0.674986, acc.: 95.31%] [G loss: 2.183350]\n",
      "1620 [D loss: 0.159427, acc.: 99.22%] [G loss: 1.979748]\n",
      "1621 [D loss: 0.060520, acc.: 97.66%] [G loss: 1.971104]\n",
      "1622 [D loss: 0.706482, acc.: 95.31%] [G loss: 2.339964]\n",
      "1623 [D loss: 0.054594, acc.: 99.22%] [G loss: 2.700011]\n",
      "1624 [D loss: 0.653643, acc.: 96.09%] [G loss: 2.529357]\n",
      "1625 [D loss: 0.041462, acc.: 99.22%] [G loss: 2.402520]\n",
      "1626 [D loss: 0.404320, acc.: 97.66%] [G loss: 2.335816]\n",
      "1627 [D loss: 0.149927, acc.: 99.22%] [G loss: 2.475936]\n",
      "1628 [D loss: 0.145760, acc.: 99.22%] [G loss: 2.457795]\n",
      "1629 [D loss: 0.025775, acc.: 100.00%] [G loss: 2.370640]\n",
      "1630 [D loss: 0.052142, acc.: 99.22%] [G loss: 2.258473]\n",
      "1631 [D loss: 0.028296, acc.: 100.00%] [G loss: 2.395779]\n",
      "1632 [D loss: 0.711870, acc.: 92.97%] [G loss: 2.360159]\n",
      "1633 [D loss: 0.745404, acc.: 95.31%] [G loss: 2.923932]\n",
      "1634 [D loss: 0.552862, acc.: 95.31%] [G loss: 3.038538]\n",
      "1635 [D loss: 0.275114, acc.: 98.44%] [G loss: 2.719849]\n",
      "1636 [D loss: 0.278357, acc.: 98.44%] [G loss: 2.430111]\n",
      "1637 [D loss: 0.279117, acc.: 98.44%] [G loss: 2.333977]\n",
      "1638 [D loss: 0.288708, acc.: 98.44%] [G loss: 2.385500]\n",
      "1639 [D loss: 0.398049, acc.: 97.66%] [G loss: 2.759233]\n",
      "1640 [D loss: 0.136528, acc.: 99.22%] [G loss: 3.142374]\n",
      "1641 [D loss: 0.269923, acc.: 98.44%] [G loss: 2.982130]\n",
      "1642 [D loss: 0.056878, acc.: 98.44%] [G loss: 2.636901]\n",
      "1643 [D loss: 0.069389, acc.: 100.00%] [G loss: 2.375209]\n",
      "1644 [D loss: 0.325221, acc.: 95.31%] [G loss: 2.527002]\n",
      "1645 [D loss: 0.431464, acc.: 91.41%] [G loss: 3.887636]\n",
      "1646 [D loss: 0.667590, acc.: 87.50%] [G loss: 4.442232]\n",
      "1647 [D loss: 0.923287, acc.: 82.81%] [G loss: 5.208879]\n",
      "1648 [D loss: 0.555251, acc.: 88.28%] [G loss: 6.524453]\n",
      "1649 [D loss: 0.132913, acc.: 99.22%] [G loss: 6.562950]\n",
      "1650 [D loss: 0.634874, acc.: 96.09%] [G loss: 5.016590]\n",
      "1651 [D loss: 0.383722, acc.: 97.66%] [G loss: 4.014666]\n",
      "1652 [D loss: 0.144320, acc.: 99.22%] [G loss: 3.616577]\n",
      "1653 [D loss: 0.647398, acc.: 96.09%] [G loss: 3.091437]\n",
      "1654 [D loss: 0.276431, acc.: 98.44%] [G loss: 2.916484]\n",
      "1655 [D loss: 0.029159, acc.: 100.00%] [G loss: 2.697568]\n",
      "1656 [D loss: 0.158548, acc.: 99.22%] [G loss: 2.499399]\n",
      "1657 [D loss: 0.307242, acc.: 98.44%] [G loss: 2.561296]\n",
      "1658 [D loss: 0.527540, acc.: 96.88%] [G loss: 2.572979]\n",
      "1659 [D loss: 0.422005, acc.: 96.88%] [G loss: 2.126580]\n",
      "1660 [D loss: 0.797483, acc.: 95.31%] [G loss: 2.202193]\n",
      "1661 [D loss: 0.413441, acc.: 97.66%] [G loss: 2.611900]\n",
      "1662 [D loss: 0.039464, acc.: 99.22%] [G loss: 2.654741]\n",
      "1663 [D loss: 0.152911, acc.: 99.22%] [G loss: 2.506598]\n",
      "1664 [D loss: 0.414996, acc.: 97.66%] [G loss: 2.245762]\n",
      "1665 [D loss: 0.299510, acc.: 98.44%] [G loss: 2.101822]\n",
      "1666 [D loss: 0.285425, acc.: 98.44%] [G loss: 2.074324]\n",
      "1667 [D loss: 0.285402, acc.: 98.44%] [G loss: 2.116415]\n",
      "1668 [D loss: 0.413283, acc.: 97.66%] [G loss: 2.140776]\n",
      "1669 [D loss: 0.405613, acc.: 97.66%] [G loss: 2.254027]\n",
      "1670 [D loss: 0.401484, acc.: 97.66%] [G loss: 2.504380]\n",
      "1671 [D loss: 0.141942, acc.: 99.22%] [G loss: 2.968580]\n",
      "1672 [D loss: 0.278737, acc.: 97.66%] [G loss: 2.793119]\n",
      "1673 [D loss: 0.284909, acc.: 98.44%] [G loss: 2.179897]\n",
      "1674 [D loss: 0.302996, acc.: 97.66%] [G loss: 1.786291]\n",
      "1675 [D loss: 0.678626, acc.: 96.09%] [G loss: 2.175823]\n",
      "1676 [D loss: 0.273068, acc.: 98.44%] [G loss: 2.619917]\n",
      "1677 [D loss: 0.654609, acc.: 96.09%] [G loss: 2.672674]\n",
      "1678 [D loss: 0.898647, acc.: 94.53%] [G loss: 2.763941]\n",
      "1679 [D loss: 0.052141, acc.: 97.66%] [G loss: 2.551729]\n",
      "1680 [D loss: 0.377520, acc.: 93.75%] [G loss: 3.876448]\n",
      "1681 [D loss: 0.798773, acc.: 92.97%] [G loss: 4.359430]\n",
      "1682 [D loss: 0.539865, acc.: 95.31%] [G loss: 3.812191]\n",
      "1683 [D loss: 0.264848, acc.: 98.44%] [G loss: 3.756725]\n",
      "1684 [D loss: 0.264010, acc.: 98.44%] [G loss: 3.401681]\n",
      "1685 [D loss: 0.055705, acc.: 99.22%] [G loss: 1.724519]\n",
      "1686 [D loss: 0.224827, acc.: 91.41%] [G loss: 1.882730]\n",
      "1687 [D loss: 0.185025, acc.: 92.19%] [G loss: 3.872786]\n",
      "1688 [D loss: 0.394329, acc.: 91.41%] [G loss: 4.374115]\n",
      "1689 [D loss: 0.361801, acc.: 85.16%] [G loss: 4.553427]\n",
      "1690 [D loss: 0.221810, acc.: 92.19%] [G loss: 5.239351]\n",
      "1691 [D loss: 0.022699, acc.: 99.22%] [G loss: 5.248126]\n",
      "1692 [D loss: 0.056098, acc.: 96.88%] [G loss: 3.903813]\n",
      "1693 [D loss: 0.310914, acc.: 98.44%] [G loss: 3.110829]\n",
      "1694 [D loss: 0.045580, acc.: 100.00%] [G loss: 2.577582]\n",
      "1695 [D loss: 0.136259, acc.: 95.31%] [G loss: 2.831551]\n",
      "1696 [D loss: 0.096637, acc.: 97.66%] [G loss: 2.830053]\n",
      "1697 [D loss: 0.113152, acc.: 97.66%] [G loss: 2.469125]\n",
      "1698 [D loss: 0.212922, acc.: 89.06%] [G loss: 3.360097]\n",
      "1699 [D loss: 0.128751, acc.: 95.31%] [G loss: 3.126886]\n",
      "1700 [D loss: 0.251451, acc.: 86.72%] [G loss: 3.102628]\n",
      "1701 [D loss: 0.284271, acc.: 93.75%] [G loss: 3.251584]\n",
      "1702 [D loss: 0.339088, acc.: 84.38%] [G loss: 3.848492]\n",
      "1703 [D loss: 0.294237, acc.: 89.84%] [G loss: 3.805377]\n",
      "1704 [D loss: 0.175948, acc.: 91.41%] [G loss: 3.123040]\n",
      "1705 [D loss: 0.388188, acc.: 81.25%] [G loss: 3.041956]\n",
      "1706 [D loss: 0.160858, acc.: 92.19%] [G loss: 3.476001]\n",
      "1707 [D loss: 0.085120, acc.: 97.66%] [G loss: 3.109280]\n",
      "1708 [D loss: 0.248850, acc.: 85.94%] [G loss: 3.063026]\n",
      "1709 [D loss: 0.223534, acc.: 89.06%] [G loss: 3.355118]\n",
      "1710 [D loss: 0.206016, acc.: 91.41%] [G loss: 3.475829]\n",
      "1711 [D loss: 0.553282, acc.: 85.16%] [G loss: 3.837974]\n",
      "1712 [D loss: 0.284068, acc.: 92.19%] [G loss: 3.376158]\n",
      "1713 [D loss: 0.173505, acc.: 92.19%] [G loss: 3.625732]\n",
      "1714 [D loss: 0.316470, acc.: 89.06%] [G loss: 3.311054]\n",
      "1715 [D loss: 0.407409, acc.: 91.41%] [G loss: 3.627571]\n",
      "1716 [D loss: 0.366993, acc.: 93.75%] [G loss: 4.068512]\n",
      "1717 [D loss: 0.565058, acc.: 86.72%] [G loss: 4.293734]\n",
      "1718 [D loss: 0.303177, acc.: 98.44%] [G loss: 3.948202]\n",
      "1719 [D loss: 0.472250, acc.: 88.28%] [G loss: 4.231157]\n",
      "1720 [D loss: 0.305638, acc.: 97.66%] [G loss: 3.795582]\n",
      "1721 [D loss: 0.408857, acc.: 84.38%] [G loss: 4.545349]\n",
      "1722 [D loss: 0.337319, acc.: 94.53%] [G loss: 4.716408]\n",
      "1723 [D loss: 0.230419, acc.: 95.31%] [G loss: 4.120580]\n",
      "1724 [D loss: 0.385621, acc.: 88.28%] [G loss: 4.199871]\n",
      "1725 [D loss: 0.347608, acc.: 94.53%] [G loss: 3.900524]\n",
      "1726 [D loss: 0.465643, acc.: 86.72%] [G loss: 4.462073]\n",
      "1727 [D loss: 0.585397, acc.: 93.75%] [G loss: 3.931880]\n",
      "1728 [D loss: 0.618893, acc.: 87.50%] [G loss: 4.831462]\n",
      "1729 [D loss: 0.580454, acc.: 94.53%] [G loss: 4.452091]\n",
      "1730 [D loss: 0.539601, acc.: 90.62%] [G loss: 4.308885]\n",
      "1731 [D loss: 0.221668, acc.: 96.88%] [G loss: 3.946313]\n",
      "1732 [D loss: 0.634382, acc.: 85.16%] [G loss: 5.001776]\n",
      "1733 [D loss: 0.290004, acc.: 96.88%] [G loss: 5.304142]\n",
      "1734 [D loss: 0.923659, acc.: 92.19%] [G loss: 3.113616]\n",
      "1735 [D loss: 0.492034, acc.: 84.38%] [G loss: 4.971992]\n",
      "1736 [D loss: 0.427336, acc.: 93.75%] [G loss: 4.753570]\n",
      "1737 [D loss: 0.077103, acc.: 99.22%] [G loss: 3.971318]\n",
      "1738 [D loss: 0.447779, acc.: 96.09%] [G loss: 3.442558]\n",
      "1739 [D loss: 0.331244, acc.: 96.09%] [G loss: 3.228547]\n",
      "1740 [D loss: 0.346243, acc.: 96.09%] [G loss: 3.451804]\n",
      "1741 [D loss: 0.200443, acc.: 97.66%] [G loss: 3.333075]\n",
      "1742 [D loss: 0.338232, acc.: 96.88%] [G loss: 3.514980]\n",
      "1743 [D loss: 0.352119, acc.: 95.31%] [G loss: 3.997527]\n",
      "1744 [D loss: 0.513930, acc.: 92.19%] [G loss: 3.806655]\n",
      "1745 [D loss: 0.445056, acc.: 86.72%] [G loss: 4.684990]\n",
      "1746 [D loss: 0.271789, acc.: 97.66%] [G loss: 5.052907]\n",
      "1747 [D loss: 0.578747, acc.: 92.97%] [G loss: 4.419571]\n",
      "1748 [D loss: 0.469082, acc.: 94.53%] [G loss: 3.843928]\n",
      "1749 [D loss: 0.200583, acc.: 96.88%] [G loss: 3.316708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750 [D loss: 0.508857, acc.: 91.41%] [G loss: 4.054994]\n",
      "1751 [D loss: 0.421031, acc.: 96.09%] [G loss: 4.267258]\n",
      "1752 [D loss: 0.422333, acc.: 96.88%] [G loss: 3.560429]\n",
      "1753 [D loss: 0.065743, acc.: 97.66%] [G loss: 3.502293]\n",
      "1754 [D loss: 0.454419, acc.: 94.53%] [G loss: 3.928654]\n",
      "1755 [D loss: 1.094331, acc.: 89.84%] [G loss: 3.772790]\n",
      "1756 [D loss: 0.138659, acc.: 92.97%] [G loss: 4.661285]\n",
      "1757 [D loss: 0.427256, acc.: 97.66%] [G loss: 4.802042]\n",
      "1758 [D loss: 0.263910, acc.: 98.44%] [G loss: 4.247717]\n",
      "1759 [D loss: 0.665193, acc.: 95.31%] [G loss: 3.531747]\n",
      "1760 [D loss: 0.196285, acc.: 97.66%] [G loss: 3.470261]\n",
      "1761 [D loss: 0.449563, acc.: 94.53%] [G loss: 3.501256]\n",
      "1762 [D loss: 0.461009, acc.: 89.06%] [G loss: 4.496755]\n",
      "1763 [D loss: 0.645653, acc.: 96.09%] [G loss: 5.009861]\n",
      "1764 [D loss: 0.451648, acc.: 95.31%] [G loss: 4.780977]\n",
      "1765 [D loss: 0.642447, acc.: 92.19%] [G loss: 4.617756]\n",
      "1766 [D loss: 0.271054, acc.: 98.44%] [G loss: 4.810607]\n",
      "1767 [D loss: 0.018823, acc.: 100.00%] [G loss: 3.885236]\n",
      "1768 [D loss: 0.434052, acc.: 96.88%] [G loss: 3.999011]\n",
      "1769 [D loss: 0.432865, acc.: 96.09%] [G loss: 3.949885]\n",
      "1770 [D loss: 0.309244, acc.: 95.31%] [G loss: 3.643153]\n",
      "1771 [D loss: 0.299354, acc.: 96.88%] [G loss: 3.351062]\n",
      "1772 [D loss: 0.474897, acc.: 94.53%] [G loss: 3.474401]\n",
      "1773 [D loss: 0.862134, acc.: 82.81%] [G loss: 2.330762]\n",
      "1774 [D loss: 0.458103, acc.: 89.06%] [G loss: 3.431427]\n",
      "1775 [D loss: 1.561850, acc.: 69.53%] [G loss: 7.883288]\n",
      "1776 [D loss: 2.194815, acc.: 82.03%] [G loss: 8.969774]\n",
      "1777 [D loss: 0.155951, acc.: 98.44%] [G loss: 7.516273]\n",
      "1778 [D loss: 0.014868, acc.: 99.22%] [G loss: 5.396212]\n",
      "1779 [D loss: 0.040570, acc.: 99.22%] [G loss: 3.675829]\n",
      "1780 [D loss: 0.422475, acc.: 87.50%] [G loss: 4.627036]\n",
      "1781 [D loss: 0.431780, acc.: 82.03%] [G loss: 4.966062]\n",
      "1782 [D loss: 0.279893, acc.: 95.31%] [G loss: 4.114026]\n",
      "1783 [D loss: 0.264981, acc.: 89.84%] [G loss: 4.075455]\n",
      "1784 [D loss: 0.200752, acc.: 92.19%] [G loss: 4.026774]\n",
      "1785 [D loss: 0.403542, acc.: 81.25%] [G loss: 4.301849]\n",
      "1786 [D loss: 0.288473, acc.: 92.19%] [G loss: 3.359406]\n",
      "1787 [D loss: 0.587448, acc.: 74.22%] [G loss: 5.075811]\n",
      "1788 [D loss: 0.201267, acc.: 95.31%] [G loss: 5.430753]\n",
      "1789 [D loss: 0.175913, acc.: 92.19%] [G loss: 3.706574]\n",
      "1790 [D loss: 0.616675, acc.: 76.56%] [G loss: 4.250122]\n",
      "1791 [D loss: 0.572463, acc.: 79.69%] [G loss: 4.685599]\n",
      "1792 [D loss: 0.564993, acc.: 80.47%] [G loss: 4.656636]\n",
      "1793 [D loss: 0.393902, acc.: 85.94%] [G loss: 5.237810]\n",
      "1794 [D loss: 0.227906, acc.: 96.09%] [G loss: 3.977009]\n",
      "1795 [D loss: 0.503304, acc.: 82.03%] [G loss: 4.067177]\n",
      "1796 [D loss: 0.238967, acc.: 89.06%] [G loss: 4.444584]\n",
      "1797 [D loss: 0.461023, acc.: 83.59%] [G loss: 4.049117]\n",
      "1798 [D loss: 0.218408, acc.: 89.84%] [G loss: 4.662631]\n",
      "1799 [D loss: 0.249703, acc.: 90.62%] [G loss: 4.025259]\n",
      "1800 [D loss: 0.631156, acc.: 82.03%] [G loss: 4.254099]\n",
      "1801 [D loss: 0.292244, acc.: 84.38%] [G loss: 4.830932]\n",
      "1802 [D loss: 0.542467, acc.: 83.59%] [G loss: 5.265291]\n",
      "1803 [D loss: 0.384808, acc.: 91.41%] [G loss: 4.967787]\n",
      "1804 [D loss: 0.274536, acc.: 92.97%] [G loss: 3.920623]\n",
      "1805 [D loss: 0.425599, acc.: 85.16%] [G loss: 3.926430]\n",
      "1806 [D loss: 0.200217, acc.: 90.62%] [G loss: 4.023299]\n",
      "1807 [D loss: 0.570420, acc.: 87.50%] [G loss: 4.599800]\n",
      "1808 [D loss: 0.213424, acc.: 96.09%] [G loss: 4.162134]\n",
      "1809 [D loss: 0.431482, acc.: 90.62%] [G loss: 3.909074]\n",
      "1810 [D loss: 0.432160, acc.: 92.19%] [G loss: 4.001189]\n",
      "1811 [D loss: 0.496344, acc.: 86.72%] [G loss: 4.420547]\n",
      "1812 [D loss: 0.299374, acc.: 97.66%] [G loss: 4.061770]\n",
      "1813 [D loss: 0.424790, acc.: 90.62%] [G loss: 3.784217]\n",
      "1814 [D loss: 0.399523, acc.: 92.19%] [G loss: 4.104911]\n",
      "1815 [D loss: 0.717303, acc.: 86.72%] [G loss: 4.583281]\n",
      "1816 [D loss: 0.608046, acc.: 78.91%] [G loss: 5.838967]\n",
      "1817 [D loss: 0.326242, acc.: 89.84%] [G loss: 6.572586]\n",
      "1818 [D loss: 0.606052, acc.: 92.97%] [G loss: 7.110861]\n",
      "1819 [D loss: 0.893183, acc.: 89.84%] [G loss: 6.242990]\n",
      "1820 [D loss: 0.866744, acc.: 74.22%] [G loss: 3.881301]\n",
      "1821 [D loss: 0.453554, acc.: 82.81%] [G loss: 5.209167]\n",
      "1822 [D loss: 0.343610, acc.: 85.16%] [G loss: 5.717168]\n",
      "1823 [D loss: 0.435017, acc.: 83.59%] [G loss: 6.418035]\n",
      "1824 [D loss: 0.080154, acc.: 96.09%] [G loss: 5.356349]\n",
      "1825 [D loss: 0.213679, acc.: 86.72%] [G loss: 4.584906]\n",
      "1826 [D loss: 0.248183, acc.: 88.28%] [G loss: 4.665267]\n",
      "1827 [D loss: 0.294029, acc.: 85.16%] [G loss: 4.095611]\n",
      "1828 [D loss: 0.903555, acc.: 71.88%] [G loss: 5.859171]\n",
      "1829 [D loss: 0.228690, acc.: 90.62%] [G loss: 5.564583]\n",
      "1830 [D loss: 0.368324, acc.: 90.62%] [G loss: 4.809576]\n",
      "1831 [D loss: 0.348609, acc.: 83.59%] [G loss: 4.064350]\n",
      "1832 [D loss: 0.341530, acc.: 85.16%] [G loss: 3.528107]\n",
      "1833 [D loss: 0.543941, acc.: 79.69%] [G loss: 5.076138]\n",
      "1834 [D loss: 1.800177, acc.: 68.75%] [G loss: 5.738401]\n",
      "1835 [D loss: 1.090734, acc.: 82.81%] [G loss: 4.498211]\n",
      "1836 [D loss: 1.031584, acc.: 71.88%] [G loss: 5.781532]\n",
      "1837 [D loss: 0.467900, acc.: 86.72%] [G loss: 5.504403]\n",
      "1838 [D loss: 0.136541, acc.: 93.75%] [G loss: 4.310850]\n",
      "1839 [D loss: 0.348315, acc.: 85.94%] [G loss: 3.698474]\n",
      "1840 [D loss: 0.727835, acc.: 75.00%] [G loss: 3.841022]\n",
      "1841 [D loss: 0.845371, acc.: 69.53%] [G loss: 4.768397]\n",
      "1842 [D loss: 0.305962, acc.: 85.16%] [G loss: 4.444980]\n",
      "1843 [D loss: 0.436493, acc.: 92.97%] [G loss: 4.383900]\n",
      "1844 [D loss: 0.397745, acc.: 84.38%] [G loss: 2.728933]\n",
      "1845 [D loss: 0.448824, acc.: 75.00%] [G loss: 3.220498]\n",
      "1846 [D loss: 0.496727, acc.: 81.25%] [G loss: 3.801433]\n",
      "1847 [D loss: 0.317395, acc.: 84.38%] [G loss: 4.334853]\n",
      "1848 [D loss: 0.364009, acc.: 89.84%] [G loss: 4.479518]\n",
      "1849 [D loss: 0.885791, acc.: 77.34%] [G loss: 3.779556]\n",
      "1850 [D loss: 0.419363, acc.: 86.72%] [G loss: 3.931400]\n",
      "1851 [D loss: 0.465709, acc.: 78.91%] [G loss: 4.399685]\n",
      "1852 [D loss: 0.426814, acc.: 83.59%] [G loss: 4.836405]\n",
      "1853 [D loss: 0.525976, acc.: 79.69%] [G loss: 4.646578]\n",
      "1854 [D loss: 0.534428, acc.: 85.94%] [G loss: 4.371861]\n",
      "1855 [D loss: 1.292193, acc.: 67.19%] [G loss: 5.635734]\n",
      "1856 [D loss: 0.262412, acc.: 92.19%] [G loss: 5.512752]\n",
      "1857 [D loss: 0.165107, acc.: 92.97%] [G loss: 4.205181]\n",
      "1858 [D loss: 0.605986, acc.: 85.94%] [G loss: 3.761884]\n",
      "1859 [D loss: 0.685072, acc.: 76.56%] [G loss: 4.298635]\n",
      "1860 [D loss: 0.338189, acc.: 89.06%] [G loss: 4.622520]\n",
      "1861 [D loss: 0.882642, acc.: 76.56%] [G loss: 5.721074]\n",
      "1862 [D loss: 0.306217, acc.: 96.88%] [G loss: 5.164384]\n",
      "1863 [D loss: 0.642913, acc.: 89.84%] [G loss: 4.191044]\n",
      "1864 [D loss: 0.306243, acc.: 84.38%] [G loss: 4.419456]\n",
      "1865 [D loss: 0.430892, acc.: 92.97%] [G loss: 4.176916]\n",
      "1866 [D loss: 1.177736, acc.: 70.31%] [G loss: 5.820137]\n",
      "1867 [D loss: 0.230701, acc.: 96.88%] [G loss: 5.932108]\n",
      "1868 [D loss: 0.595459, acc.: 92.97%] [G loss: 4.631574]\n",
      "1869 [D loss: 0.432190, acc.: 92.97%] [G loss: 4.012725]\n",
      "1870 [D loss: 0.186015, acc.: 92.97%] [G loss: 3.700955]\n",
      "1871 [D loss: 0.588842, acc.: 89.06%] [G loss: 4.018708]\n",
      "1872 [D loss: 0.300337, acc.: 90.62%] [G loss: 4.566420]\n",
      "1873 [D loss: 0.315771, acc.: 94.53%] [G loss: 3.693876]\n",
      "1874 [D loss: 0.887975, acc.: 73.44%] [G loss: 6.006807]\n",
      "1875 [D loss: 0.442028, acc.: 91.41%] [G loss: 7.249982]\n",
      "1876 [D loss: 0.788328, acc.: 90.62%] [G loss: 6.361416]\n",
      "1877 [D loss: 0.147304, acc.: 92.97%] [G loss: 5.265285]\n",
      "1878 [D loss: 0.770058, acc.: 91.41%] [G loss: 4.929025]\n",
      "1879 [D loss: 0.632812, acc.: 91.41%] [G loss: 4.198406]\n",
      "1880 [D loss: 0.310703, acc.: 89.06%] [G loss: 3.960833]\n",
      "1881 [D loss: 0.461088, acc.: 87.50%] [G loss: 4.910610]\n",
      "1882 [D loss: 0.517155, acc.: 89.06%] [G loss: 5.472433]\n",
      "1883 [D loss: 0.935256, acc.: 84.38%] [G loss: 5.872469]\n",
      "1884 [D loss: 0.590931, acc.: 82.03%] [G loss: 7.250061]\n",
      "1885 [D loss: 0.394467, acc.: 96.09%] [G loss: 6.548821]\n",
      "1886 [D loss: 0.422653, acc.: 96.09%] [G loss: 4.659451]\n",
      "1887 [D loss: 0.482596, acc.: 93.75%] [G loss: 3.823843]\n",
      "1888 [D loss: 0.494907, acc.: 93.75%] [G loss: 3.228513]\n",
      "1889 [D loss: 0.263653, acc.: 96.88%] [G loss: 3.635664]\n",
      "1890 [D loss: 0.792456, acc.: 91.41%] [G loss: 3.712202]\n",
      "1891 [D loss: 0.827228, acc.: 82.81%] [G loss: 4.951622]\n",
      "1892 [D loss: 0.603527, acc.: 85.16%] [G loss: 6.077478]\n",
      "1893 [D loss: 0.273551, acc.: 98.44%] [G loss: 5.371240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894 [D loss: 0.721307, acc.: 93.75%] [G loss: 4.950537]\n",
      "1895 [D loss: 0.299475, acc.: 97.66%] [G loss: 4.430223]\n",
      "1896 [D loss: 0.459321, acc.: 94.53%] [G loss: 3.853739]\n",
      "1897 [D loss: 1.000008, acc.: 89.84%] [G loss: 3.890504]\n",
      "1898 [D loss: 0.230891, acc.: 94.53%] [G loss: 4.344337]\n",
      "1899 [D loss: 0.781367, acc.: 81.25%] [G loss: 5.424516]\n",
      "1900 [D loss: 0.750538, acc.: 78.12%] [G loss: 6.338954]\n",
      "1901 [D loss: 0.135175, acc.: 99.22%] [G loss: 6.824257]\n",
      "1902 [D loss: 0.044616, acc.: 98.44%] [G loss: 4.464977]\n",
      "1903 [D loss: 0.390552, acc.: 92.97%] [G loss: 4.068597]\n",
      "1904 [D loss: 0.628716, acc.: 92.97%] [G loss: 3.695965]\n",
      "1905 [D loss: 0.706537, acc.: 89.06%] [G loss: 4.395631]\n",
      "1906 [D loss: 0.409281, acc.: 95.31%] [G loss: 3.868623]\n",
      "1907 [D loss: 0.390903, acc.: 91.41%] [G loss: 3.994063]\n",
      "1908 [D loss: 0.936252, acc.: 92.97%] [G loss: 4.059129]\n",
      "1909 [D loss: 0.561766, acc.: 95.31%] [G loss: 3.754907]\n",
      "1910 [D loss: 0.605801, acc.: 92.97%] [G loss: 4.482265]\n",
      "1911 [D loss: 0.296495, acc.: 96.88%] [G loss: 4.733653]\n",
      "1912 [D loss: 0.519623, acc.: 89.84%] [G loss: 4.360315]\n",
      "1913 [D loss: 0.562850, acc.: 94.53%] [G loss: 3.958828]\n",
      "1914 [D loss: 0.720741, acc.: 92.19%] [G loss: 3.915254]\n",
      "1915 [D loss: 0.294822, acc.: 97.66%] [G loss: 3.619730]\n",
      "1916 [D loss: 0.798655, acc.: 95.31%] [G loss: 3.082178]\n",
      "1917 [D loss: 0.579747, acc.: 96.09%] [G loss: 3.285168]\n",
      "1918 [D loss: 0.328492, acc.: 96.09%] [G loss: 3.272368]\n",
      "1919 [D loss: 0.666011, acc.: 89.06%] [G loss: 4.746790]\n",
      "1920 [D loss: 0.335854, acc.: 95.31%] [G loss: 5.001672]\n",
      "1921 [D loss: 0.400797, acc.: 97.66%] [G loss: 3.857603]\n",
      "1922 [D loss: 0.430485, acc.: 96.09%] [G loss: 3.852493]\n",
      "1923 [D loss: 0.408606, acc.: 97.66%] [G loss: 3.397074]\n",
      "1924 [D loss: 0.311514, acc.: 98.44%] [G loss: 2.994812]\n",
      "1925 [D loss: 0.990332, acc.: 90.62%] [G loss: 3.755919]\n",
      "1926 [D loss: 1.130980, acc.: 80.47%] [G loss: 5.551010]\n",
      "1927 [D loss: 1.338904, acc.: 79.69%] [G loss: 7.512262]\n",
      "1928 [D loss: 0.380580, acc.: 97.66%] [G loss: 7.865506]\n",
      "1929 [D loss: 0.389547, acc.: 97.66%] [G loss: 6.889738]\n",
      "1930 [D loss: 0.530557, acc.: 96.88%] [G loss: 5.696925]\n",
      "1931 [D loss: 0.418483, acc.: 96.88%] [G loss: 3.949472]\n",
      "1932 [D loss: 0.219851, acc.: 96.09%] [G loss: 3.796402]\n",
      "1933 [D loss: 0.045513, acc.: 100.00%] [G loss: 3.728499]\n",
      "1934 [D loss: 0.277662, acc.: 98.44%] [G loss: 3.370019]\n",
      "1935 [D loss: 0.038268, acc.: 100.00%] [G loss: 3.587933]\n",
      "1936 [D loss: 0.050543, acc.: 99.22%] [G loss: 3.314079]\n",
      "1937 [D loss: 0.134504, acc.: 93.75%] [G loss: 2.891131]\n",
      "1938 [D loss: 0.529800, acc.: 92.97%] [G loss: 3.259663]\n",
      "1939 [D loss: 0.194440, acc.: 98.44%] [G loss: 3.941582]\n",
      "1940 [D loss: 0.391487, acc.: 97.66%] [G loss: 3.515755]\n",
      "1941 [D loss: 0.421332, acc.: 97.66%] [G loss: 3.291584]\n",
      "1942 [D loss: 0.557905, acc.: 96.88%] [G loss: 3.280387]\n",
      "1943 [D loss: 0.041966, acc.: 100.00%] [G loss: 3.176109]\n",
      "1944 [D loss: 0.229690, acc.: 97.66%] [G loss: 2.713411]\n",
      "1945 [D loss: 0.163184, acc.: 92.97%] [G loss: 3.712737]\n",
      "1946 [D loss: 0.309184, acc.: 91.41%] [G loss: 3.577611]\n",
      "1947 [D loss: 0.564063, acc.: 79.69%] [G loss: 6.987846]\n",
      "1948 [D loss: 0.129210, acc.: 96.09%] [G loss: 8.269697]\n",
      "1949 [D loss: 0.393609, acc.: 96.09%] [G loss: 7.001895]\n",
      "1950 [D loss: 0.388468, acc.: 97.66%] [G loss: 5.928034]\n",
      "1951 [D loss: 0.386834, acc.: 97.66%] [G loss: 4.297516]\n",
      "1952 [D loss: 0.419247, acc.: 96.88%] [G loss: 3.860784]\n",
      "1953 [D loss: 0.183045, acc.: 96.09%] [G loss: 3.134269]\n",
      "1954 [D loss: 0.092950, acc.: 95.31%] [G loss: 3.024793]\n",
      "1955 [D loss: 0.110828, acc.: 96.88%] [G loss: 3.278963]\n",
      "1956 [D loss: 0.416591, acc.: 96.88%] [G loss: 3.474277]\n",
      "1957 [D loss: 0.646390, acc.: 96.09%] [G loss: 3.474478]\n",
      "1958 [D loss: 0.294525, acc.: 97.66%] [G loss: 3.237857]\n",
      "1959 [D loss: 0.177926, acc.: 98.44%] [G loss: 3.048824]\n",
      "1960 [D loss: 0.523531, acc.: 73.44%] [G loss: 3.595129]\n",
      "1961 [D loss: 1.366073, acc.: 78.91%] [G loss: 4.978136]\n",
      "1962 [D loss: 1.238156, acc.: 82.03%] [G loss: 4.899499]\n",
      "1963 [D loss: 0.078971, acc.: 96.09%] [G loss: 6.229702]\n",
      "1964 [D loss: 0.123310, acc.: 96.09%] [G loss: 4.384119]\n",
      "1965 [D loss: 0.344609, acc.: 82.03%] [G loss: 5.061826]\n",
      "1966 [D loss: 0.442335, acc.: 88.28%] [G loss: 3.995172]\n",
      "1967 [D loss: 0.390426, acc.: 77.34%] [G loss: 4.513381]\n",
      "1968 [D loss: 0.125621, acc.: 92.19%] [G loss: 4.786672]\n",
      "1969 [D loss: 0.219288, acc.: 90.62%] [G loss: 3.513971]\n",
      "1970 [D loss: 0.458885, acc.: 82.81%] [G loss: 3.008867]\n",
      "1971 [D loss: 0.793822, acc.: 66.41%] [G loss: 4.542077]\n",
      "1972 [D loss: 0.993920, acc.: 79.69%] [G loss: 5.050549]\n",
      "1973 [D loss: 1.506453, acc.: 71.88%] [G loss: 6.421739]\n",
      "1974 [D loss: 0.360951, acc.: 86.72%] [G loss: 5.704400]\n",
      "1975 [D loss: 0.424595, acc.: 83.59%] [G loss: 6.061381]\n",
      "1976 [D loss: 0.489647, acc.: 78.91%] [G loss: 4.618969]\n",
      "1977 [D loss: 0.479765, acc.: 80.47%] [G loss: 4.313437]\n",
      "1978 [D loss: 1.103508, acc.: 66.41%] [G loss: 3.386616]\n",
      "1979 [D loss: 0.584928, acc.: 79.69%] [G loss: 3.431969]\n",
      "1980 [D loss: 0.858889, acc.: 69.53%] [G loss: 4.806409]\n",
      "1981 [D loss: 0.683131, acc.: 80.47%] [G loss: 4.473250]\n",
      "1982 [D loss: 0.695458, acc.: 73.44%] [G loss: 4.790119]\n",
      "1983 [D loss: 0.796584, acc.: 75.78%] [G loss: 5.282165]\n",
      "1984 [D loss: 0.697964, acc.: 80.47%] [G loss: 5.549087]\n",
      "1985 [D loss: 0.845463, acc.: 74.22%] [G loss: 5.074554]\n",
      "1986 [D loss: 0.871055, acc.: 72.66%] [G loss: 6.257787]\n",
      "1987 [D loss: 0.426211, acc.: 85.16%] [G loss: 5.805380]\n",
      "1988 [D loss: 0.846392, acc.: 78.91%] [G loss: 5.472149]\n",
      "1989 [D loss: 0.679989, acc.: 78.12%] [G loss: 5.746847]\n",
      "1990 [D loss: 0.917943, acc.: 71.88%] [G loss: 5.775199]\n",
      "1991 [D loss: 0.675369, acc.: 78.91%] [G loss: 7.096336]\n",
      "1992 [D loss: 0.477371, acc.: 89.84%] [G loss: 5.861571]\n",
      "1993 [D loss: 1.348257, acc.: 68.75%] [G loss: 6.088980]\n",
      "1994 [D loss: 0.439173, acc.: 89.84%] [G loss: 5.771090]\n",
      "1995 [D loss: 0.599839, acc.: 79.69%] [G loss: 6.167893]\n",
      "1996 [D loss: 0.998003, acc.: 76.56%] [G loss: 6.769727]\n",
      "1997 [D loss: 0.986402, acc.: 78.12%] [G loss: 6.964988]\n",
      "1998 [D loss: 0.172580, acc.: 92.97%] [G loss: 6.032239]\n",
      "1999 [D loss: 0.806471, acc.: 82.81%] [G loss: 5.836398]\n",
      "2000 [D loss: 0.737257, acc.: 83.59%] [G loss: 4.903366]\n",
      "2001 [D loss: 0.493352, acc.: 83.59%] [G loss: 5.709748]\n",
      "2002 [D loss: 0.799618, acc.: 85.94%] [G loss: 6.222040]\n",
      "2003 [D loss: 0.125492, acc.: 93.75%] [G loss: 5.532037]\n",
      "2004 [D loss: 0.591397, acc.: 85.16%] [G loss: 5.112366]\n",
      "2005 [D loss: 0.737429, acc.: 82.03%] [G loss: 5.853013]\n",
      "2006 [D loss: 0.766228, acc.: 89.06%] [G loss: 6.088888]\n",
      "2007 [D loss: 0.739172, acc.: 90.62%] [G loss: 5.456708]\n",
      "2008 [D loss: 1.140489, acc.: 77.34%] [G loss: 5.948072]\n",
      "2009 [D loss: 0.427103, acc.: 85.16%] [G loss: 6.661764]\n",
      "2010 [D loss: 0.580126, acc.: 89.84%] [G loss: 6.318651]\n",
      "2011 [D loss: 0.601287, acc.: 92.97%] [G loss: 5.875016]\n",
      "2012 [D loss: 1.391634, acc.: 79.69%] [G loss: 6.013503]\n",
      "2013 [D loss: 0.301916, acc.: 96.09%] [G loss: 5.774119]\n",
      "2014 [D loss: 1.094375, acc.: 81.25%] [G loss: 4.970643]\n",
      "2015 [D loss: 0.896581, acc.: 87.50%] [G loss: 4.981208]\n",
      "2016 [D loss: 0.417170, acc.: 90.62%] [G loss: 5.238113]\n",
      "2017 [D loss: 0.610720, acc.: 92.97%] [G loss: 5.078324]\n",
      "2018 [D loss: 0.780903, acc.: 89.84%] [G loss: 4.761342]\n",
      "2019 [D loss: 0.363370, acc.: 94.53%] [G loss: 4.847300]\n",
      "2020 [D loss: 0.766556, acc.: 87.50%] [G loss: 5.139522]\n",
      "2021 [D loss: 0.525759, acc.: 92.97%] [G loss: 5.108612]\n",
      "2022 [D loss: 0.782222, acc.: 86.72%] [G loss: 5.226152]\n",
      "2023 [D loss: 0.570704, acc.: 93.75%] [G loss: 5.266148]\n",
      "2024 [D loss: 0.937837, acc.: 92.97%] [G loss: 4.505197]\n",
      "2025 [D loss: 0.374734, acc.: 86.72%] [G loss: 5.327014]\n",
      "2026 [D loss: 0.553668, acc.: 95.31%] [G loss: 5.613734]\n",
      "2027 [D loss: 0.417320, acc.: 95.31%] [G loss: 4.816905]\n",
      "2028 [D loss: 0.685223, acc.: 95.31%] [G loss: 4.817991]\n",
      "2029 [D loss: 0.475131, acc.: 92.19%] [G loss: 5.512951]\n",
      "2030 [D loss: 0.354115, acc.: 93.75%] [G loss: 5.231845]\n",
      "2031 [D loss: 0.736838, acc.: 93.75%] [G loss: 3.786132]\n",
      "2032 [D loss: 1.344129, acc.: 81.25%] [G loss: 4.666995]\n",
      "2033 [D loss: 0.399511, acc.: 97.66%] [G loss: 5.078965]\n",
      "2034 [D loss: 0.524218, acc.: 96.88%] [G loss: 4.510648]\n",
      "2035 [D loss: 0.422205, acc.: 96.88%] [G loss: 3.806560]\n",
      "2036 [D loss: 0.375567, acc.: 92.97%] [G loss: 3.939949]\n",
      "2037 [D loss: 0.354415, acc.: 94.53%] [G loss: 4.218625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038 [D loss: 0.485020, acc.: 92.19%] [G loss: 4.838431]\n",
      "2039 [D loss: 0.215375, acc.: 96.88%] [G loss: 5.466570]\n",
      "2040 [D loss: 0.508509, acc.: 96.88%] [G loss: 5.648494]\n",
      "2041 [D loss: 0.517802, acc.: 96.88%] [G loss: 4.305248]\n",
      "2042 [D loss: 0.552034, acc.: 95.31%] [G loss: 3.768802]\n",
      "2043 [D loss: 0.216744, acc.: 95.31%] [G loss: 3.811715]\n",
      "2044 [D loss: 0.452762, acc.: 97.66%] [G loss: 4.403435]\n",
      "2045 [D loss: 0.277352, acc.: 97.66%] [G loss: 4.258698]\n",
      "2046 [D loss: 0.028904, acc.: 100.00%] [G loss: 3.574399]\n",
      "2047 [D loss: 0.323670, acc.: 96.09%] [G loss: 3.808282]\n",
      "2048 [D loss: 0.317340, acc.: 94.53%] [G loss: 4.217771]\n",
      "2049 [D loss: 1.149944, acc.: 81.25%] [G loss: 6.096539]\n",
      "2050 [D loss: 0.840086, acc.: 84.38%] [G loss: 7.219642]\n",
      "2051 [D loss: 0.534868, acc.: 96.09%] [G loss: 6.153715]\n",
      "2052 [D loss: 0.381290, acc.: 92.19%] [G loss: 5.585026]\n",
      "2053 [D loss: 0.243633, acc.: 92.97%] [G loss: 5.792888]\n",
      "2054 [D loss: 0.257266, acc.: 98.44%] [G loss: 5.999866]\n",
      "2055 [D loss: 0.008863, acc.: 100.00%] [G loss: 4.514256]\n",
      "2056 [D loss: 0.163423, acc.: 97.66%] [G loss: 3.762018]\n",
      "2057 [D loss: 0.060258, acc.: 97.66%] [G loss: 3.614787]\n",
      "2058 [D loss: 0.288931, acc.: 98.44%] [G loss: 3.252795]\n",
      "2059 [D loss: 0.179660, acc.: 98.44%] [G loss: 3.060520]\n",
      "2060 [D loss: 0.188153, acc.: 99.22%] [G loss: 3.842491]\n",
      "2061 [D loss: 0.167165, acc.: 97.66%] [G loss: 4.267047]\n",
      "2062 [D loss: 0.185208, acc.: 97.66%] [G loss: 3.411860]\n",
      "2063 [D loss: 0.262744, acc.: 93.75%] [G loss: 3.576954]\n",
      "2064 [D loss: 0.703296, acc.: 95.31%] [G loss: 4.156180]\n",
      "2065 [D loss: 0.288810, acc.: 97.66%] [G loss: 3.970638]\n",
      "2066 [D loss: 0.029770, acc.: 100.00%] [G loss: 3.697433]\n",
      "2067 [D loss: 0.292058, acc.: 98.44%] [G loss: 3.376920]\n",
      "2068 [D loss: 0.152384, acc.: 99.22%] [G loss: 3.436599]\n",
      "2069 [D loss: 0.059718, acc.: 98.44%] [G loss: 3.917046]\n",
      "2070 [D loss: 0.513157, acc.: 89.84%] [G loss: 4.670997]\n",
      "2071 [D loss: 0.391695, acc.: 89.06%] [G loss: 4.908235]\n",
      "2072 [D loss: 0.583551, acc.: 82.03%] [G loss: 6.988172]\n",
      "2073 [D loss: 0.695172, acc.: 80.47%] [G loss: 9.985157]\n",
      "2074 [D loss: 0.134985, acc.: 99.22%] [G loss: 9.157701]\n",
      "2075 [D loss: 0.255223, acc.: 94.53%] [G loss: 8.127651]\n",
      "2076 [D loss: 0.210696, acc.: 94.53%] [G loss: 8.788861]\n",
      "2077 [D loss: 0.012268, acc.: 100.00%] [G loss: 7.632734]\n",
      "2078 [D loss: 0.325543, acc.: 96.09%] [G loss: 7.356768]\n",
      "2079 [D loss: 0.276192, acc.: 98.44%] [G loss: 7.408846]\n",
      "2080 [D loss: 0.147071, acc.: 99.22%] [G loss: 6.131719]\n",
      "2081 [D loss: 0.293065, acc.: 97.66%] [G loss: 5.291050]\n",
      "2082 [D loss: 0.031267, acc.: 98.44%] [G loss: 4.716637]\n",
      "2083 [D loss: 0.297936, acc.: 98.44%] [G loss: 4.586281]\n",
      "2084 [D loss: 0.294226, acc.: 96.88%] [G loss: 4.020780]\n",
      "2085 [D loss: 0.236064, acc.: 94.53%] [G loss: 4.427898]\n",
      "2086 [D loss: 0.671721, acc.: 94.53%] [G loss: 4.488236]\n",
      "2087 [D loss: 0.187518, acc.: 97.66%] [G loss: 4.516587]\n",
      "2088 [D loss: 0.026870, acc.: 100.00%] [G loss: 3.781377]\n",
      "2089 [D loss: 0.339227, acc.: 96.09%] [G loss: 3.546991]\n",
      "2090 [D loss: 0.303211, acc.: 96.09%] [G loss: 3.381141]\n",
      "2091 [D loss: 0.078684, acc.: 98.44%] [G loss: 3.067978]\n",
      "2092 [D loss: 0.239924, acc.: 97.66%] [G loss: 3.624256]\n",
      "2093 [D loss: 0.235310, acc.: 92.97%] [G loss: 3.936093]\n",
      "2094 [D loss: 0.959414, acc.: 73.44%] [G loss: 7.409955]\n",
      "2095 [D loss: 0.836898, acc.: 84.38%] [G loss: 9.122927]\n",
      "2096 [D loss: 0.509730, acc.: 96.88%] [G loss: 8.035758]\n",
      "2097 [D loss: 0.131288, acc.: 99.22%] [G loss: 5.865155]\n",
      "2098 [D loss: 0.282687, acc.: 98.44%] [G loss: 5.094797]\n",
      "2099 [D loss: 0.229751, acc.: 96.88%] [G loss: 5.260937]\n",
      "2100 [D loss: 0.147228, acc.: 99.22%] [G loss: 5.831178]\n",
      "2101 [D loss: 0.278094, acc.: 98.44%] [G loss: 5.046146]\n",
      "2102 [D loss: 0.301045, acc.: 97.66%] [G loss: 4.902463]\n",
      "2103 [D loss: 0.168807, acc.: 98.44%] [G loss: 5.225016]\n",
      "2104 [D loss: 0.520334, acc.: 96.88%] [G loss: 4.707515]\n",
      "2105 [D loss: 0.400865, acc.: 97.66%] [G loss: 4.641511]\n",
      "2106 [D loss: 0.434676, acc.: 96.09%] [G loss: 4.137284]\n",
      "2107 [D loss: 0.155320, acc.: 99.22%] [G loss: 4.433947]\n",
      "2108 [D loss: 0.421280, acc.: 96.88%] [G loss: 3.607444]\n",
      "2109 [D loss: 0.161068, acc.: 98.44%] [G loss: 3.286817]\n",
      "2110 [D loss: 0.323981, acc.: 96.88%] [G loss: 3.635518]\n",
      "2111 [D loss: 0.044641, acc.: 99.22%] [G loss: 3.401720]\n",
      "2112 [D loss: 0.171023, acc.: 99.22%] [G loss: 2.900953]\n",
      "2113 [D loss: 0.433794, acc.: 96.88%] [G loss: 3.287444]\n",
      "2114 [D loss: 0.051773, acc.: 97.66%] [G loss: 3.385060]\n",
      "2115 [D loss: 0.676671, acc.: 95.31%] [G loss: 4.414368]\n",
      "2116 [D loss: 0.264215, acc.: 98.44%] [G loss: 5.736842]\n",
      "2117 [D loss: 0.402810, acc.: 97.66%] [G loss: 3.741561]\n",
      "2118 [D loss: 0.428008, acc.: 97.66%] [G loss: 3.247642]\n",
      "2119 [D loss: 0.031951, acc.: 100.00%] [G loss: 3.156878]\n",
      "2120 [D loss: 0.176967, acc.: 99.22%] [G loss: 2.966755]\n",
      "2121 [D loss: 0.416549, acc.: 96.88%] [G loss: 3.228008]\n",
      "2122 [D loss: 0.287676, acc.: 98.44%] [G loss: 3.253855]\n",
      "2123 [D loss: 0.160117, acc.: 99.22%] [G loss: 3.304864]\n",
      "2124 [D loss: 0.399404, acc.: 97.66%] [G loss: 3.422756]\n",
      "2125 [D loss: 0.408577, acc.: 97.66%] [G loss: 3.213760]\n",
      "2126 [D loss: 0.145794, acc.: 99.22%] [G loss: 3.592052]\n",
      "2127 [D loss: 0.162207, acc.: 99.22%] [G loss: 4.439408]\n",
      "2128 [D loss: 0.007316, acc.: 100.00%] [G loss: 4.658870]\n",
      "2129 [D loss: 0.077435, acc.: 96.09%] [G loss: 3.655472]\n",
      "2130 [D loss: 0.745991, acc.: 78.12%] [G loss: 6.567189]\n",
      "2131 [D loss: 1.028298, acc.: 83.59%] [G loss: 8.035012]\n",
      "2132 [D loss: 1.254616, acc.: 83.59%] [G loss: 8.637157]\n",
      "2133 [D loss: 0.649180, acc.: 85.94%] [G loss: 10.767818]\n",
      "2134 [D loss: 0.251959, acc.: 98.44%] [G loss: 10.936752]\n",
      "2135 [D loss: 0.378116, acc.: 97.66%] [G loss: 9.844688]\n",
      "2136 [D loss: 0.635855, acc.: 96.09%] [G loss: 8.699236]\n",
      "2137 [D loss: 0.403763, acc.: 96.88%] [G loss: 7.254487]\n",
      "2138 [D loss: 0.436737, acc.: 96.09%] [G loss: 7.162706]\n",
      "2139 [D loss: 0.004703, acc.: 100.00%] [G loss: 6.704553]\n",
      "2140 [D loss: 0.393298, acc.: 97.66%] [G loss: 6.056896]\n",
      "2141 [D loss: 0.280733, acc.: 98.44%] [G loss: 5.722928]\n",
      "2142 [D loss: 0.270043, acc.: 98.44%] [G loss: 5.478567]\n",
      "2143 [D loss: 0.397376, acc.: 97.66%] [G loss: 5.667253]\n",
      "2144 [D loss: 0.169822, acc.: 97.66%] [G loss: 5.293918]\n",
      "2145 [D loss: 0.520632, acc.: 96.88%] [G loss: 4.788364]\n",
      "2146 [D loss: 0.650265, acc.: 96.09%] [G loss: 4.406843]\n",
      "2147 [D loss: 0.284674, acc.: 98.44%] [G loss: 4.185419]\n",
      "2148 [D loss: 0.152159, acc.: 99.22%] [G loss: 4.178013]\n",
      "2149 [D loss: 0.273678, acc.: 98.44%] [G loss: 4.179742]\n",
      "2150 [D loss: 0.926393, acc.: 92.97%] [G loss: 4.203113]\n",
      "2151 [D loss: 0.012176, acc.: 100.00%] [G loss: 4.019853]\n",
      "2152 [D loss: 0.396888, acc.: 97.66%] [G loss: 3.399230]\n",
      "2153 [D loss: 0.410861, acc.: 97.66%] [G loss: 3.186448]\n",
      "2154 [D loss: 0.525672, acc.: 96.88%] [G loss: 3.099173]\n",
      "2155 [D loss: 0.537920, acc.: 96.88%] [G loss: 2.941466]\n",
      "2156 [D loss: 0.296229, acc.: 98.44%] [G loss: 3.168804]\n",
      "2157 [D loss: 0.156062, acc.: 99.22%] [G loss: 2.799044]\n",
      "2158 [D loss: 0.543812, acc.: 96.88%] [G loss: 2.795588]\n",
      "2159 [D loss: 0.537658, acc.: 96.88%] [G loss: 3.068202]\n",
      "2160 [D loss: 0.389432, acc.: 97.66%] [G loss: 3.592821]\n",
      "2161 [D loss: 0.138547, acc.: 99.22%] [G loss: 3.928072]\n",
      "2162 [D loss: 0.415852, acc.: 96.09%] [G loss: 3.541178]\n",
      "2163 [D loss: 0.318555, acc.: 96.88%] [G loss: 3.496252]\n",
      "2164 [D loss: 0.283359, acc.: 97.66%] [G loss: 3.400653]\n",
      "2165 [D loss: 0.156063, acc.: 99.22%] [G loss: 3.384558]\n",
      "2166 [D loss: 0.396857, acc.: 97.66%] [G loss: 3.245376]\n",
      "2167 [D loss: 0.391396, acc.: 97.66%] [G loss: 3.041599]\n",
      "2168 [D loss: 0.520671, acc.: 96.88%] [G loss: 2.917339]\n",
      "2169 [D loss: 0.276758, acc.: 98.44%] [G loss: 2.849064]\n",
      "2170 [D loss: 0.149755, acc.: 99.22%] [G loss: 3.212990]\n",
      "2171 [D loss: 0.260246, acc.: 98.44%] [G loss: 3.505443]\n",
      "2172 [D loss: 0.261237, acc.: 98.44%] [G loss: 3.802487]\n",
      "2173 [D loss: 0.392963, acc.: 97.66%] [G loss: 3.278821]\n",
      "2174 [D loss: 0.157966, acc.: 99.22%] [G loss: 2.874323]\n",
      "2175 [D loss: 0.151706, acc.: 99.22%] [G loss: 3.118329]\n",
      "2176 [D loss: 0.024872, acc.: 99.22%] [G loss: 2.959620]\n",
      "2177 [D loss: 0.267218, acc.: 98.44%] [G loss: 3.502203]\n",
      "2178 [D loss: 0.138694, acc.: 99.22%] [G loss: 3.464118]\n",
      "2179 [D loss: 0.517004, acc.: 96.88%] [G loss: 3.903851]\n",
      "2180 [D loss: 0.278354, acc.: 98.44%] [G loss: 3.684051]\n",
      "2181 [D loss: 0.267625, acc.: 98.44%] [G loss: 3.259609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2182 [D loss: 0.270940, acc.: 98.44%] [G loss: 3.019237]\n",
      "2183 [D loss: 0.946412, acc.: 89.84%] [G loss: 2.489531]\n",
      "2184 [D loss: 0.681331, acc.: 81.25%] [G loss: 5.874387]\n",
      "2185 [D loss: 1.000482, acc.: 85.94%] [G loss: 9.357304]\n",
      "2186 [D loss: 0.048389, acc.: 98.44%] [G loss: 9.687551]\n",
      "2187 [D loss: 0.004564, acc.: 100.00%] [G loss: 7.237790]\n",
      "2188 [D loss: 0.147629, acc.: 98.44%] [G loss: 6.483561]\n",
      "2189 [D loss: 0.022568, acc.: 99.22%] [G loss: 5.275255]\n",
      "2190 [D loss: 0.024944, acc.: 100.00%] [G loss: 4.318451]\n",
      "2191 [D loss: 0.012159, acc.: 100.00%] [G loss: 3.767128]\n",
      "2192 [D loss: 0.014624, acc.: 100.00%] [G loss: 3.209013]\n",
      "2193 [D loss: 0.023305, acc.: 100.00%] [G loss: 2.753247]\n",
      "2194 [D loss: 0.056497, acc.: 98.44%] [G loss: 2.630684]\n",
      "2195 [D loss: 0.272638, acc.: 93.75%] [G loss: 4.433941]\n",
      "2196 [D loss: 0.175031, acc.: 92.97%] [G loss: 5.498882]\n",
      "2197 [D loss: 0.009129, acc.: 100.00%] [G loss: 4.981155]\n",
      "2198 [D loss: 0.004924, acc.: 100.00%] [G loss: 4.871770]\n",
      "2199 [D loss: 0.134805, acc.: 99.22%] [G loss: 3.663529]\n",
      "2200 [D loss: 0.019654, acc.: 100.00%] [G loss: 3.066849]\n",
      "2201 [D loss: 0.277604, acc.: 98.44%] [G loss: 2.791560]\n",
      "2202 [D loss: 0.070414, acc.: 97.66%] [G loss: 2.638021]\n",
      "2203 [D loss: 0.107334, acc.: 95.31%] [G loss: 3.431517]\n",
      "2204 [D loss: 0.153584, acc.: 98.44%] [G loss: 3.517023]\n",
      "2205 [D loss: 0.023562, acc.: 99.22%] [G loss: 3.127506]\n",
      "2206 [D loss: 0.299340, acc.: 98.44%] [G loss: 2.921327]\n",
      "2207 [D loss: 0.031106, acc.: 100.00%] [G loss: 2.830051]\n",
      "2208 [D loss: 0.049463, acc.: 99.22%] [G loss: 2.601269]\n",
      "2209 [D loss: 0.046340, acc.: 100.00%] [G loss: 2.665352]\n",
      "2210 [D loss: 0.312798, acc.: 96.09%] [G loss: 2.861666]\n",
      "2211 [D loss: 0.121191, acc.: 95.31%] [G loss: 4.130684]\n",
      "2212 [D loss: 0.158105, acc.: 99.22%] [G loss: 4.469756]\n",
      "2213 [D loss: 0.031847, acc.: 99.22%] [G loss: 3.737473]\n",
      "2214 [D loss: 0.036687, acc.: 100.00%] [G loss: 3.581760]\n",
      "2215 [D loss: 0.152863, acc.: 99.22%] [G loss: 3.234315]\n",
      "2216 [D loss: 0.030458, acc.: 100.00%] [G loss: 2.861401]\n",
      "2217 [D loss: 0.173770, acc.: 99.22%] [G loss: 2.958332]\n",
      "2218 [D loss: 0.190235, acc.: 96.88%] [G loss: 2.960214]\n",
      "2219 [D loss: 0.219691, acc.: 87.50%] [G loss: 4.516507]\n",
      "2220 [D loss: 1.047870, acc.: 82.81%] [G loss: 5.745736]\n",
      "2221 [D loss: 1.037890, acc.: 79.69%] [G loss: 8.776108]\n",
      "2222 [D loss: 0.378076, acc.: 97.66%] [G loss: 10.760233]\n",
      "2223 [D loss: 0.259294, acc.: 98.44%] [G loss: 9.573891]\n",
      "2224 [D loss: 0.048556, acc.: 98.44%] [G loss: 8.408712]\n",
      "2225 [D loss: 0.253204, acc.: 98.44%] [G loss: 7.072466]\n",
      "2226 [D loss: 0.255168, acc.: 98.44%] [G loss: 5.696706]\n",
      "2227 [D loss: 0.274372, acc.: 98.44%] [G loss: 4.343350]\n",
      "2228 [D loss: 0.173302, acc.: 99.22%] [G loss: 4.192295]\n",
      "2229 [D loss: 0.021447, acc.: 100.00%] [G loss: 4.149850]\n",
      "2230 [D loss: 0.045003, acc.: 99.22%] [G loss: 3.148464]\n",
      "2231 [D loss: 0.438819, acc.: 96.09%] [G loss: 3.097996]\n",
      "2232 [D loss: 0.301603, acc.: 96.88%] [G loss: 3.691782]\n",
      "2233 [D loss: 0.272406, acc.: 98.44%] [G loss: 3.683143]\n",
      "2234 [D loss: 0.094155, acc.: 96.09%] [G loss: 3.267187]\n",
      "2235 [D loss: 0.192380, acc.: 89.84%] [G loss: 4.667143]\n",
      "2236 [D loss: 0.576460, acc.: 92.97%] [G loss: 5.211514]\n",
      "2237 [D loss: 0.140281, acc.: 99.22%] [G loss: 4.263962]\n",
      "2238 [D loss: 0.149314, acc.: 99.22%] [G loss: 3.674026]\n",
      "2239 [D loss: 0.043675, acc.: 98.44%] [G loss: 3.509990]\n",
      "2240 [D loss: 0.295336, acc.: 98.44%] [G loss: 3.256182]\n",
      "2241 [D loss: 0.157430, acc.: 99.22%] [G loss: 3.557954]\n",
      "2242 [D loss: 0.024677, acc.: 100.00%] [G loss: 3.074649]\n",
      "2243 [D loss: 0.156259, acc.: 99.22%] [G loss: 3.033477]\n",
      "2244 [D loss: 0.274424, acc.: 98.44%] [G loss: 2.770334]\n",
      "2245 [D loss: 0.150211, acc.: 99.22%] [G loss: 3.009338]\n",
      "2246 [D loss: 0.020809, acc.: 100.00%] [G loss: 3.709451]\n",
      "2247 [D loss: 0.145749, acc.: 99.22%] [G loss: 3.811987]\n",
      "2248 [D loss: 0.054128, acc.: 98.44%] [G loss: 3.007480]\n",
      "2249 [D loss: 0.339038, acc.: 96.09%] [G loss: 3.188136]\n",
      "2250 [D loss: 0.305230, acc.: 96.88%] [G loss: 3.346266]\n",
      "2251 [D loss: 0.024140, acc.: 100.00%] [G loss: 3.227486]\n",
      "2252 [D loss: 0.394386, acc.: 97.66%] [G loss: 3.206514]\n",
      "2253 [D loss: 0.267307, acc.: 98.44%] [G loss: 3.383418]\n",
      "2254 [D loss: 0.263373, acc.: 98.44%] [G loss: 3.214006]\n",
      "2255 [D loss: 0.147666, acc.: 99.22%] [G loss: 3.356500]\n",
      "2256 [D loss: 0.154688, acc.: 99.22%] [G loss: 4.263970]\n",
      "2257 [D loss: 0.137164, acc.: 99.22%] [G loss: 3.971497]\n",
      "2258 [D loss: 0.413193, acc.: 97.66%] [G loss: 3.050392]\n",
      "2259 [D loss: 0.228136, acc.: 95.31%] [G loss: 4.099457]\n",
      "2260 [D loss: 0.226848, acc.: 94.53%] [G loss: 4.813991]\n",
      "2261 [D loss: 0.577128, acc.: 94.53%] [G loss: 4.937261]\n",
      "2262 [D loss: 0.275541, acc.: 98.44%] [G loss: 5.072180]\n",
      "2263 [D loss: 0.256956, acc.: 98.44%] [G loss: 4.507175]\n",
      "2264 [D loss: 0.263008, acc.: 98.44%] [G loss: 3.903684]\n",
      "2265 [D loss: 0.397076, acc.: 97.66%] [G loss: 3.798312]\n",
      "2266 [D loss: 0.389228, acc.: 97.66%] [G loss: 3.813251]\n",
      "2267 [D loss: 0.260431, acc.: 98.44%] [G loss: 3.645478]\n",
      "2268 [D loss: 0.015243, acc.: 100.00%] [G loss: 4.190310]\n",
      "2269 [D loss: 0.263113, acc.: 98.44%] [G loss: 4.071632]\n",
      "2270 [D loss: 0.173073, acc.: 98.44%] [G loss: 3.021545]\n",
      "2271 [D loss: 0.149243, acc.: 99.22%] [G loss: 2.812048]\n",
      "2272 [D loss: 0.524050, acc.: 96.88%] [G loss: 3.081095]\n",
      "2273 [D loss: 0.400436, acc.: 97.66%] [G loss: 2.759278]\n",
      "2274 [D loss: 0.148037, acc.: 99.22%] [G loss: 2.551791]\n",
      "2275 [D loss: 0.147028, acc.: 99.22%] [G loss: 2.990794]\n",
      "2276 [D loss: 0.141500, acc.: 99.22%] [G loss: 2.760284]\n",
      "2277 [D loss: 0.139443, acc.: 99.22%] [G loss: 3.152292]\n",
      "2278 [D loss: 0.012209, acc.: 100.00%] [G loss: 3.563613]\n",
      "2279 [D loss: 0.024697, acc.: 99.22%] [G loss: 2.587870]\n",
      "2280 [D loss: 0.543456, acc.: 96.88%] [G loss: 3.085056]\n",
      "2281 [D loss: 0.029152, acc.: 100.00%] [G loss: 3.294429]\n",
      "2282 [D loss: 0.153984, acc.: 98.44%] [G loss: 2.796105]\n",
      "2283 [D loss: 0.039700, acc.: 100.00%] [G loss: 2.762477]\n",
      "2284 [D loss: 0.163846, acc.: 99.22%] [G loss: 3.141454]\n",
      "2285 [D loss: 0.159874, acc.: 99.22%] [G loss: 3.657317]\n",
      "2286 [D loss: 0.262205, acc.: 98.44%] [G loss: 3.874153]\n",
      "2287 [D loss: 0.264394, acc.: 98.44%] [G loss: 3.576641]\n",
      "2288 [D loss: 0.270565, acc.: 98.44%] [G loss: 3.359842]\n",
      "2289 [D loss: 0.275507, acc.: 98.44%] [G loss: 2.916666]\n",
      "2290 [D loss: 0.146281, acc.: 99.22%] [G loss: 2.624453]\n",
      "2291 [D loss: 0.396750, acc.: 97.66%] [G loss: 2.784252]\n",
      "2292 [D loss: 0.650196, acc.: 96.09%] [G loss: 2.754545]\n",
      "2293 [D loss: 0.140541, acc.: 99.22%] [G loss: 2.691556]\n",
      "2294 [D loss: 0.398308, acc.: 97.66%] [G loss: 2.884537]\n",
      "2295 [D loss: 0.020390, acc.: 100.00%] [G loss: 3.217137]\n",
      "2296 [D loss: 0.264038, acc.: 98.44%] [G loss: 3.609678]\n",
      "2297 [D loss: 0.404445, acc.: 96.88%] [G loss: 3.217811]\n",
      "2298 [D loss: 0.438556, acc.: 96.88%] [G loss: 3.348087]\n",
      "2299 [D loss: 0.165705, acc.: 99.22%] [G loss: 3.714310]\n",
      "2300 [D loss: 0.382510, acc.: 97.66%] [G loss: 3.806296]\n",
      "2301 [D loss: 0.390965, acc.: 97.66%] [G loss: 3.306531]\n",
      "2302 [D loss: 0.264745, acc.: 98.44%] [G loss: 3.267027]\n",
      "2303 [D loss: 0.390252, acc.: 97.66%] [G loss: 3.163208]\n",
      "2304 [D loss: 0.009799, acc.: 100.00%] [G loss: 3.214907]\n",
      "2305 [D loss: 0.139136, acc.: 99.22%] [G loss: 3.588442]\n",
      "2306 [D loss: 0.263707, acc.: 98.44%] [G loss: 3.217406]\n",
      "2307 [D loss: 0.144991, acc.: 99.22%] [G loss: 3.161867]\n",
      "2308 [D loss: 0.030144, acc.: 100.00%] [G loss: 2.643064]\n",
      "2309 [D loss: 0.032729, acc.: 100.00%] [G loss: 2.859510]\n",
      "2310 [D loss: 0.275065, acc.: 98.44%] [G loss: 3.071372]\n",
      "2311 [D loss: 0.395605, acc.: 97.66%] [G loss: 2.953182]\n",
      "2312 [D loss: 0.139621, acc.: 99.22%] [G loss: 2.985310]\n",
      "2313 [D loss: 0.262050, acc.: 98.44%] [G loss: 3.314851]\n",
      "2314 [D loss: 0.261324, acc.: 98.44%] [G loss: 3.612337]\n",
      "2315 [D loss: 0.267979, acc.: 98.44%] [G loss: 3.280435]\n",
      "2316 [D loss: 0.022159, acc.: 99.22%] [G loss: 2.794267]\n",
      "2317 [D loss: 0.285844, acc.: 96.88%] [G loss: 2.918147]\n",
      "2318 [D loss: 0.541969, acc.: 96.88%] [G loss: 2.891700]\n",
      "2319 [D loss: 0.018077, acc.: 100.00%] [G loss: 3.150722]\n",
      "2320 [D loss: 0.260318, acc.: 98.44%] [G loss: 3.190477]\n",
      "2321 [D loss: 0.261493, acc.: 98.44%] [G loss: 3.240342]\n",
      "2322 [D loss: 0.261010, acc.: 98.44%] [G loss: 3.305258]\n",
      "2323 [D loss: 0.269109, acc.: 98.44%] [G loss: 2.997828]\n",
      "2324 [D loss: 0.022772, acc.: 100.00%] [G loss: 3.030314]\n",
      "2325 [D loss: 0.140719, acc.: 99.22%] [G loss: 2.963408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2326 [D loss: 0.270670, acc.: 98.44%] [G loss: 2.716863]\n",
      "2327 [D loss: 0.018487, acc.: 100.00%] [G loss: 2.741771]\n",
      "2328 [D loss: 0.266454, acc.: 98.44%] [G loss: 3.030582]\n",
      "2329 [D loss: 0.134922, acc.: 99.22%] [G loss: 3.289235]\n",
      "2330 [D loss: 0.269037, acc.: 98.44%] [G loss: 2.753197]\n",
      "2331 [D loss: 0.146118, acc.: 99.22%] [G loss: 2.478538]\n",
      "2332 [D loss: 0.148242, acc.: 99.22%] [G loss: 2.901040]\n",
      "2333 [D loss: 0.275207, acc.: 98.44%] [G loss: 3.121982]\n",
      "2334 [D loss: 0.019652, acc.: 100.00%] [G loss: 3.360336]\n",
      "2335 [D loss: 0.391742, acc.: 97.66%] [G loss: 3.664040]\n",
      "2336 [D loss: 0.141711, acc.: 99.22%] [G loss: 3.490894]\n",
      "2337 [D loss: 0.499845, acc.: 96.88%] [G loss: 2.283516]\n",
      "2338 [D loss: 0.056346, acc.: 99.22%] [G loss: 2.850518]\n",
      "2339 [D loss: 0.218921, acc.: 89.06%] [G loss: 6.283517]\n",
      "2340 [D loss: 2.211825, acc.: 74.22%] [G loss: 8.567753]\n",
      "2341 [D loss: 2.175166, acc.: 83.59%] [G loss: 9.114302]\n",
      "2342 [D loss: 2.265048, acc.: 82.81%] [G loss: 11.376190]\n",
      "2343 [D loss: 1.287902, acc.: 89.84%] [G loss: 13.466779]\n",
      "2344 [D loss: 0.361667, acc.: 96.09%] [G loss: 14.965425]\n",
      "2345 [D loss: 0.000004, acc.: 100.00%] [G loss: 14.564299]\n",
      "2346 [D loss: 0.140192, acc.: 98.44%] [G loss: 13.765949]\n",
      "2347 [D loss: 0.000878, acc.: 100.00%] [G loss: 13.276277]\n",
      "2348 [D loss: 0.173789, acc.: 97.66%] [G loss: 12.242565]\n",
      "2349 [D loss: 0.139046, acc.: 99.22%] [G loss: 11.294299]\n",
      "2350 [D loss: 0.403746, acc.: 96.88%] [G loss: 10.952583]\n",
      "2351 [D loss: 0.144809, acc.: 99.22%] [G loss: 10.728346]\n",
      "2352 [D loss: 0.135895, acc.: 99.22%] [G loss: 9.682365]\n",
      "2353 [D loss: 0.137481, acc.: 99.22%] [G loss: 8.447918]\n",
      "2354 [D loss: 0.636793, acc.: 96.09%] [G loss: 7.153797]\n",
      "2355 [D loss: 0.131646, acc.: 99.22%] [G loss: 5.679428]\n",
      "2356 [D loss: 0.152316, acc.: 97.66%] [G loss: 5.203880]\n",
      "2357 [D loss: 0.042850, acc.: 99.22%] [G loss: 4.808855]\n",
      "2358 [D loss: 0.385772, acc.: 97.66%] [G loss: 4.619163]\n",
      "2359 [D loss: 0.135907, acc.: 99.22%] [G loss: 3.808874]\n",
      "2360 [D loss: 0.019269, acc.: 100.00%] [G loss: 3.311534]\n",
      "2361 [D loss: 0.158536, acc.: 99.22%] [G loss: 3.007670]\n",
      "2362 [D loss: 0.046168, acc.: 98.44%] [G loss: 3.043322]\n",
      "2363 [D loss: 0.180374, acc.: 99.22%] [G loss: 3.757542]\n",
      "2364 [D loss: 0.189936, acc.: 96.88%] [G loss: 3.610025]\n",
      "2365 [D loss: 0.262884, acc.: 96.09%] [G loss: 4.370201]\n",
      "2366 [D loss: 0.248924, acc.: 87.50%] [G loss: 5.709430]\n",
      "2367 [D loss: 0.127646, acc.: 99.22%] [G loss: 6.199768]\n",
      "2368 [D loss: 0.036483, acc.: 98.44%] [G loss: 5.228311]\n",
      "2369 [D loss: 0.006168, acc.: 100.00%] [G loss: 4.749590]\n",
      "2370 [D loss: 0.011724, acc.: 100.00%] [G loss: 4.306909]\n",
      "2371 [D loss: 0.424892, acc.: 97.66%] [G loss: 4.848144]\n",
      "2372 [D loss: 0.280488, acc.: 96.88%] [G loss: 4.523171]\n",
      "2373 [D loss: 0.141656, acc.: 99.22%] [G loss: 4.185507]\n",
      "2374 [D loss: 0.016323, acc.: 100.00%] [G loss: 3.952193]\n",
      "2375 [D loss: 0.264070, acc.: 98.44%] [G loss: 4.206779]\n",
      "2376 [D loss: 0.146769, acc.: 99.22%] [G loss: 3.980178]\n",
      "2377 [D loss: 0.140925, acc.: 99.22%] [G loss: 4.010335]\n",
      "2378 [D loss: 0.147135, acc.: 99.22%] [G loss: 3.275585]\n",
      "2379 [D loss: 0.171638, acc.: 97.66%] [G loss: 3.061604]\n",
      "2380 [D loss: 0.061126, acc.: 99.22%] [G loss: 4.490931]\n",
      "2381 [D loss: 0.153068, acc.: 99.22%] [G loss: 4.780279]\n",
      "2382 [D loss: 0.135026, acc.: 99.22%] [G loss: 4.153777]\n",
      "2383 [D loss: 0.265095, acc.: 98.44%] [G loss: 3.722511]\n",
      "2384 [D loss: 0.273577, acc.: 98.44%] [G loss: 4.043544]\n",
      "2385 [D loss: 0.012425, acc.: 100.00%] [G loss: 3.509108]\n",
      "2386 [D loss: 0.271288, acc.: 98.44%] [G loss: 3.401331]\n",
      "2387 [D loss: 0.272898, acc.: 98.44%] [G loss: 2.955122]\n",
      "2388 [D loss: 0.160803, acc.: 99.22%] [G loss: 3.213673]\n",
      "2389 [D loss: 0.141192, acc.: 99.22%] [G loss: 3.355230]\n",
      "2390 [D loss: 0.020274, acc.: 100.00%] [G loss: 3.381296]\n",
      "2391 [D loss: 0.014654, acc.: 100.00%] [G loss: 3.480006]\n",
      "2392 [D loss: 0.267636, acc.: 98.44%] [G loss: 3.522836]\n",
      "2393 [D loss: 0.526177, acc.: 96.88%] [G loss: 3.091866]\n",
      "2394 [D loss: 0.145951, acc.: 99.22%] [G loss: 2.826987]\n",
      "2395 [D loss: 0.025103, acc.: 100.00%] [G loss: 2.874279]\n",
      "2396 [D loss: 0.289321, acc.: 98.44%] [G loss: 3.595898]\n",
      "2397 [D loss: 0.168801, acc.: 98.44%] [G loss: 3.431623]\n",
      "2398 [D loss: 0.152142, acc.: 99.22%] [G loss: 3.854555]\n",
      "2399 [D loss: 0.134610, acc.: 99.22%] [G loss: 3.995089]\n",
      "2400 [D loss: 0.131561, acc.: 99.22%] [G loss: 3.587275]\n",
      "2401 [D loss: 0.264994, acc.: 98.44%] [G loss: 3.296178]\n",
      "2402 [D loss: 0.259510, acc.: 98.44%] [G loss: 3.664426]\n",
      "2403 [D loss: 0.265818, acc.: 98.44%] [G loss: 3.884183]\n",
      "2404 [D loss: 0.137996, acc.: 99.22%] [G loss: 3.400352]\n",
      "2405 [D loss: 0.154566, acc.: 99.22%] [G loss: 2.602187]\n",
      "2406 [D loss: 0.147689, acc.: 99.22%] [G loss: 2.928321]\n",
      "2407 [D loss: 0.150211, acc.: 99.22%] [G loss: 3.908955]\n",
      "2408 [D loss: 0.134019, acc.: 99.22%] [G loss: 3.968896]\n",
      "2409 [D loss: 0.389530, acc.: 97.66%] [G loss: 3.140015]\n",
      "2410 [D loss: 0.147824, acc.: 99.22%] [G loss: 2.855243]\n",
      "2411 [D loss: 0.015952, acc.: 100.00%] [G loss: 2.914546]\n",
      "2412 [D loss: 0.391144, acc.: 97.66%] [G loss: 3.036675]\n",
      "2413 [D loss: 0.393516, acc.: 97.66%] [G loss: 3.092176]\n",
      "2414 [D loss: 0.385913, acc.: 97.66%] [G loss: 3.280439]\n",
      "2415 [D loss: 0.385101, acc.: 97.66%] [G loss: 3.723246]\n",
      "2416 [D loss: 0.007548, acc.: 100.00%] [G loss: 3.262766]\n",
      "2417 [D loss: 0.262999, acc.: 98.44%] [G loss: 3.087024]\n",
      "2418 [D loss: 1.034229, acc.: 93.75%] [G loss: 2.484849]\n",
      "2419 [D loss: 0.182226, acc.: 98.44%] [G loss: 4.095790]\n",
      "2420 [D loss: 0.785163, acc.: 79.69%] [G loss: 6.332801]\n",
      "2421 [D loss: 1.663409, acc.: 82.03%] [G loss: 10.873620]\n",
      "2422 [D loss: 0.252098, acc.: 98.44%] [G loss: 12.426207]\n",
      "2423 [D loss: 0.252166, acc.: 98.44%] [G loss: 11.042313]\n",
      "2424 [D loss: 0.126123, acc.: 99.22%] [G loss: 9.061845]\n",
      "2425 [D loss: 0.256247, acc.: 98.44%] [G loss: 7.764726]\n",
      "2426 [D loss: 0.260028, acc.: 98.44%] [G loss: 6.832978]\n",
      "2427 [D loss: 0.255950, acc.: 98.44%] [G loss: 6.011771]\n",
      "2428 [D loss: 0.261526, acc.: 98.44%] [G loss: 5.314232]\n",
      "2429 [D loss: 0.132585, acc.: 99.22%] [G loss: 4.907735]\n",
      "2430 [D loss: 0.008027, acc.: 100.00%] [G loss: 4.527585]\n",
      "2431 [D loss: 0.267075, acc.: 98.44%] [G loss: 4.158841]\n",
      "2432 [D loss: 0.015944, acc.: 100.00%] [G loss: 4.040523]\n",
      "2433 [D loss: 0.512092, acc.: 96.88%] [G loss: 4.286738]\n",
      "2434 [D loss: 0.257253, acc.: 98.44%] [G loss: 4.010368]\n",
      "2435 [D loss: 0.273022, acc.: 98.44%] [G loss: 3.433762]\n",
      "2436 [D loss: 0.265743, acc.: 98.44%] [G loss: 3.331420]\n",
      "2437 [D loss: 0.398809, acc.: 97.66%] [G loss: 3.252651]\n",
      "2438 [D loss: 0.149091, acc.: 99.22%] [G loss: 3.043146]\n",
      "2439 [D loss: 0.527922, acc.: 96.88%] [G loss: 3.221214]\n",
      "2440 [D loss: 0.265309, acc.: 98.44%] [G loss: 3.477965]\n",
      "2441 [D loss: 0.280640, acc.: 97.66%] [G loss: 2.800716]\n",
      "2442 [D loss: 0.281914, acc.: 98.44%] [G loss: 2.755783]\n",
      "2443 [D loss: 0.025968, acc.: 100.00%] [G loss: 2.857282]\n",
      "2444 [D loss: 0.156221, acc.: 99.22%] [G loss: 3.458096]\n",
      "2445 [D loss: 0.159844, acc.: 99.22%] [G loss: 3.796647]\n",
      "2446 [D loss: 0.259153, acc.: 98.44%] [G loss: 3.925575]\n",
      "2447 [D loss: 0.261197, acc.: 98.44%] [G loss: 3.542476]\n",
      "2448 [D loss: 0.153383, acc.: 99.22%] [G loss: 2.970313]\n",
      "2449 [D loss: 0.160008, acc.: 99.22%] [G loss: 3.417635]\n",
      "2450 [D loss: 0.014021, acc.: 100.00%] [G loss: 3.627037]\n",
      "2451 [D loss: 0.134632, acc.: 99.22%] [G loss: 3.548201]\n",
      "2452 [D loss: 0.010553, acc.: 100.00%] [G loss: 2.994489]\n",
      "2453 [D loss: 0.143572, acc.: 99.22%] [G loss: 3.096962]\n",
      "2454 [D loss: 0.269290, acc.: 98.44%] [G loss: 3.117080]\n",
      "2455 [D loss: 0.016416, acc.: 100.00%] [G loss: 3.413507]\n",
      "2456 [D loss: 0.010683, acc.: 100.00%] [G loss: 3.140815]\n",
      "2457 [D loss: 0.136778, acc.: 99.22%] [G loss: 3.225094]\n",
      "2458 [D loss: 0.166851, acc.: 98.44%] [G loss: 2.733020]\n",
      "2459 [D loss: 0.166797, acc.: 92.97%] [G loss: 4.113256]\n",
      "2460 [D loss: 0.407293, acc.: 89.84%] [G loss: 6.436374]\n",
      "2461 [D loss: 0.152408, acc.: 96.09%] [G loss: 6.713005]\n",
      "2462 [D loss: 0.001664, acc.: 100.00%] [G loss: 5.730694]\n",
      "2463 [D loss: 0.018105, acc.: 100.00%] [G loss: 4.725525]\n",
      "2464 [D loss: 0.023484, acc.: 100.00%] [G loss: 3.974886]\n",
      "2465 [D loss: 0.036551, acc.: 99.22%] [G loss: 3.306386]\n",
      "2466 [D loss: 0.024343, acc.: 100.00%] [G loss: 3.602618]\n",
      "2467 [D loss: 0.042063, acc.: 98.44%] [G loss: 2.881068]\n",
      "2468 [D loss: 0.127418, acc.: 96.09%] [G loss: 4.979343]\n",
      "2469 [D loss: 0.721766, acc.: 85.94%] [G loss: 5.427712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470 [D loss: 1.238622, acc.: 78.91%] [G loss: 7.544969]\n",
      "2471 [D loss: 0.001221, acc.: 100.00%] [G loss: 9.022926]\n",
      "2472 [D loss: 0.015071, acc.: 100.00%] [G loss: 7.985071]\n",
      "2473 [D loss: 0.015911, acc.: 100.00%] [G loss: 7.063883]\n",
      "2474 [D loss: 0.037881, acc.: 99.22%] [G loss: 5.765665]\n",
      "2475 [D loss: 0.042616, acc.: 97.66%] [G loss: 4.715046]\n",
      "2476 [D loss: 0.152208, acc.: 93.75%] [G loss: 5.253896]\n",
      "2477 [D loss: 0.055033, acc.: 99.22%] [G loss: 5.448543]\n",
      "2478 [D loss: 0.142082, acc.: 96.09%] [G loss: 5.313566]\n",
      "2479 [D loss: 0.310330, acc.: 87.50%] [G loss: 5.341184]\n",
      "2480 [D loss: 0.082184, acc.: 95.31%] [G loss: 5.746911]\n",
      "2481 [D loss: 0.722551, acc.: 74.22%] [G loss: 2.509840]\n",
      "2482 [D loss: 0.926943, acc.: 71.88%] [G loss: 6.704791]\n",
      "2483 [D loss: 2.546068, acc.: 75.00%] [G loss: 7.800263]\n",
      "2484 [D loss: 1.287394, acc.: 84.38%] [G loss: 7.522838]\n",
      "2485 [D loss: 2.294892, acc.: 80.47%] [G loss: 6.521247]\n",
      "2486 [D loss: 2.162825, acc.: 72.66%] [G loss: 8.121924]\n",
      "2487 [D loss: 1.016208, acc.: 87.50%] [G loss: 8.609430]\n",
      "2488 [D loss: 2.309306, acc.: 75.00%] [G loss: 5.965759]\n",
      "2489 [D loss: 3.351645, acc.: 67.97%] [G loss: 5.302147]\n",
      "2490 [D loss: 4.565680, acc.: 58.59%] [G loss: 5.630285]\n",
      "2491 [D loss: 2.082373, acc.: 73.44%] [G loss: 6.054473]\n",
      "2492 [D loss: 2.774024, acc.: 72.66%] [G loss: 7.032737]\n",
      "2493 [D loss: 2.014729, acc.: 70.31%] [G loss: 8.300574]\n",
      "2494 [D loss: 2.249260, acc.: 67.97%] [G loss: 8.093716]\n",
      "2495 [D loss: 1.536428, acc.: 67.97%] [G loss: 8.220333]\n",
      "2496 [D loss: 2.977482, acc.: 64.84%] [G loss: 9.890514]\n",
      "2497 [D loss: 1.891553, acc.: 75.00%] [G loss: 9.500147]\n",
      "2498 [D loss: 2.874374, acc.: 67.97%] [G loss: 9.988356]\n",
      "2499 [D loss: 2.044620, acc.: 73.44%] [G loss: 11.173392]\n",
      "2500 [D loss: 1.769107, acc.: 82.81%] [G loss: 11.741219]\n",
      "2501 [D loss: 2.026726, acc.: 78.12%] [G loss: 12.161231]\n",
      "2502 [D loss: 2.300056, acc.: 79.69%] [G loss: 11.973124]\n",
      "2503 [D loss: 1.226195, acc.: 88.28%] [G loss: 10.531006]\n",
      "2504 [D loss: 2.880075, acc.: 76.56%] [G loss: 10.938000]\n",
      "2505 [D loss: 3.232082, acc.: 72.66%] [G loss: 10.095090]\n",
      "2506 [D loss: 2.753139, acc.: 75.00%] [G loss: 12.529409]\n",
      "2507 [D loss: 0.695484, acc.: 92.19%] [G loss: 13.368810]\n",
      "2508 [D loss: 1.289474, acc.: 89.06%] [G loss: 12.264711]\n",
      "2509 [D loss: 1.624713, acc.: 82.81%] [G loss: 12.320915]\n",
      "2510 [D loss: 1.297513, acc.: 85.16%] [G loss: 13.734688]\n",
      "2511 [D loss: 1.023832, acc.: 86.72%] [G loss: 15.174069]\n",
      "2512 [D loss: 1.263988, acc.: 92.19%] [G loss: 14.725044]\n",
      "2513 [D loss: 1.191984, acc.: 87.50%] [G loss: 13.661649]\n",
      "2514 [D loss: 0.946848, acc.: 89.06%] [G loss: 13.290063]\n",
      "2515 [D loss: 1.175042, acc.: 88.28%] [G loss: 13.186924]\n",
      "2516 [D loss: 1.568820, acc.: 84.38%] [G loss: 15.409311]\n",
      "2517 [D loss: 0.503860, acc.: 96.88%] [G loss: 15.222952]\n",
      "2518 [D loss: 0.145197, acc.: 98.44%] [G loss: 14.601378]\n",
      "2519 [D loss: 0.653897, acc.: 91.41%] [G loss: 12.976550]\n",
      "2520 [D loss: 0.987728, acc.: 87.50%] [G loss: 13.746667]\n",
      "2521 [D loss: 1.208248, acc.: 85.94%] [G loss: 13.111731]\n",
      "2522 [D loss: 0.791814, acc.: 88.28%] [G loss: 13.491766]\n",
      "2523 [D loss: 0.858056, acc.: 90.62%] [G loss: 13.764593]\n",
      "2524 [D loss: 0.646746, acc.: 92.97%] [G loss: 12.631984]\n",
      "2525 [D loss: 1.657313, acc.: 75.00%] [G loss: 14.595819]\n",
      "2526 [D loss: 0.505485, acc.: 96.88%] [G loss: 14.849402]\n",
      "2527 [D loss: 0.644058, acc.: 95.31%] [G loss: 13.685514]\n",
      "2528 [D loss: 0.680850, acc.: 93.75%] [G loss: 12.937974]\n",
      "2529 [D loss: 0.884593, acc.: 83.59%] [G loss: 12.839168]\n",
      "2530 [D loss: 0.532733, acc.: 96.09%] [G loss: 13.362040]\n",
      "2531 [D loss: 0.562480, acc.: 94.53%] [G loss: 11.821445]\n",
      "2532 [D loss: 1.070776, acc.: 84.38%] [G loss: 12.091743]\n",
      "2533 [D loss: 0.287435, acc.: 96.88%] [G loss: 11.956320]\n",
      "2534 [D loss: 0.731897, acc.: 87.50%] [G loss: 12.061413]\n",
      "2535 [D loss: 0.301628, acc.: 94.53%] [G loss: 11.078341]\n",
      "2536 [D loss: 0.363134, acc.: 87.50%] [G loss: 11.972181]\n",
      "2537 [D loss: 0.416150, acc.: 96.88%] [G loss: 10.530417]\n",
      "2538 [D loss: 0.766388, acc.: 87.50%] [G loss: 10.049565]\n",
      "2539 [D loss: 0.716375, acc.: 82.03%] [G loss: 11.567554]\n",
      "2540 [D loss: 0.302057, acc.: 95.31%] [G loss: 10.244133]\n",
      "2541 [D loss: 0.400864, acc.: 91.41%] [G loss: 8.915479]\n",
      "2542 [D loss: 0.255305, acc.: 94.53%] [G loss: 7.886280]\n",
      "2543 [D loss: 0.327325, acc.: 92.19%] [G loss: 6.644304]\n",
      "2544 [D loss: 0.088420, acc.: 96.88%] [G loss: 5.613519]\n",
      "2545 [D loss: 0.477962, acc.: 88.28%] [G loss: 6.016893]\n",
      "2546 [D loss: 0.726827, acc.: 84.38%] [G loss: 6.563492]\n",
      "2547 [D loss: 0.407672, acc.: 97.66%] [G loss: 5.553587]\n",
      "2548 [D loss: 0.585987, acc.: 89.06%] [G loss: 5.958983]\n",
      "2549 [D loss: 0.220719, acc.: 96.88%] [G loss: 5.696641]\n",
      "2550 [D loss: 0.205886, acc.: 96.09%] [G loss: 4.608182]\n",
      "2551 [D loss: 0.412498, acc.: 89.06%] [G loss: 4.953169]\n",
      "2552 [D loss: 0.196898, acc.: 96.88%] [G loss: 4.606703]\n",
      "2553 [D loss: 0.371853, acc.: 92.97%] [G loss: 4.330018]\n",
      "2554 [D loss: 0.201193, acc.: 98.44%] [G loss: 3.837927]\n",
      "2555 [D loss: 0.507414, acc.: 93.75%] [G loss: 4.268456]\n",
      "2556 [D loss: 0.336619, acc.: 96.09%] [G loss: 4.259901]\n",
      "2557 [D loss: 0.364457, acc.: 92.97%] [G loss: 3.961592]\n",
      "2558 [D loss: 0.355253, acc.: 95.31%] [G loss: 3.563596]\n",
      "2559 [D loss: 0.340254, acc.: 96.88%] [G loss: 3.961226]\n",
      "2560 [D loss: 0.324662, acc.: 97.66%] [G loss: 3.335520]\n",
      "2561 [D loss: 0.375119, acc.: 93.75%] [G loss: 3.869184]\n",
      "2562 [D loss: 0.241889, acc.: 93.75%] [G loss: 4.041764]\n",
      "2563 [D loss: 0.371768, acc.: 84.38%] [G loss: 5.043907]\n",
      "2564 [D loss: 0.572165, acc.: 82.81%] [G loss: 7.098448]\n",
      "2565 [D loss: 0.833931, acc.: 83.59%] [G loss: 8.510185]\n",
      "2566 [D loss: 0.278581, acc.: 96.88%] [G loss: 7.875633]\n",
      "2567 [D loss: 0.785593, acc.: 88.28%] [G loss: 8.316085]\n",
      "2568 [D loss: 0.265000, acc.: 98.44%] [G loss: 7.456635]\n",
      "2569 [D loss: 0.489672, acc.: 92.19%] [G loss: 7.049024]\n",
      "2570 [D loss: 0.694675, acc.: 92.97%] [G loss: 6.977453]\n",
      "2571 [D loss: 0.363339, acc.: 93.75%] [G loss: 7.289682]\n",
      "2572 [D loss: 0.165242, acc.: 98.44%] [G loss: 6.589065]\n",
      "2573 [D loss: 0.286368, acc.: 89.06%] [G loss: 7.754186]\n",
      "2574 [D loss: 0.508161, acc.: 96.88%] [G loss: 7.817012]\n",
      "2575 [D loss: 0.534048, acc.: 96.09%] [G loss: 6.183843]\n",
      "2576 [D loss: 0.336507, acc.: 93.75%] [G loss: 6.206738]\n",
      "2577 [D loss: 0.297040, acc.: 98.44%] [G loss: 6.437595]\n",
      "2578 [D loss: 0.022808, acc.: 100.00%] [G loss: 6.035676]\n",
      "2579 [D loss: 0.320985, acc.: 96.88%] [G loss: 6.223010]\n",
      "2580 [D loss: 0.281106, acc.: 98.44%] [G loss: 6.209464]\n",
      "2581 [D loss: 0.626093, acc.: 91.41%] [G loss: 7.049056]\n",
      "2582 [D loss: 0.255671, acc.: 98.44%] [G loss: 6.667251]\n",
      "2583 [D loss: 0.262308, acc.: 98.44%] [G loss: 6.064373]\n",
      "2584 [D loss: 0.486356, acc.: 94.53%] [G loss: 5.942191]\n",
      "2585 [D loss: 0.145567, acc.: 99.22%] [G loss: 5.642155]\n",
      "2586 [D loss: 0.404792, acc.: 97.66%] [G loss: 5.152657]\n",
      "2587 [D loss: 0.494919, acc.: 93.75%] [G loss: 5.536328]\n",
      "2588 [D loss: 0.140514, acc.: 99.22%] [G loss: 4.870881]\n",
      "2589 [D loss: 0.806231, acc.: 94.53%] [G loss: 4.604424]\n",
      "2590 [D loss: 0.426093, acc.: 96.88%] [G loss: 4.480452]\n",
      "2591 [D loss: 0.292653, acc.: 98.44%] [G loss: 4.422838]\n",
      "2592 [D loss: 0.437911, acc.: 96.09%] [G loss: 4.723712]\n",
      "2593 [D loss: 0.530686, acc.: 96.88%] [G loss: 4.391005]\n",
      "2594 [D loss: 0.298009, acc.: 98.44%] [G loss: 3.792691]\n",
      "2595 [D loss: 0.417822, acc.: 97.66%] [G loss: 3.724457]\n",
      "2596 [D loss: 0.424968, acc.: 97.66%] [G loss: 4.162168]\n",
      "2597 [D loss: 0.276103, acc.: 98.44%] [G loss: 4.038207]\n",
      "2598 [D loss: 0.665359, acc.: 95.31%] [G loss: 4.007124]\n",
      "2599 [D loss: 0.814992, acc.: 93.75%] [G loss: 4.792298]\n",
      "2600 [D loss: 0.161680, acc.: 99.22%] [G loss: 4.126368]\n",
      "2601 [D loss: 0.533068, acc.: 96.88%] [G loss: 3.389541]\n",
      "2602 [D loss: 0.510561, acc.: 86.72%] [G loss: 3.254845]\n",
      "2603 [D loss: 0.151009, acc.: 93.75%] [G loss: 3.295607]\n",
      "2604 [D loss: 0.191039, acc.: 99.22%] [G loss: 3.957534]\n",
      "2605 [D loss: 0.027380, acc.: 100.00%] [G loss: 3.557384]\n",
      "2606 [D loss: 0.068767, acc.: 98.44%] [G loss: 4.302000]\n",
      "2607 [D loss: 0.020295, acc.: 100.00%] [G loss: 3.534014]\n",
      "2608 [D loss: 0.193818, acc.: 96.88%] [G loss: 2.850549]\n",
      "2609 [D loss: 0.402276, acc.: 93.75%] [G loss: 3.854306]\n",
      "2610 [D loss: 0.251460, acc.: 90.62%] [G loss: 5.844684]\n",
      "2611 [D loss: 0.250129, acc.: 85.94%] [G loss: 6.954246]\n",
      "2612 [D loss: 0.341555, acc.: 90.62%] [G loss: 8.398037]\n",
      "2613 [D loss: 0.302601, acc.: 89.84%] [G loss: 7.350158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2614 [D loss: 0.081906, acc.: 98.44%] [G loss: 7.508266]\n",
      "2615 [D loss: 0.178913, acc.: 97.66%] [G loss: 6.404256]\n",
      "2616 [D loss: 0.153798, acc.: 91.41%] [G loss: 7.917178]\n",
      "2617 [D loss: 0.008052, acc.: 100.00%] [G loss: 7.992954]\n",
      "2618 [D loss: 0.023223, acc.: 100.00%] [G loss: 5.376809]\n",
      "2619 [D loss: 0.388488, acc.: 92.19%] [G loss: 5.220609]\n",
      "2620 [D loss: 0.042055, acc.: 99.22%] [G loss: 5.053200]\n",
      "2621 [D loss: 0.199802, acc.: 96.09%] [G loss: 4.118123]\n",
      "2622 [D loss: 0.099170, acc.: 96.09%] [G loss: 4.283944]\n",
      "2623 [D loss: 0.661574, acc.: 76.56%] [G loss: 5.857640]\n",
      "2624 [D loss: 0.318818, acc.: 88.28%] [G loss: 7.239440]\n",
      "2625 [D loss: 0.307272, acc.: 95.31%] [G loss: 6.096066]\n",
      "2626 [D loss: 0.360571, acc.: 92.97%] [G loss: 5.404055]\n",
      "2627 [D loss: 0.302094, acc.: 96.09%] [G loss: 4.749024]\n",
      "2628 [D loss: 0.191677, acc.: 96.09%] [G loss: 4.140007]\n",
      "2629 [D loss: 0.098054, acc.: 97.66%] [G loss: 4.189094]\n",
      "2630 [D loss: 0.025863, acc.: 100.00%] [G loss: 4.511258]\n",
      "2631 [D loss: 0.225505, acc.: 93.75%] [G loss: 3.802847]\n",
      "2632 [D loss: 0.204315, acc.: 98.44%] [G loss: 4.312546]\n",
      "2633 [D loss: 0.046497, acc.: 99.22%] [G loss: 3.717878]\n",
      "2634 [D loss: 0.048752, acc.: 100.00%] [G loss: 2.806934]\n",
      "2635 [D loss: 0.385841, acc.: 92.97%] [G loss: 3.483046]\n",
      "2636 [D loss: 0.176067, acc.: 97.66%] [G loss: 3.196380]\n",
      "2637 [D loss: 0.059471, acc.: 99.22%] [G loss: 3.235453]\n",
      "2638 [D loss: 0.447629, acc.: 96.09%] [G loss: 3.432588]\n",
      "2639 [D loss: 0.081407, acc.: 96.88%] [G loss: 3.207030]\n",
      "2640 [D loss: 0.237636, acc.: 97.66%] [G loss: 3.988996]\n",
      "2641 [D loss: 0.225601, acc.: 96.09%] [G loss: 4.051396]\n",
      "2642 [D loss: 0.162354, acc.: 99.22%] [G loss: 3.695326]\n",
      "2643 [D loss: 0.076975, acc.: 98.44%] [G loss: 3.386299]\n",
      "2644 [D loss: 0.040618, acc.: 99.22%] [G loss: 3.415215]\n",
      "2645 [D loss: 0.423965, acc.: 96.88%] [G loss: 3.109230]\n",
      "2646 [D loss: 0.049019, acc.: 100.00%] [G loss: 3.202023]\n",
      "2647 [D loss: 0.329547, acc.: 94.53%] [G loss: 3.380603]\n",
      "2648 [D loss: 0.337174, acc.: 97.66%] [G loss: 3.546885]\n",
      "2649 [D loss: 0.321777, acc.: 88.28%] [G loss: 4.542999]\n",
      "2650 [D loss: 0.372344, acc.: 89.06%] [G loss: 5.853954]\n",
      "2651 [D loss: 0.438547, acc.: 82.03%] [G loss: 8.142827]\n",
      "2652 [D loss: 0.134493, acc.: 98.44%] [G loss: 7.971544]\n",
      "2653 [D loss: 0.173794, acc.: 98.44%] [G loss: 6.122224]\n",
      "2654 [D loss: 0.291343, acc.: 96.88%] [G loss: 5.418520]\n",
      "2655 [D loss: 0.072321, acc.: 97.66%] [G loss: 5.312295]\n",
      "2656 [D loss: 0.022955, acc.: 99.22%] [G loss: 4.951901]\n",
      "2657 [D loss: 0.279483, acc.: 98.44%] [G loss: 4.119864]\n",
      "2658 [D loss: 0.541666, acc.: 96.09%] [G loss: 3.781803]\n",
      "2659 [D loss: 0.310307, acc.: 96.88%] [G loss: 3.887509]\n",
      "2660 [D loss: 0.143474, acc.: 99.22%] [G loss: 4.032041]\n",
      "2661 [D loss: 0.163831, acc.: 98.44%] [G loss: 4.124360]\n",
      "2662 [D loss: 0.660376, acc.: 96.09%] [G loss: 4.130306]\n",
      "2663 [D loss: 0.411273, acc.: 96.88%] [G loss: 2.916640]\n",
      "2664 [D loss: 0.366164, acc.: 95.31%] [G loss: 3.136627]\n",
      "2665 [D loss: 0.144721, acc.: 99.22%] [G loss: 3.392403]\n",
      "2666 [D loss: 0.415878, acc.: 97.66%] [G loss: 3.109104]\n",
      "2667 [D loss: 0.807445, acc.: 94.53%] [G loss: 3.352147]\n",
      "2668 [D loss: 0.407522, acc.: 97.66%] [G loss: 3.098502]\n",
      "2669 [D loss: 0.150766, acc.: 99.22%] [G loss: 2.991696]\n",
      "2670 [D loss: 0.417168, acc.: 96.88%] [G loss: 3.129569]\n",
      "2671 [D loss: 0.277346, acc.: 98.44%] [G loss: 3.323601]\n",
      "2672 [D loss: 0.158013, acc.: 99.22%] [G loss: 2.973446]\n",
      "2673 [D loss: 0.409440, acc.: 97.66%] [G loss: 2.769679]\n",
      "2674 [D loss: 0.284382, acc.: 98.44%] [G loss: 3.023209]\n",
      "2675 [D loss: 0.286810, acc.: 98.44%] [G loss: 2.887987]\n",
      "2676 [D loss: 0.913208, acc.: 94.53%] [G loss: 2.795726]\n",
      "2677 [D loss: 0.154666, acc.: 99.22%] [G loss: 2.975241]\n",
      "2678 [D loss: 0.266500, acc.: 98.44%] [G loss: 3.479699]\n",
      "2679 [D loss: 0.262814, acc.: 98.44%] [G loss: 3.437928]\n",
      "2680 [D loss: 0.393112, acc.: 97.66%] [G loss: 3.610700]\n",
      "2681 [D loss: 0.422707, acc.: 96.88%] [G loss: 2.512525]\n",
      "2682 [D loss: 0.282476, acc.: 98.44%] [G loss: 2.467944]\n",
      "2683 [D loss: 0.403886, acc.: 97.66%] [G loss: 2.397465]\n",
      "2684 [D loss: 0.282923, acc.: 98.44%] [G loss: 2.596644]\n",
      "2685 [D loss: 0.645878, acc.: 96.09%] [G loss: 3.105109]\n",
      "2686 [D loss: 0.269219, acc.: 98.44%] [G loss: 2.614266]\n",
      "2687 [D loss: 0.653580, acc.: 96.09%] [G loss: 2.748505]\n",
      "2688 [D loss: 0.273236, acc.: 98.44%] [G loss: 2.660181]\n",
      "2689 [D loss: 0.394960, acc.: 97.66%] [G loss: 2.743488]\n",
      "2690 [D loss: 0.528372, acc.: 96.88%] [G loss: 2.688570]\n",
      "2691 [D loss: 0.650770, acc.: 96.09%] [G loss: 3.018840]\n",
      "2692 [D loss: 0.149685, acc.: 99.22%] [G loss: 2.308518]\n",
      "2693 [D loss: 0.278841, acc.: 98.44%] [G loss: 2.621491]\n",
      "2694 [D loss: 0.145528, acc.: 99.22%] [G loss: 2.714721]\n",
      "2695 [D loss: 0.395720, acc.: 97.66%] [G loss: 2.496083]\n",
      "2696 [D loss: 0.263995, acc.: 98.44%] [G loss: 3.019918]\n",
      "2697 [D loss: 0.018888, acc.: 100.00%] [G loss: 2.869666]\n",
      "2698 [D loss: 0.393986, acc.: 97.66%] [G loss: 2.839785]\n",
      "2699 [D loss: 0.399056, acc.: 97.66%] [G loss: 2.651862]\n",
      "2700 [D loss: 0.267816, acc.: 98.44%] [G loss: 2.756777]\n",
      "2701 [D loss: 0.162726, acc.: 97.66%] [G loss: 2.245010]\n",
      "2702 [D loss: 0.164867, acc.: 99.22%] [G loss: 2.882645]\n",
      "2703 [D loss: 0.386886, acc.: 97.66%] [G loss: 3.717442]\n",
      "2704 [D loss: 0.135641, acc.: 99.22%] [G loss: 3.348082]\n",
      "2705 [D loss: 0.434921, acc.: 96.09%] [G loss: 3.093368]\n",
      "2706 [D loss: 0.021610, acc.: 100.00%] [G loss: 3.118048]\n",
      "2707 [D loss: 0.264008, acc.: 98.44%] [G loss: 3.363515]\n",
      "2708 [D loss: 0.391097, acc.: 97.66%] [G loss: 3.329629]\n",
      "2709 [D loss: 0.769869, acc.: 95.31%] [G loss: 2.653363]\n",
      "2710 [D loss: 0.271373, acc.: 98.44%] [G loss: 2.500166]\n",
      "2711 [D loss: 0.272013, acc.: 98.44%] [G loss: 2.712700]\n",
      "2712 [D loss: 0.261922, acc.: 98.44%] [G loss: 3.116534]\n",
      "2713 [D loss: 0.392233, acc.: 97.66%] [G loss: 3.033331]\n",
      "2714 [D loss: 0.144870, acc.: 99.22%] [G loss: 2.993113]\n",
      "2715 [D loss: 0.395677, acc.: 97.66%] [G loss: 2.726699]\n",
      "2716 [D loss: 0.404986, acc.: 96.88%] [G loss: 2.589085]\n",
      "2717 [D loss: 0.518653, acc.: 96.88%] [G loss: 2.508781]\n",
      "2718 [D loss: 0.646877, acc.: 96.09%] [G loss: 2.550235]\n",
      "2719 [D loss: 0.513324, acc.: 96.88%] [G loss: 2.968354]\n",
      "2720 [D loss: 0.639247, acc.: 96.09%] [G loss: 3.151402]\n",
      "2721 [D loss: 0.267521, acc.: 98.44%] [G loss: 3.104308]\n",
      "2722 [D loss: 0.524610, acc.: 96.88%] [G loss: 3.005750]\n",
      "2723 [D loss: 0.525306, acc.: 96.88%] [G loss: 3.116399]\n",
      "2724 [D loss: 0.390895, acc.: 97.66%] [G loss: 3.000735]\n",
      "2725 [D loss: 0.392396, acc.: 97.66%] [G loss: 2.681067]\n",
      "2726 [D loss: 0.514446, acc.: 96.88%] [G loss: 2.506819]\n",
      "2727 [D loss: 0.268120, acc.: 98.44%] [G loss: 2.672632]\n",
      "2728 [D loss: 0.393284, acc.: 97.66%] [G loss: 2.696783]\n",
      "2729 [D loss: 0.643373, acc.: 96.09%] [G loss: 2.918861]\n",
      "2730 [D loss: 0.265214, acc.: 98.44%] [G loss: 2.883990]\n",
      "2731 [D loss: 0.138075, acc.: 99.22%] [G loss: 2.960209]\n",
      "2732 [D loss: 0.642438, acc.: 96.09%] [G loss: 2.697990]\n",
      "2733 [D loss: 0.521221, acc.: 96.88%] [G loss: 2.882979]\n",
      "2734 [D loss: 0.391171, acc.: 97.66%] [G loss: 3.384726]\n",
      "2735 [D loss: 0.136858, acc.: 99.22%] [G loss: 3.603977]\n",
      "2736 [D loss: 0.259531, acc.: 98.44%] [G loss: 3.524950]\n",
      "2737 [D loss: 0.529756, acc.: 96.88%] [G loss: 2.813037]\n",
      "2738 [D loss: 0.182582, acc.: 99.22%] [G loss: 3.062429]\n",
      "2739 [D loss: 0.673948, acc.: 95.31%] [G loss: 3.668011]\n",
      "2740 [D loss: 0.276683, acc.: 97.66%] [G loss: 3.614868]\n",
      "2741 [D loss: 0.140226, acc.: 99.22%] [G loss: 3.603683]\n",
      "2742 [D loss: 0.384690, acc.: 97.66%] [G loss: 3.596986]\n",
      "2743 [D loss: 0.140022, acc.: 99.22%] [G loss: 3.347365]\n",
      "2744 [D loss: 0.006356, acc.: 100.00%] [G loss: 3.332358]\n",
      "2745 [D loss: 0.132940, acc.: 99.22%] [G loss: 3.211107]\n",
      "2746 [D loss: 0.137671, acc.: 99.22%] [G loss: 2.858076]\n",
      "2747 [D loss: 0.142282, acc.: 99.22%] [G loss: 2.736253]\n",
      "2748 [D loss: 0.270014, acc.: 98.44%] [G loss: 2.939330]\n",
      "2749 [D loss: 0.637017, acc.: 96.09%] [G loss: 3.277619]\n",
      "2750 [D loss: 0.262748, acc.: 98.44%] [G loss: 2.720549]\n",
      "2751 [D loss: 0.028589, acc.: 100.00%] [G loss: 2.955930]\n",
      "2752 [D loss: 0.012193, acc.: 100.00%] [G loss: 3.174882]\n",
      "2753 [D loss: 0.637406, acc.: 96.09%] [G loss: 3.138540]\n",
      "2754 [D loss: 0.263806, acc.: 98.44%] [G loss: 2.914874]\n",
      "2755 [D loss: 0.527069, acc.: 96.88%] [G loss: 2.416674]\n",
      "2756 [D loss: 0.140312, acc.: 99.22%] [G loss: 2.498043]\n",
      "2757 [D loss: 0.264745, acc.: 98.44%] [G loss: 2.666443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2758 [D loss: 0.263319, acc.: 98.44%] [G loss: 2.743167]\n",
      "2759 [D loss: 0.138907, acc.: 99.22%] [G loss: 2.647094]\n",
      "2760 [D loss: 0.391437, acc.: 97.66%] [G loss: 2.893323]\n",
      "2761 [D loss: 0.268223, acc.: 98.44%] [G loss: 3.066585]\n",
      "2762 [D loss: 0.271223, acc.: 98.44%] [G loss: 2.749476]\n",
      "2763 [D loss: 0.148468, acc.: 99.22%] [G loss: 2.902217]\n",
      "2764 [D loss: 0.135737, acc.: 99.22%] [G loss: 2.876010]\n",
      "2765 [D loss: 0.515788, acc.: 96.88%] [G loss: 3.149513]\n",
      "2766 [D loss: 0.386175, acc.: 97.66%] [G loss: 3.600340]\n",
      "2767 [D loss: 0.133982, acc.: 99.22%] [G loss: 3.376153]\n",
      "2768 [D loss: 0.017562, acc.: 100.00%] [G loss: 2.705798]\n",
      "2769 [D loss: 0.392653, acc.: 97.66%] [G loss: 2.392373]\n",
      "2770 [D loss: 0.396571, acc.: 97.66%] [G loss: 2.574300]\n",
      "2771 [D loss: 0.142435, acc.: 99.22%] [G loss: 2.850234]\n",
      "2772 [D loss: 0.385275, acc.: 97.66%] [G loss: 3.229812]\n",
      "2773 [D loss: 0.518173, acc.: 96.88%] [G loss: 3.046167]\n",
      "2774 [D loss: 0.266136, acc.: 98.44%] [G loss: 3.152546]\n",
      "2775 [D loss: 0.267425, acc.: 98.44%] [G loss: 2.826751]\n",
      "2776 [D loss: 0.146635, acc.: 99.22%] [G loss: 2.904794]\n",
      "2777 [D loss: 0.005099, acc.: 100.00%] [G loss: 3.348684]\n",
      "2778 [D loss: 0.258607, acc.: 98.44%] [G loss: 3.226630]\n",
      "2779 [D loss: 0.385225, acc.: 97.66%] [G loss: 2.949826]\n",
      "2780 [D loss: 0.270764, acc.: 98.44%] [G loss: 2.724698]\n",
      "2781 [D loss: 0.268716, acc.: 98.44%] [G loss: 2.566406]\n",
      "2782 [D loss: 0.269888, acc.: 98.44%] [G loss: 2.811218]\n",
      "2783 [D loss: 0.139711, acc.: 99.22%] [G loss: 2.832293]\n",
      "2784 [D loss: 0.387186, acc.: 97.66%] [G loss: 2.884596]\n",
      "2785 [D loss: 0.259573, acc.: 98.44%] [G loss: 2.991978]\n",
      "2786 [D loss: 0.388287, acc.: 97.66%] [G loss: 2.888484]\n",
      "2787 [D loss: 0.263738, acc.: 98.44%] [G loss: 2.889582]\n",
      "2788 [D loss: 0.515256, acc.: 96.88%] [G loss: 2.681324]\n",
      "2789 [D loss: 0.134445, acc.: 99.22%] [G loss: 2.683424]\n",
      "2790 [D loss: 0.140966, acc.: 99.22%] [G loss: 2.812671]\n",
      "2791 [D loss: 0.262636, acc.: 98.44%] [G loss: 2.729343]\n",
      "2792 [D loss: 0.139173, acc.: 99.22%] [G loss: 2.930872]\n",
      "2793 [D loss: 0.132703, acc.: 99.22%] [G loss: 3.202325]\n",
      "2794 [D loss: 0.390249, acc.: 97.66%] [G loss: 2.791859]\n",
      "2795 [D loss: 0.266266, acc.: 98.44%] [G loss: 2.352753]\n",
      "2796 [D loss: 0.273150, acc.: 98.44%] [G loss: 2.637050]\n",
      "2797 [D loss: 0.015880, acc.: 100.00%] [G loss: 3.236102]\n",
      "2798 [D loss: 0.005467, acc.: 100.00%] [G loss: 3.308927]\n",
      "2799 [D loss: 0.133496, acc.: 99.22%] [G loss: 3.114214]\n",
      "2800 [D loss: 0.139274, acc.: 99.22%] [G loss: 2.465895]\n",
      "2801 [D loss: 0.137160, acc.: 99.22%] [G loss: 2.539401]\n",
      "2802 [D loss: 0.139683, acc.: 99.22%] [G loss: 2.585472]\n",
      "2803 [D loss: 0.389013, acc.: 97.66%] [G loss: 2.738977]\n",
      "2804 [D loss: 0.009633, acc.: 100.00%] [G loss: 3.059774]\n",
      "2805 [D loss: 0.382916, acc.: 97.66%] [G loss: 3.287201]\n",
      "2806 [D loss: 0.134771, acc.: 99.22%] [G loss: 2.899110]\n",
      "2807 [D loss: 0.262545, acc.: 98.44%] [G loss: 2.533376]\n",
      "2808 [D loss: 0.263274, acc.: 98.44%] [G loss: 2.548314]\n",
      "2809 [D loss: 0.011511, acc.: 100.00%] [G loss: 2.472814]\n",
      "2810 [D loss: 0.014730, acc.: 100.00%] [G loss: 2.586899]\n",
      "2811 [D loss: 0.135950, acc.: 99.22%] [G loss: 2.546376]\n",
      "2812 [D loss: 0.010255, acc.: 100.00%] [G loss: 2.516019]\n",
      "2813 [D loss: 0.139101, acc.: 99.22%] [G loss: 2.635263]\n",
      "2814 [D loss: 0.007800, acc.: 100.00%] [G loss: 2.873921]\n",
      "2815 [D loss: 0.385583, acc.: 97.66%] [G loss: 3.305769]\n",
      "2816 [D loss: 0.140549, acc.: 99.22%] [G loss: 2.371562]\n",
      "2817 [D loss: 0.264460, acc.: 98.44%] [G loss: 2.677296]\n",
      "2818 [D loss: 0.509800, acc.: 96.88%] [G loss: 2.867862]\n",
      "2819 [D loss: 0.260092, acc.: 98.44%] [G loss: 2.763880]\n",
      "2820 [D loss: 0.261577, acc.: 98.44%] [G loss: 3.248231]\n",
      "2821 [D loss: 0.259383, acc.: 98.44%] [G loss: 3.161351]\n",
      "2822 [D loss: 0.677549, acc.: 95.31%] [G loss: 2.891874]\n",
      "2823 [D loss: 0.259941, acc.: 98.44%] [G loss: 2.601609]\n",
      "2824 [D loss: 0.136494, acc.: 99.22%] [G loss: 2.608712]\n",
      "2825 [D loss: 0.135726, acc.: 99.22%] [G loss: 2.537148]\n",
      "2826 [D loss: 0.388075, acc.: 97.66%] [G loss: 2.682415]\n",
      "2827 [D loss: 0.259531, acc.: 98.44%] [G loss: 3.138813]\n",
      "2828 [D loss: 0.258382, acc.: 98.44%] [G loss: 3.300290]\n",
      "2829 [D loss: 0.134624, acc.: 99.22%] [G loss: 2.975967]\n",
      "2830 [D loss: 0.276362, acc.: 97.66%] [G loss: 2.038343]\n",
      "2831 [D loss: 0.155694, acc.: 99.22%] [G loss: 2.160508]\n",
      "2832 [D loss: 0.028037, acc.: 100.00%] [G loss: 2.774175]\n",
      "2833 [D loss: 0.309355, acc.: 98.44%] [G loss: 3.614949]\n",
      "2834 [D loss: 0.351605, acc.: 81.25%] [G loss: 7.636532]\n",
      "2835 [D loss: 2.397790, acc.: 75.78%] [G loss: 10.125885]\n",
      "2836 [D loss: 0.541789, acc.: 89.84%] [G loss: 12.652077]\n",
      "2837 [D loss: 0.284371, acc.: 97.66%] [G loss: 13.603844]\n",
      "2838 [D loss: 0.384842, acc.: 96.09%] [G loss: 12.331998]\n",
      "2839 [D loss: 0.003072, acc.: 100.00%] [G loss: 10.278751]\n",
      "2840 [D loss: 0.025315, acc.: 100.00%] [G loss: 9.628658]\n",
      "2841 [D loss: 0.020856, acc.: 99.22%] [G loss: 9.292452]\n",
      "2842 [D loss: 0.136080, acc.: 99.22%] [G loss: 9.373650]\n",
      "2843 [D loss: 0.003641, acc.: 100.00%] [G loss: 8.641167]\n",
      "2844 [D loss: 0.008644, acc.: 100.00%] [G loss: 7.865728]\n",
      "2845 [D loss: 0.475474, acc.: 85.94%] [G loss: 7.417759]\n",
      "2846 [D loss: 0.509871, acc.: 89.84%] [G loss: 7.502437]\n",
      "2847 [D loss: 0.167388, acc.: 92.97%] [G loss: 6.329705]\n",
      "2848 [D loss: 0.460588, acc.: 81.25%] [G loss: 6.314437]\n",
      "2849 [D loss: 0.810205, acc.: 82.03%] [G loss: 6.635697]\n",
      "2850 [D loss: 2.224772, acc.: 75.00%] [G loss: 8.391847]\n",
      "2851 [D loss: 0.485960, acc.: 87.50%] [G loss: 8.425121]\n",
      "2852 [D loss: 0.575475, acc.: 84.38%] [G loss: 9.685796]\n",
      "2853 [D loss: 0.036921, acc.: 99.22%] [G loss: 8.303797]\n",
      "2854 [D loss: 0.793645, acc.: 77.34%] [G loss: 7.276347]\n",
      "2855 [D loss: 0.851774, acc.: 77.34%] [G loss: 6.699260]\n",
      "2856 [D loss: 1.777164, acc.: 67.19%] [G loss: 6.731003]\n",
      "2857 [D loss: 2.611610, acc.: 72.66%] [G loss: 6.594534]\n",
      "2858 [D loss: 2.650435, acc.: 69.53%] [G loss: 7.097648]\n",
      "2859 [D loss: 2.473680, acc.: 76.56%] [G loss: 6.279427]\n",
      "2860 [D loss: 3.745840, acc.: 66.41%] [G loss: 7.598283]\n",
      "2861 [D loss: 1.274292, acc.: 79.69%] [G loss: 7.670595]\n",
      "2862 [D loss: 3.040707, acc.: 64.06%] [G loss: 8.334126]\n",
      "2863 [D loss: 1.526754, acc.: 78.12%] [G loss: 8.286198]\n",
      "2864 [D loss: 1.488485, acc.: 72.66%] [G loss: 9.690174]\n",
      "2865 [D loss: 1.129262, acc.: 75.78%] [G loss: 9.092499]\n",
      "2866 [D loss: 1.750131, acc.: 68.75%] [G loss: 10.310925]\n",
      "2867 [D loss: 2.094702, acc.: 74.22%] [G loss: 12.208886]\n",
      "2868 [D loss: 1.414244, acc.: 85.16%] [G loss: 11.500038]\n",
      "2869 [D loss: 2.372843, acc.: 78.91%] [G loss: 11.179247]\n",
      "2870 [D loss: 2.461669, acc.: 78.12%] [G loss: 11.965961]\n",
      "2871 [D loss: 2.652852, acc.: 75.78%] [G loss: 15.408632]\n",
      "2872 [D loss: 0.160214, acc.: 96.09%] [G loss: 15.653484]\n",
      "2873 [D loss: 0.449530, acc.: 94.53%] [G loss: 15.091522]\n",
      "2874 [D loss: 0.485064, acc.: 95.31%] [G loss: 13.898313]\n",
      "2875 [D loss: 1.284093, acc.: 89.84%] [G loss: 11.944087]\n",
      "2876 [D loss: 1.453494, acc.: 89.06%] [G loss: 12.055068]\n",
      "2877 [D loss: 1.024056, acc.: 87.50%] [G loss: 14.184959]\n",
      "2878 [D loss: 0.122985, acc.: 93.75%] [G loss: 15.245149]\n",
      "2879 [D loss: 0.179178, acc.: 96.88%] [G loss: 13.326254]\n",
      "2880 [D loss: 1.724720, acc.: 83.59%] [G loss: 12.663595]\n",
      "2881 [D loss: 1.499527, acc.: 82.03%] [G loss: 15.644768]\n",
      "2882 [D loss: 0.126092, acc.: 99.22%] [G loss: 15.394294]\n",
      "2883 [D loss: 0.989048, acc.: 85.94%] [G loss: 15.351262]\n",
      "2884 [D loss: 0.509652, acc.: 96.88%] [G loss: 13.718431]\n",
      "2885 [D loss: 1.077218, acc.: 86.72%] [G loss: 12.750311]\n",
      "2886 [D loss: 0.566692, acc.: 90.62%] [G loss: 13.772918]\n",
      "2887 [D loss: 0.364090, acc.: 92.19%] [G loss: 13.551886]\n",
      "2888 [D loss: 0.836465, acc.: 89.06%] [G loss: 14.394344]\n",
      "2889 [D loss: 0.382728, acc.: 97.66%] [G loss: 13.415936]\n",
      "2890 [D loss: 1.652938, acc.: 79.69%] [G loss: 13.491976]\n",
      "2891 [D loss: 0.155697, acc.: 94.53%] [G loss: 12.473232]\n",
      "2892 [D loss: 0.576966, acc.: 84.38%] [G loss: 13.680748]\n",
      "2893 [D loss: 0.215640, acc.: 92.19%] [G loss: 12.628566]\n",
      "2894 [D loss: 0.836133, acc.: 82.81%] [G loss: 11.555679]\n",
      "2895 [D loss: 0.857589, acc.: 82.03%] [G loss: 11.501612]\n",
      "2896 [D loss: 2.508075, acc.: 70.31%] [G loss: 14.711647]\n",
      "2897 [D loss: 0.056701, acc.: 97.66%] [G loss: 12.748762]\n",
      "2898 [D loss: 1.568123, acc.: 82.03%] [G loss: 12.391362]\n",
      "2899 [D loss: 1.454234, acc.: 81.25%] [G loss: 13.495174]\n",
      "2900 [D loss: 0.577074, acc.: 84.38%] [G loss: 12.954925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2901 [D loss: 0.402675, acc.: 84.38%] [G loss: 13.954885]\n",
      "2902 [D loss: 0.406252, acc.: 92.19%] [G loss: 10.771850]\n",
      "2903 [D loss: 2.468430, acc.: 67.97%] [G loss: 12.809406]\n",
      "2904 [D loss: 0.060034, acc.: 98.44%] [G loss: 10.643737]\n",
      "2905 [D loss: 1.447750, acc.: 68.75%] [G loss: 11.135262]\n",
      "2906 [D loss: 0.235003, acc.: 90.62%] [G loss: 10.851358]\n",
      "2907 [D loss: 0.588939, acc.: 83.59%] [G loss: 11.062445]\n",
      "2908 [D loss: 0.374657, acc.: 88.28%] [G loss: 10.193170]\n",
      "2909 [D loss: 2.021262, acc.: 67.19%] [G loss: 13.917454]\n",
      "2910 [D loss: 0.126423, acc.: 99.22%] [G loss: 13.194870]\n",
      "2911 [D loss: 0.280934, acc.: 94.53%] [G loss: 10.374378]\n",
      "2912 [D loss: 1.183626, acc.: 83.59%] [G loss: 9.860765]\n",
      "2913 [D loss: 0.883186, acc.: 82.03%] [G loss: 11.349927]\n",
      "2914 [D loss: 0.287391, acc.: 96.88%] [G loss: 10.091030]\n",
      "2915 [D loss: 0.771143, acc.: 85.16%] [G loss: 10.418358]\n",
      "2916 [D loss: 0.962334, acc.: 80.47%] [G loss: 12.692631]\n",
      "2917 [D loss: 0.136922, acc.: 99.22%] [G loss: 11.012571]\n",
      "2918 [D loss: 1.347290, acc.: 78.91%] [G loss: 10.255400]\n",
      "2919 [D loss: 0.715856, acc.: 85.94%] [G loss: 13.204466]\n",
      "2920 [D loss: 0.629948, acc.: 89.84%] [G loss: 13.992751]\n",
      "2921 [D loss: 0.521488, acc.: 90.62%] [G loss: 14.109811]\n",
      "2922 [D loss: 0.514100, acc.: 89.06%] [G loss: 14.097166]\n",
      "2923 [D loss: 0.517809, acc.: 89.06%] [G loss: 15.269196]\n",
      "2924 [D loss: 0.669043, acc.: 92.97%] [G loss: 13.698487]\n",
      "2925 [D loss: 1.076491, acc.: 85.94%] [G loss: 12.889858]\n",
      "2926 [D loss: 1.508870, acc.: 84.38%] [G loss: 13.882967]\n",
      "2927 [D loss: 0.416267, acc.: 95.31%] [G loss: 12.741177]\n",
      "2928 [D loss: 1.451837, acc.: 78.12%] [G loss: 14.758674]\n",
      "2929 [D loss: 0.398691, acc.: 96.88%] [G loss: 13.822840]\n",
      "2930 [D loss: 0.858995, acc.: 85.94%] [G loss: 12.466422]\n",
      "2931 [D loss: 0.352450, acc.: 89.84%] [G loss: 11.753347]\n",
      "2932 [D loss: 0.326247, acc.: 96.09%] [G loss: 10.188319]\n",
      "2933 [D loss: 1.260423, acc.: 76.56%] [G loss: 11.019223]\n",
      "2934 [D loss: 0.393252, acc.: 96.88%] [G loss: 10.301546]\n",
      "2935 [D loss: 0.973968, acc.: 78.12%] [G loss: 9.559908]\n",
      "2936 [D loss: 0.101698, acc.: 93.75%] [G loss: 7.324196]\n",
      "2937 [D loss: 0.412963, acc.: 78.12%] [G loss: 8.423679]\n",
      "2938 [D loss: 0.415216, acc.: 96.88%] [G loss: 7.509221]\n",
      "2939 [D loss: 1.075420, acc.: 69.53%] [G loss: 9.031247]\n",
      "2940 [D loss: 0.266807, acc.: 98.44%] [G loss: 7.856735]\n",
      "2941 [D loss: 0.343148, acc.: 89.06%] [G loss: 6.104157]\n",
      "2942 [D loss: 0.469702, acc.: 73.44%] [G loss: 6.549540]\n",
      "2943 [D loss: 0.070183, acc.: 98.44%] [G loss: 5.169973]\n",
      "2944 [D loss: 0.218802, acc.: 87.50%] [G loss: 4.729422]\n",
      "2945 [D loss: 0.303179, acc.: 78.12%] [G loss: 5.604966]\n",
      "2946 [D loss: 0.335981, acc.: 96.09%] [G loss: 4.372202]\n",
      "2947 [D loss: 0.457582, acc.: 73.44%] [G loss: 5.228537]\n",
      "2948 [D loss: 0.348959, acc.: 93.75%] [G loss: 5.411084]\n",
      "2949 [D loss: 0.644092, acc.: 82.03%] [G loss: 6.061674]\n",
      "2950 [D loss: 0.562518, acc.: 96.88%] [G loss: 5.873112]\n",
      "2951 [D loss: 0.937213, acc.: 68.75%] [G loss: 7.083186]\n",
      "2952 [D loss: 0.284037, acc.: 98.44%] [G loss: 5.757665]\n",
      "2953 [D loss: 0.663929, acc.: 75.78%] [G loss: 7.245564]\n",
      "2954 [D loss: 0.263251, acc.: 98.44%] [G loss: 5.908593]\n",
      "2955 [D loss: 0.505540, acc.: 75.00%] [G loss: 6.589805]\n",
      "2956 [D loss: 0.145290, acc.: 99.22%] [G loss: 5.771372]\n",
      "2957 [D loss: 0.700883, acc.: 75.00%] [G loss: 6.562523]\n",
      "2958 [D loss: 0.146163, acc.: 99.22%] [G loss: 5.496542]\n",
      "2959 [D loss: 0.170720, acc.: 91.41%] [G loss: 3.869992]\n",
      "2960 [D loss: 0.768001, acc.: 71.88%] [G loss: 6.076533]\n",
      "2961 [D loss: 0.136249, acc.: 99.22%] [G loss: 5.444569]\n",
      "2962 [D loss: 0.412283, acc.: 85.16%] [G loss: 4.886705]\n",
      "2963 [D loss: 0.515197, acc.: 92.19%] [G loss: 4.929969]\n",
      "2964 [D loss: 0.573694, acc.: 92.97%] [G loss: 4.024575]\n",
      "2965 [D loss: 0.336870, acc.: 85.94%] [G loss: 4.999524]\n",
      "2966 [D loss: 0.247583, acc.: 93.75%] [G loss: 4.605543]\n",
      "2967 [D loss: 0.470393, acc.: 85.16%] [G loss: 5.572341]\n",
      "2968 [D loss: 0.952593, acc.: 92.19%] [G loss: 4.193502]\n",
      "2969 [D loss: 0.293644, acc.: 92.19%] [G loss: 4.571178]\n",
      "2970 [D loss: 0.265948, acc.: 92.97%] [G loss: 4.160637]\n",
      "2971 [D loss: 0.402411, acc.: 97.66%] [G loss: 3.479866]\n",
      "2972 [D loss: 0.270000, acc.: 93.75%] [G loss: 3.661842]\n",
      "2973 [D loss: 0.199162, acc.: 96.88%] [G loss: 2.976049]\n",
      "2974 [D loss: 0.503779, acc.: 84.38%] [G loss: 4.447793]\n",
      "2975 [D loss: 0.396354, acc.: 97.66%] [G loss: 3.564860]\n",
      "2976 [D loss: 0.845390, acc.: 86.72%] [G loss: 4.107303]\n",
      "2977 [D loss: 0.047874, acc.: 98.44%] [G loss: 3.763492]\n",
      "2978 [D loss: 0.760776, acc.: 75.78%] [G loss: 3.356284]\n",
      "2979 [D loss: 0.216279, acc.: 95.31%] [G loss: 4.610820]\n",
      "2980 [D loss: 0.230438, acc.: 96.09%] [G loss: 3.720209]\n",
      "2981 [D loss: 0.230007, acc.: 92.97%] [G loss: 3.596010]\n",
      "2982 [D loss: 0.182555, acc.: 99.22%] [G loss: 2.904784]\n",
      "2983 [D loss: 0.177002, acc.: 93.75%] [G loss: 3.589898]\n",
      "2984 [D loss: 0.070375, acc.: 96.88%] [G loss: 3.226948]\n",
      "2985 [D loss: 0.361228, acc.: 96.88%] [G loss: 3.255479]\n",
      "2986 [D loss: 0.190819, acc.: 99.22%] [G loss: 3.085818]\n",
      "2987 [D loss: 0.258461, acc.: 92.97%] [G loss: 3.596685]\n",
      "2988 [D loss: 0.044046, acc.: 100.00%] [G loss: 3.234609]\n",
      "2989 [D loss: 0.258966, acc.: 94.53%] [G loss: 3.098188]\n",
      "2990 [D loss: 0.050553, acc.: 99.22%] [G loss: 2.815956]\n",
      "2991 [D loss: 0.344006, acc.: 94.53%] [G loss: 2.786081]\n",
      "2992 [D loss: 0.205493, acc.: 96.09%] [G loss: 2.823956]\n",
      "2993 [D loss: 0.177665, acc.: 99.22%] [G loss: 2.952578]\n",
      "2994 [D loss: 0.238693, acc.: 96.09%] [G loss: 2.597485]\n",
      "2995 [D loss: 0.185995, acc.: 99.22%] [G loss: 2.663280]\n",
      "2996 [D loss: 0.443526, acc.: 96.88%] [G loss: 2.854105]\n",
      "2997 [D loss: 0.290107, acc.: 98.44%] [G loss: 2.759503]\n",
      "2998 [D loss: 0.067775, acc.: 98.44%] [G loss: 2.611537]\n",
      "2999 [D loss: 0.234554, acc.: 95.31%] [G loss: 2.567186]\n",
      "3000 [D loss: 0.290432, acc.: 98.44%] [G loss: 2.344699]\n",
      "3001 [D loss: 0.357881, acc.: 94.53%] [G loss: 2.672292]\n",
      "3002 [D loss: 0.431552, acc.: 97.66%] [G loss: 2.721703]\n",
      "3003 [D loss: 0.301791, acc.: 98.44%] [G loss: 2.657838]\n",
      "3004 [D loss: 0.321820, acc.: 97.66%] [G loss: 2.915791]\n",
      "3005 [D loss: 0.059106, acc.: 98.44%] [G loss: 3.032730]\n",
      "3006 [D loss: 0.559586, acc.: 96.88%] [G loss: 2.224019]\n",
      "3007 [D loss: 0.067556, acc.: 97.66%] [G loss: 2.178548]\n",
      "3008 [D loss: 0.297873, acc.: 97.66%] [G loss: 2.644940]\n",
      "3009 [D loss: 0.185337, acc.: 96.88%] [G loss: 2.488951]\n",
      "3010 [D loss: 0.055689, acc.: 99.22%] [G loss: 2.869387]\n",
      "3011 [D loss: 0.039326, acc.: 100.00%] [G loss: 2.188894]\n",
      "3012 [D loss: 0.168938, acc.: 99.22%] [G loss: 1.807732]\n",
      "3013 [D loss: 0.338795, acc.: 95.31%] [G loss: 2.344821]\n",
      "3014 [D loss: 0.282455, acc.: 98.44%] [G loss: 2.357601]\n",
      "3015 [D loss: 0.184385, acc.: 97.66%] [G loss: 2.271395]\n",
      "3016 [D loss: 0.158841, acc.: 99.22%] [G loss: 2.346459]\n",
      "3017 [D loss: 0.424168, acc.: 96.88%] [G loss: 2.355298]\n",
      "3018 [D loss: 0.033286, acc.: 100.00%] [G loss: 2.094894]\n",
      "3019 [D loss: 0.042093, acc.: 100.00%] [G loss: 2.033139]\n",
      "3020 [D loss: 0.296906, acc.: 96.88%] [G loss: 2.057819]\n",
      "3021 [D loss: 0.158462, acc.: 99.22%] [G loss: 2.071741]\n",
      "3022 [D loss: 0.294850, acc.: 98.44%] [G loss: 1.839974]\n",
      "3023 [D loss: 0.160284, acc.: 99.22%] [G loss: 2.006649]\n",
      "3024 [D loss: 0.275553, acc.: 98.44%] [G loss: 1.790112]\n",
      "3025 [D loss: 0.288582, acc.: 98.44%] [G loss: 1.881673]\n",
      "3026 [D loss: 0.197251, acc.: 96.09%] [G loss: 2.411035]\n",
      "3027 [D loss: 0.414179, acc.: 97.66%] [G loss: 2.726260]\n",
      "3028 [D loss: 0.162786, acc.: 98.44%] [G loss: 2.670179]\n",
      "3029 [D loss: 0.660930, acc.: 96.09%] [G loss: 2.701871]\n",
      "3030 [D loss: 0.408345, acc.: 97.66%] [G loss: 2.120868]\n",
      "3031 [D loss: 0.527846, acc.: 96.88%] [G loss: 2.228206]\n",
      "3032 [D loss: 0.284534, acc.: 98.44%] [G loss: 2.012025]\n",
      "3033 [D loss: 0.275245, acc.: 98.44%] [G loss: 2.046060]\n",
      "3034 [D loss: 0.152048, acc.: 99.22%] [G loss: 1.928445]\n",
      "3035 [D loss: 0.540705, acc.: 96.88%] [G loss: 2.242908]\n",
      "3036 [D loss: 0.024272, acc.: 100.00%] [G loss: 1.899338]\n",
      "3037 [D loss: 0.036200, acc.: 100.00%] [G loss: 1.804266]\n",
      "3038 [D loss: 0.167174, acc.: 97.66%] [G loss: 2.005933]\n",
      "3039 [D loss: 0.406691, acc.: 97.66%] [G loss: 1.965443]\n",
      "3040 [D loss: 0.395861, acc.: 97.66%] [G loss: 2.400789]\n",
      "3041 [D loss: 0.295220, acc.: 97.66%] [G loss: 2.035145]\n",
      "3042 [D loss: 0.158104, acc.: 99.22%] [G loss: 2.356683]\n",
      "3043 [D loss: 0.027787, acc.: 100.00%] [G loss: 2.386672]\n",
      "3044 [D loss: 0.156143, acc.: 99.22%] [G loss: 2.632689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3045 [D loss: 0.144014, acc.: 99.22%] [G loss: 2.224937]\n",
      "3046 [D loss: 0.025453, acc.: 100.00%] [G loss: 2.239089]\n",
      "3047 [D loss: 0.158181, acc.: 99.22%] [G loss: 2.033263]\n",
      "3048 [D loss: 0.026698, acc.: 100.00%] [G loss: 2.081045]\n",
      "3049 [D loss: 0.405198, acc.: 97.66%] [G loss: 2.079428]\n",
      "3050 [D loss: 0.525402, acc.: 96.88%] [G loss: 2.011522]\n",
      "3051 [D loss: 0.157007, acc.: 99.22%] [G loss: 2.020858]\n",
      "3052 [D loss: 0.784536, acc.: 95.31%] [G loss: 2.388520]\n",
      "3053 [D loss: 0.267744, acc.: 98.44%] [G loss: 2.810266]\n",
      "3054 [D loss: 0.144273, acc.: 99.22%] [G loss: 3.042064]\n",
      "3055 [D loss: 0.270219, acc.: 98.44%] [G loss: 2.437177]\n",
      "3056 [D loss: 0.274209, acc.: 98.44%] [G loss: 2.327238]\n",
      "3057 [D loss: 0.189965, acc.: 98.44%] [G loss: 1.991946]\n",
      "3058 [D loss: 0.271276, acc.: 98.44%] [G loss: 2.261288]\n",
      "3059 [D loss: 0.150546, acc.: 99.22%] [G loss: 2.874482]\n",
      "3060 [D loss: 0.393540, acc.: 97.66%] [G loss: 2.318942]\n",
      "3061 [D loss: 0.153798, acc.: 99.22%] [G loss: 2.198208]\n",
      "3062 [D loss: 0.408275, acc.: 97.66%] [G loss: 2.068691]\n",
      "3063 [D loss: 0.265430, acc.: 98.44%] [G loss: 2.177179]\n",
      "3064 [D loss: 0.142585, acc.: 99.22%] [G loss: 2.214416]\n",
      "3065 [D loss: 0.268721, acc.: 98.44%] [G loss: 2.502719]\n",
      "3066 [D loss: 0.516855, acc.: 96.88%] [G loss: 2.490952]\n",
      "3067 [D loss: 0.404943, acc.: 97.66%] [G loss: 2.517196]\n",
      "3068 [D loss: 0.276035, acc.: 98.44%] [G loss: 2.366412]\n",
      "3069 [D loss: 0.150415, acc.: 99.22%] [G loss: 2.670790]\n",
      "3070 [D loss: 0.140852, acc.: 99.22%] [G loss: 2.521495]\n",
      "3071 [D loss: 0.265753, acc.: 98.44%] [G loss: 2.407233]\n",
      "3072 [D loss: 0.025013, acc.: 100.00%] [G loss: 2.511546]\n",
      "3073 [D loss: 0.261115, acc.: 98.44%] [G loss: 2.618159]\n",
      "3074 [D loss: 0.035040, acc.: 99.22%] [G loss: 2.870853]\n",
      "3075 [D loss: 0.521183, acc.: 96.88%] [G loss: 3.562390]\n",
      "3076 [D loss: 0.142846, acc.: 99.22%] [G loss: 3.309065]\n",
      "3077 [D loss: 0.275853, acc.: 98.44%] [G loss: 2.683992]\n",
      "3078 [D loss: 0.270239, acc.: 98.44%] [G loss: 2.333550]\n",
      "3079 [D loss: 0.273653, acc.: 98.44%] [G loss: 2.436115]\n",
      "3080 [D loss: 0.893972, acc.: 94.53%] [G loss: 2.714987]\n",
      "3081 [D loss: 0.147492, acc.: 99.22%] [G loss: 2.872507]\n",
      "3082 [D loss: 0.898339, acc.: 94.53%] [G loss: 2.797931]\n",
      "3083 [D loss: 0.139668, acc.: 99.22%] [G loss: 2.487335]\n",
      "3084 [D loss: 0.266208, acc.: 98.44%] [G loss: 2.446766]\n",
      "3085 [D loss: 0.141312, acc.: 99.22%] [G loss: 2.281675]\n",
      "3086 [D loss: 0.267186, acc.: 98.44%] [G loss: 2.349289]\n",
      "3087 [D loss: 0.270165, acc.: 98.44%] [G loss: 2.559548]\n",
      "3088 [D loss: 0.390733, acc.: 97.66%] [G loss: 2.640810]\n",
      "3089 [D loss: 0.008841, acc.: 100.00%] [G loss: 2.579244]\n",
      "3090 [D loss: 0.386030, acc.: 97.66%] [G loss: 2.854081]\n",
      "3091 [D loss: 0.011826, acc.: 100.00%] [G loss: 2.478273]\n",
      "3092 [D loss: 0.141510, acc.: 99.22%] [G loss: 2.321935]\n",
      "3093 [D loss: 0.155103, acc.: 99.22%] [G loss: 2.656707]\n",
      "3094 [D loss: 0.142329, acc.: 99.22%] [G loss: 2.333458]\n",
      "3095 [D loss: 0.262012, acc.: 98.44%] [G loss: 2.325327]\n",
      "3096 [D loss: 0.392610, acc.: 97.66%] [G loss: 2.646168]\n",
      "3097 [D loss: 0.389184, acc.: 97.66%] [G loss: 2.600403]\n",
      "3098 [D loss: 0.388278, acc.: 97.66%] [G loss: 2.656424]\n",
      "3099 [D loss: 0.136797, acc.: 99.22%] [G loss: 2.849574]\n",
      "3100 [D loss: 0.018006, acc.: 100.00%] [G loss: 2.457318]\n",
      "3101 [D loss: 0.392100, acc.: 97.66%] [G loss: 2.317186]\n",
      "3102 [D loss: 0.518554, acc.: 96.88%] [G loss: 2.350254]\n",
      "3103 [D loss: 0.765482, acc.: 95.31%] [G loss: 2.208113]\n",
      "3104 [D loss: 0.139597, acc.: 99.22%] [G loss: 2.458277]\n",
      "3105 [D loss: 0.260721, acc.: 98.44%] [G loss: 2.095355]\n",
      "3106 [D loss: 0.139498, acc.: 99.22%] [G loss: 2.177453]\n",
      "3107 [D loss: 0.285713, acc.: 96.88%] [G loss: 2.684116]\n",
      "3108 [D loss: 0.292588, acc.: 98.44%] [G loss: 3.333616]\n",
      "3109 [D loss: 0.393505, acc.: 97.66%] [G loss: 3.573773]\n",
      "3110 [D loss: 0.508530, acc.: 96.88%] [G loss: 3.227014]\n",
      "3111 [D loss: 0.257456, acc.: 98.44%] [G loss: 3.001663]\n",
      "3112 [D loss: 0.387946, acc.: 97.66%] [G loss: 2.509579]\n",
      "3113 [D loss: 0.138789, acc.: 99.22%] [G loss: 2.575068]\n",
      "3114 [D loss: 0.260398, acc.: 98.44%] [G loss: 2.444087]\n",
      "3115 [D loss: 0.389804, acc.: 97.66%] [G loss: 2.481362]\n",
      "3116 [D loss: 0.135140, acc.: 99.22%] [G loss: 2.421412]\n",
      "3117 [D loss: 0.141688, acc.: 99.22%] [G loss: 2.309861]\n",
      "3118 [D loss: 0.259945, acc.: 98.44%] [G loss: 2.295005]\n",
      "3119 [D loss: 0.014129, acc.: 100.00%] [G loss: 2.362147]\n",
      "3120 [D loss: 0.274421, acc.: 98.44%] [G loss: 2.266252]\n",
      "3121 [D loss: 0.141265, acc.: 99.22%] [G loss: 2.421849]\n",
      "3122 [D loss: 0.631743, acc.: 96.09%] [G loss: 2.441058]\n",
      "3123 [D loss: 0.389028, acc.: 97.66%] [G loss: 2.046979]\n",
      "3124 [D loss: 0.014361, acc.: 100.00%] [G loss: 1.968370]\n",
      "3125 [D loss: 0.145518, acc.: 99.22%] [G loss: 2.191716]\n",
      "3126 [D loss: 0.265299, acc.: 98.44%] [G loss: 2.278307]\n",
      "3127 [D loss: 0.016583, acc.: 99.22%] [G loss: 2.475688]\n",
      "3128 [D loss: 0.258865, acc.: 98.44%] [G loss: 2.510184]\n",
      "3129 [D loss: 0.266386, acc.: 98.44%] [G loss: 2.095903]\n",
      "3130 [D loss: 0.016486, acc.: 100.00%] [G loss: 2.093831]\n",
      "3131 [D loss: 0.395955, acc.: 97.66%] [G loss: 2.292830]\n",
      "3132 [D loss: 0.266248, acc.: 98.44%] [G loss: 2.118658]\n",
      "3133 [D loss: 0.264299, acc.: 98.44%] [G loss: 2.609968]\n",
      "3134 [D loss: 0.136760, acc.: 99.22%] [G loss: 2.157752]\n",
      "3135 [D loss: 0.640109, acc.: 96.09%] [G loss: 2.199169]\n",
      "3136 [D loss: 0.014353, acc.: 100.00%] [G loss: 2.203578]\n",
      "3137 [D loss: 0.012097, acc.: 100.00%] [G loss: 2.220388]\n",
      "3138 [D loss: 0.135016, acc.: 99.22%] [G loss: 2.663052]\n",
      "3139 [D loss: 0.133651, acc.: 99.22%] [G loss: 2.711204]\n",
      "3140 [D loss: 0.265958, acc.: 98.44%] [G loss: 2.520838]\n",
      "3141 [D loss: 0.016837, acc.: 100.00%] [G loss: 2.393557]\n",
      "3142 [D loss: 0.027081, acc.: 100.00%] [G loss: 2.336295]\n",
      "3143 [D loss: 0.156046, acc.: 99.22%] [G loss: 2.834701]\n",
      "3144 [D loss: 0.142342, acc.: 99.22%] [G loss: 3.367008]\n",
      "3145 [D loss: 0.132098, acc.: 99.22%] [G loss: 3.260511]\n",
      "3146 [D loss: 0.131624, acc.: 99.22%] [G loss: 3.073165]\n",
      "3147 [D loss: 0.259388, acc.: 98.44%] [G loss: 2.539163]\n",
      "3148 [D loss: 0.265694, acc.: 98.44%] [G loss: 2.669702]\n",
      "3149 [D loss: 0.134726, acc.: 99.22%] [G loss: 2.398511]\n",
      "3150 [D loss: 0.390127, acc.: 97.66%] [G loss: 2.525465]\n",
      "3151 [D loss: 0.261844, acc.: 98.44%] [G loss: 2.451232]\n",
      "3152 [D loss: 0.138728, acc.: 99.22%] [G loss: 2.475589]\n",
      "3153 [D loss: 0.388556, acc.: 97.66%] [G loss: 2.387454]\n",
      "3154 [D loss: 0.262825, acc.: 98.44%] [G loss: 2.724866]\n",
      "3155 [D loss: 0.135858, acc.: 99.22%] [G loss: 3.129211]\n",
      "3156 [D loss: 0.259824, acc.: 98.44%] [G loss: 2.740150]\n",
      "3157 [D loss: 0.264364, acc.: 98.44%] [G loss: 2.705368]\n",
      "3158 [D loss: 0.140675, acc.: 99.22%] [G loss: 2.451554]\n",
      "3159 [D loss: 0.139786, acc.: 99.22%] [G loss: 2.394785]\n",
      "3160 [D loss: 0.392704, acc.: 97.66%] [G loss: 2.317687]\n",
      "3161 [D loss: 0.016317, acc.: 100.00%] [G loss: 2.193674]\n",
      "3162 [D loss: 0.015027, acc.: 100.00%] [G loss: 2.476305]\n",
      "3163 [D loss: 0.178023, acc.: 98.44%] [G loss: 2.467763]\n",
      "3164 [D loss: 0.389066, acc.: 97.66%] [G loss: 2.811021]\n",
      "3165 [D loss: 0.266103, acc.: 98.44%] [G loss: 2.313417]\n",
      "3166 [D loss: 0.269019, acc.: 98.44%] [G loss: 2.627775]\n",
      "3167 [D loss: 0.258786, acc.: 98.44%] [G loss: 2.835317]\n",
      "3168 [D loss: 0.383765, acc.: 97.66%] [G loss: 2.962326]\n",
      "3169 [D loss: 0.134636, acc.: 99.22%] [G loss: 2.472725]\n",
      "3170 [D loss: 0.137469, acc.: 99.22%] [G loss: 2.550011]\n",
      "3171 [D loss: 0.011327, acc.: 100.00%] [G loss: 2.140081]\n",
      "3172 [D loss: 0.138593, acc.: 99.22%] [G loss: 2.427594]\n",
      "3173 [D loss: 0.008803, acc.: 100.00%] [G loss: 2.410036]\n",
      "3174 [D loss: 0.262284, acc.: 98.44%] [G loss: 2.480124]\n",
      "3175 [D loss: 0.392848, acc.: 97.66%] [G loss: 2.274622]\n",
      "3176 [D loss: 0.020724, acc.: 100.00%] [G loss: 2.417167]\n",
      "3177 [D loss: 0.390545, acc.: 97.66%] [G loss: 2.539480]\n",
      "3178 [D loss: 0.261259, acc.: 98.44%] [G loss: 2.567819]\n",
      "3179 [D loss: 0.386493, acc.: 97.66%] [G loss: 2.888864]\n",
      "3180 [D loss: 0.510723, acc.: 96.88%] [G loss: 2.655044]\n",
      "3181 [D loss: 0.010739, acc.: 100.00%] [G loss: 2.585593]\n",
      "3182 [D loss: 0.274088, acc.: 98.44%] [G loss: 2.207458]\n",
      "3183 [D loss: 0.401780, acc.: 97.66%] [G loss: 2.650183]\n",
      "3184 [D loss: 0.261675, acc.: 98.44%] [G loss: 2.991125]\n",
      "3185 [D loss: 0.256384, acc.: 98.44%] [G loss: 2.813883]\n",
      "3186 [D loss: 0.258291, acc.: 98.44%] [G loss: 2.545398]\n",
      "3187 [D loss: 0.637622, acc.: 96.09%] [G loss: 2.536546]\n",
      "3188 [D loss: 0.264130, acc.: 98.44%] [G loss: 2.649914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189 [D loss: 0.648613, acc.: 95.31%] [G loss: 2.357648]\n",
      "3190 [D loss: 0.433876, acc.: 96.88%] [G loss: 2.399556]\n",
      "3191 [D loss: 0.264231, acc.: 98.44%] [G loss: 2.334823]\n",
      "3192 [D loss: 0.388691, acc.: 97.66%] [G loss: 2.275024]\n",
      "3193 [D loss: 0.176868, acc.: 98.44%] [G loss: 2.510766]\n",
      "3194 [D loss: 0.387233, acc.: 97.66%] [G loss: 2.611028]\n",
      "3195 [D loss: 0.259322, acc.: 98.44%] [G loss: 2.820315]\n",
      "3196 [D loss: 0.131084, acc.: 99.22%] [G loss: 2.807298]\n",
      "3197 [D loss: 0.133557, acc.: 99.22%] [G loss: 2.532679]\n",
      "3198 [D loss: 0.267307, acc.: 98.44%] [G loss: 2.021452]\n",
      "3199 [D loss: 0.262825, acc.: 98.44%] [G loss: 2.156752]\n",
      "3200 [D loss: 0.388824, acc.: 97.66%] [G loss: 2.477919]\n",
      "3201 [D loss: 0.391232, acc.: 97.66%] [G loss: 2.457579]\n",
      "3202 [D loss: 0.133441, acc.: 99.22%] [G loss: 2.590249]\n",
      "3203 [D loss: 0.387421, acc.: 97.66%] [G loss: 2.361401]\n",
      "3204 [D loss: 0.262331, acc.: 98.44%] [G loss: 2.641599]\n",
      "3205 [D loss: 0.007422, acc.: 100.00%] [G loss: 2.384426]\n",
      "3206 [D loss: 0.259501, acc.: 98.44%] [G loss: 2.544255]\n",
      "3207 [D loss: 0.133174, acc.: 99.22%] [G loss: 2.552722]\n",
      "3208 [D loss: 0.261354, acc.: 98.44%] [G loss: 2.303736]\n",
      "3209 [D loss: 0.012476, acc.: 100.00%] [G loss: 2.167750]\n",
      "3210 [D loss: 0.262200, acc.: 98.44%] [G loss: 2.216167]\n",
      "3211 [D loss: 0.263088, acc.: 98.44%] [G loss: 2.297512]\n",
      "3212 [D loss: 0.135459, acc.: 99.22%] [G loss: 2.401088]\n",
      "3213 [D loss: 0.639292, acc.: 96.09%] [G loss: 2.499753]\n",
      "3214 [D loss: 0.131550, acc.: 99.22%] [G loss: 2.760036]\n",
      "3215 [D loss: 0.260292, acc.: 98.44%] [G loss: 2.486761]\n",
      "3216 [D loss: 0.636669, acc.: 96.09%] [G loss: 2.482022]\n",
      "3217 [D loss: 0.389141, acc.: 97.66%] [G loss: 2.331397]\n",
      "3218 [D loss: 0.263631, acc.: 98.44%] [G loss: 2.219342]\n",
      "3219 [D loss: 0.010669, acc.: 100.00%] [G loss: 2.127576]\n",
      "3220 [D loss: 0.643278, acc.: 96.09%] [G loss: 2.301271]\n",
      "3221 [D loss: 0.387043, acc.: 97.66%] [G loss: 2.379084]\n",
      "3222 [D loss: 0.511006, acc.: 96.88%] [G loss: 2.548518]\n",
      "3223 [D loss: 0.008582, acc.: 100.00%] [G loss: 2.652072]\n",
      "3224 [D loss: 0.005654, acc.: 100.00%] [G loss: 2.885129]\n",
      "3225 [D loss: 0.261336, acc.: 98.44%] [G loss: 2.391986]\n",
      "3226 [D loss: 0.263318, acc.: 98.44%] [G loss: 2.209556]\n",
      "3227 [D loss: 0.137520, acc.: 99.22%] [G loss: 2.216669]\n",
      "3228 [D loss: 0.264401, acc.: 98.44%] [G loss: 2.360060]\n",
      "3229 [D loss: 0.257917, acc.: 98.44%] [G loss: 2.390442]\n",
      "3230 [D loss: 0.259592, acc.: 98.44%] [G loss: 2.328397]\n",
      "3231 [D loss: 0.259256, acc.: 98.44%] [G loss: 2.888237]\n",
      "3232 [D loss: 0.383515, acc.: 97.66%] [G loss: 2.713387]\n",
      "3233 [D loss: 0.262196, acc.: 98.44%] [G loss: 2.597372]\n",
      "3234 [D loss: 0.387485, acc.: 97.66%] [G loss: 2.351285]\n",
      "3235 [D loss: 0.389652, acc.: 97.66%] [G loss: 2.486088]\n",
      "3236 [D loss: 0.266450, acc.: 98.44%] [G loss: 2.607708]\n",
      "3237 [D loss: 0.133281, acc.: 99.22%] [G loss: 2.590903]\n",
      "3238 [D loss: 0.263996, acc.: 98.44%] [G loss: 2.532717]\n",
      "3239 [D loss: 0.258019, acc.: 98.44%] [G loss: 2.714509]\n",
      "3240 [D loss: 0.130766, acc.: 99.22%] [G loss: 3.104340]\n",
      "3241 [D loss: 0.508749, acc.: 96.88%] [G loss: 3.134955]\n",
      "3242 [D loss: 0.385883, acc.: 97.66%] [G loss: 2.790895]\n",
      "3243 [D loss: 0.135155, acc.: 99.22%] [G loss: 2.493229]\n",
      "3244 [D loss: 0.137874, acc.: 99.22%] [G loss: 2.444700]\n",
      "3245 [D loss: 0.390191, acc.: 97.66%] [G loss: 2.323558]\n",
      "3246 [D loss: 0.135194, acc.: 99.22%] [G loss: 2.249895]\n",
      "3247 [D loss: 0.137212, acc.: 99.22%] [G loss: 2.371736]\n",
      "3248 [D loss: 0.387675, acc.: 97.66%] [G loss: 2.464409]\n",
      "3249 [D loss: 0.135300, acc.: 99.22%] [G loss: 2.456002]\n",
      "3250 [D loss: 0.636530, acc.: 96.09%] [G loss: 2.691272]\n",
      "3251 [D loss: 0.510538, acc.: 96.88%] [G loss: 2.913987]\n",
      "3252 [D loss: 0.258216, acc.: 98.44%] [G loss: 2.675950]\n",
      "3253 [D loss: 0.260676, acc.: 98.44%] [G loss: 2.899587]\n",
      "3254 [D loss: 0.258525, acc.: 98.44%] [G loss: 2.654218]\n",
      "3255 [D loss: 0.638666, acc.: 96.09%] [G loss: 2.674776]\n",
      "3256 [D loss: 0.132654, acc.: 99.22%] [G loss: 2.537652]\n",
      "3257 [D loss: 0.765273, acc.: 95.31%] [G loss: 2.372505]\n",
      "3258 [D loss: 0.135492, acc.: 99.22%] [G loss: 2.290595]\n",
      "3259 [D loss: 0.011951, acc.: 100.00%] [G loss: 2.401346]\n",
      "3260 [D loss: 0.136196, acc.: 99.22%] [G loss: 2.431016]\n",
      "3261 [D loss: 0.142287, acc.: 98.44%] [G loss: 2.894889]\n",
      "3262 [D loss: 0.131251, acc.: 99.22%] [G loss: 2.691285]\n",
      "3263 [D loss: 0.172171, acc.: 98.44%] [G loss: 2.592404]\n",
      "3264 [D loss: 0.384290, acc.: 97.66%] [G loss: 2.494270]\n",
      "3265 [D loss: 0.511005, acc.: 96.88%] [G loss: 2.747470]\n",
      "3266 [D loss: 0.008864, acc.: 100.00%] [G loss: 2.607160]\n",
      "3267 [D loss: 0.637303, acc.: 96.09%] [G loss: 2.579610]\n",
      "3268 [D loss: 0.008435, acc.: 100.00%] [G loss: 2.813175]\n",
      "3269 [D loss: 0.258572, acc.: 98.44%] [G loss: 2.894793]\n",
      "3270 [D loss: 0.263126, acc.: 98.44%] [G loss: 2.707490]\n",
      "3271 [D loss: 0.259653, acc.: 98.44%] [G loss: 2.658387]\n",
      "3272 [D loss: 0.258921, acc.: 98.44%] [G loss: 2.460181]\n",
      "3273 [D loss: 0.259961, acc.: 98.44%] [G loss: 2.508334]\n",
      "3274 [D loss: 0.007468, acc.: 100.00%] [G loss: 2.406651]\n",
      "3275 [D loss: 0.136304, acc.: 99.22%] [G loss: 2.401199]\n",
      "3276 [D loss: 0.510273, acc.: 96.88%] [G loss: 2.750364]\n",
      "3277 [D loss: 0.005240, acc.: 100.00%] [G loss: 2.968071]\n",
      "3278 [D loss: 0.005295, acc.: 100.00%] [G loss: 3.048756]\n",
      "3279 [D loss: 0.384969, acc.: 97.66%] [G loss: 2.830029]\n",
      "3280 [D loss: 0.386247, acc.: 97.66%] [G loss: 2.511245]\n",
      "3281 [D loss: 0.260271, acc.: 98.44%] [G loss: 2.669883]\n",
      "3282 [D loss: 0.259078, acc.: 98.44%] [G loss: 2.707810]\n",
      "3283 [D loss: 0.133919, acc.: 99.22%] [G loss: 2.921039]\n",
      "3284 [D loss: 0.359557, acc.: 95.31%] [G loss: 2.910813]\n",
      "3285 [D loss: 0.326371, acc.: 81.25%] [G loss: 6.802654]\n",
      "3286 [D loss: 0.652300, acc.: 90.62%] [G loss: 10.451183]\n",
      "3287 [D loss: 0.130630, acc.: 99.22%] [G loss: 10.467846]\n",
      "3288 [D loss: 0.081236, acc.: 96.09%] [G loss: 9.702841]\n",
      "3289 [D loss: 0.002201, acc.: 100.00%] [G loss: 7.777415]\n",
      "3290 [D loss: 0.031860, acc.: 97.66%] [G loss: 7.217850]\n",
      "3291 [D loss: 0.017890, acc.: 99.22%] [G loss: 6.095775]\n",
      "3292 [D loss: 0.006665, acc.: 100.00%] [G loss: 5.465719]\n",
      "3293 [D loss: 0.005715, acc.: 100.00%] [G loss: 4.496736]\n",
      "3294 [D loss: 0.012650, acc.: 100.00%] [G loss: 4.198204]\n",
      "3295 [D loss: 0.025816, acc.: 100.00%] [G loss: 3.636688]\n",
      "3296 [D loss: 0.023358, acc.: 100.00%] [G loss: 3.193703]\n",
      "3297 [D loss: 0.037408, acc.: 100.00%] [G loss: 2.981363]\n",
      "3298 [D loss: 0.028201, acc.: 100.00%] [G loss: 3.056113]\n",
      "3299 [D loss: 0.029842, acc.: 100.00%] [G loss: 3.105684]\n",
      "3300 [D loss: 0.026653, acc.: 100.00%] [G loss: 2.756615]\n",
      "3301 [D loss: 0.058591, acc.: 99.22%] [G loss: 2.942803]\n",
      "3302 [D loss: 0.224039, acc.: 86.72%] [G loss: 5.341579]\n",
      "3303 [D loss: 1.077631, acc.: 81.25%] [G loss: 5.766869]\n",
      "3304 [D loss: 0.062123, acc.: 96.88%] [G loss: 6.733366]\n",
      "3305 [D loss: 0.008747, acc.: 100.00%] [G loss: 5.664863]\n",
      "3306 [D loss: 0.142001, acc.: 99.22%] [G loss: 4.518596]\n",
      "3307 [D loss: 0.052319, acc.: 100.00%] [G loss: 4.071407]\n",
      "3308 [D loss: 0.026702, acc.: 100.00%] [G loss: 3.535981]\n",
      "3309 [D loss: 0.086307, acc.: 98.44%] [G loss: 3.031400]\n",
      "3310 [D loss: 0.098635, acc.: 97.66%] [G loss: 3.615611]\n",
      "3311 [D loss: 0.325862, acc.: 89.06%] [G loss: 4.028877]\n",
      "3312 [D loss: 0.530905, acc.: 80.47%] [G loss: 6.418621]\n",
      "3313 [D loss: 0.522792, acc.: 86.72%] [G loss: 7.652617]\n",
      "3314 [D loss: 0.176384, acc.: 97.66%] [G loss: 6.178710]\n",
      "3315 [D loss: 0.178968, acc.: 98.44%] [G loss: 5.209552]\n",
      "3316 [D loss: 0.141109, acc.: 91.41%] [G loss: 4.772605]\n",
      "3317 [D loss: 0.069369, acc.: 98.44%] [G loss: 4.490297]\n",
      "3318 [D loss: 0.195399, acc.: 97.66%] [G loss: 4.431248]\n",
      "3319 [D loss: 0.210053, acc.: 96.09%] [G loss: 4.178424]\n",
      "3320 [D loss: 0.045313, acc.: 100.00%] [G loss: 3.501693]\n",
      "3321 [D loss: 0.221051, acc.: 99.22%] [G loss: 3.698379]\n",
      "3322 [D loss: 0.094781, acc.: 96.88%] [G loss: 3.629638]\n",
      "3323 [D loss: 0.070373, acc.: 99.22%] [G loss: 3.528013]\n",
      "3324 [D loss: 0.207684, acc.: 99.22%] [G loss: 3.435824]\n",
      "3325 [D loss: 0.166778, acc.: 99.22%] [G loss: 2.829171]\n",
      "3326 [D loss: 0.069913, acc.: 100.00%] [G loss: 2.293921]\n",
      "3327 [D loss: 0.041285, acc.: 100.00%] [G loss: 2.609665]\n",
      "3328 [D loss: 0.047337, acc.: 100.00%] [G loss: 2.241908]\n",
      "3329 [D loss: 0.039960, acc.: 100.00%] [G loss: 1.894635]\n",
      "3330 [D loss: 0.314036, acc.: 97.66%] [G loss: 2.294463]\n",
      "3331 [D loss: 0.059285, acc.: 100.00%] [G loss: 2.780568]\n",
      "3332 [D loss: 0.170815, acc.: 99.22%] [G loss: 2.723902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333 [D loss: 0.045289, acc.: 100.00%] [G loss: 2.509589]\n",
      "3334 [D loss: 0.041999, acc.: 100.00%] [G loss: 2.343612]\n",
      "3335 [D loss: 0.038314, acc.: 100.00%] [G loss: 2.546252]\n",
      "3336 [D loss: 0.041581, acc.: 99.22%] [G loss: 2.264537]\n",
      "3337 [D loss: 0.306085, acc.: 98.44%] [G loss: 2.704380]\n",
      "3338 [D loss: 0.145076, acc.: 99.22%] [G loss: 2.358148]\n",
      "3339 [D loss: 0.032540, acc.: 100.00%] [G loss: 2.174837]\n",
      "3340 [D loss: 0.157837, acc.: 99.22%] [G loss: 2.453185]\n",
      "3341 [D loss: 0.404315, acc.: 97.66%] [G loss: 2.298076]\n",
      "3342 [D loss: 0.038665, acc.: 99.22%] [G loss: 2.246801]\n",
      "3343 [D loss: 0.161740, acc.: 98.44%] [G loss: 2.140712]\n",
      "3344 [D loss: 0.030360, acc.: 100.00%] [G loss: 1.724710]\n",
      "3345 [D loss: 0.171112, acc.: 97.66%] [G loss: 2.087926]\n",
      "3346 [D loss: 0.147168, acc.: 99.22%] [G loss: 1.780881]\n",
      "3347 [D loss: 0.018577, acc.: 100.00%] [G loss: 1.998798]\n",
      "3348 [D loss: 0.288678, acc.: 97.66%] [G loss: 1.824612]\n",
      "3349 [D loss: 0.020631, acc.: 100.00%] [G loss: 1.696969]\n",
      "3350 [D loss: 0.017393, acc.: 100.00%] [G loss: 1.882396]\n",
      "3351 [D loss: 0.139033, acc.: 99.22%] [G loss: 1.864514]\n",
      "3352 [D loss: 0.017630, acc.: 100.00%] [G loss: 1.890659]\n",
      "3353 [D loss: 0.020792, acc.: 100.00%] [G loss: 2.035940]\n",
      "3354 [D loss: 0.021085, acc.: 100.00%] [G loss: 1.732484]\n",
      "3355 [D loss: 0.032293, acc.: 100.00%] [G loss: 1.993507]\n",
      "3356 [D loss: 0.156667, acc.: 98.44%] [G loss: 1.945178]\n",
      "3357 [D loss: 0.270573, acc.: 98.44%] [G loss: 2.059219]\n",
      "3358 [D loss: 0.141346, acc.: 99.22%] [G loss: 1.881385]\n",
      "3359 [D loss: 0.142432, acc.: 99.22%] [G loss: 1.921216]\n",
      "3360 [D loss: 0.019076, acc.: 100.00%] [G loss: 1.974548]\n",
      "3361 [D loss: 0.142199, acc.: 99.22%] [G loss: 2.113884]\n",
      "3362 [D loss: 0.145573, acc.: 99.22%] [G loss: 2.322278]\n",
      "3363 [D loss: 0.011147, acc.: 100.00%] [G loss: 2.079864]\n",
      "3364 [D loss: 0.145702, acc.: 99.22%] [G loss: 2.424921]\n",
      "3365 [D loss: 0.006818, acc.: 100.00%] [G loss: 2.747511]\n",
      "3366 [D loss: 0.055530, acc.: 99.22%] [G loss: 2.504995]\n",
      "3367 [D loss: 0.023934, acc.: 100.00%] [G loss: 2.423592]\n",
      "3368 [D loss: 0.143839, acc.: 99.22%] [G loss: 2.481285]\n",
      "3369 [D loss: 0.264263, acc.: 98.44%] [G loss: 2.514943]\n",
      "3370 [D loss: 0.393919, acc.: 96.88%] [G loss: 2.381706]\n",
      "3371 [D loss: 0.021624, acc.: 100.00%] [G loss: 2.413215]\n",
      "3372 [D loss: 0.019425, acc.: 100.00%] [G loss: 2.316347]\n",
      "3373 [D loss: 0.031073, acc.: 98.44%] [G loss: 2.211396]\n",
      "3374 [D loss: 0.020337, acc.: 100.00%] [G loss: 2.863449]\n",
      "3375 [D loss: 0.009813, acc.: 100.00%] [G loss: 2.462187]\n",
      "3376 [D loss: 0.015630, acc.: 100.00%] [G loss: 2.233928]\n",
      "3377 [D loss: 0.140970, acc.: 99.22%] [G loss: 2.371033]\n",
      "3378 [D loss: 0.143513, acc.: 99.22%] [G loss: 2.485022]\n",
      "3379 [D loss: 0.138985, acc.: 99.22%] [G loss: 2.219971]\n",
      "3380 [D loss: 0.012683, acc.: 100.00%] [G loss: 2.097225]\n",
      "3381 [D loss: 0.181117, acc.: 98.44%] [G loss: 2.023584]\n",
      "3382 [D loss: 0.392254, acc.: 97.66%] [G loss: 2.134414]\n",
      "3383 [D loss: 0.009811, acc.: 100.00%] [G loss: 2.230561]\n",
      "3384 [D loss: 0.014416, acc.: 100.00%] [G loss: 2.465672]\n",
      "3385 [D loss: 0.007829, acc.: 100.00%] [G loss: 2.414926]\n",
      "3386 [D loss: 0.015989, acc.: 100.00%] [G loss: 2.142568]\n",
      "3387 [D loss: 0.139372, acc.: 99.22%] [G loss: 2.230685]\n",
      "3388 [D loss: 0.011157, acc.: 100.00%] [G loss: 2.395950]\n",
      "3389 [D loss: 0.011016, acc.: 100.00%] [G loss: 2.015900]\n",
      "3390 [D loss: 0.265380, acc.: 98.44%] [G loss: 1.906956]\n",
      "3391 [D loss: 0.018019, acc.: 100.00%] [G loss: 1.941420]\n",
      "3392 [D loss: 0.273345, acc.: 98.44%] [G loss: 2.137804]\n",
      "3393 [D loss: 0.005747, acc.: 100.00%] [G loss: 2.328771]\n",
      "3394 [D loss: 0.010146, acc.: 100.00%] [G loss: 2.394559]\n",
      "3395 [D loss: 0.004877, acc.: 100.00%] [G loss: 2.464622]\n",
      "3396 [D loss: 0.134636, acc.: 99.22%] [G loss: 2.317775]\n",
      "3397 [D loss: 0.135578, acc.: 99.22%] [G loss: 2.562700]\n",
      "3398 [D loss: 0.136088, acc.: 99.22%] [G loss: 2.771933]\n",
      "3399 [D loss: 0.139856, acc.: 99.22%] [G loss: 2.673456]\n",
      "3400 [D loss: 0.175578, acc.: 98.44%] [G loss: 2.574437]\n",
      "3401 [D loss: 0.142183, acc.: 98.44%] [G loss: 2.513469]\n",
      "3402 [D loss: 0.392075, acc.: 97.66%] [G loss: 2.242765]\n",
      "3403 [D loss: 0.285573, acc.: 97.66%] [G loss: 2.444654]\n",
      "3404 [D loss: 0.036576, acc.: 99.22%] [G loss: 2.341387]\n",
      "3405 [D loss: 0.009835, acc.: 100.00%] [G loss: 2.452171]\n",
      "3406 [D loss: 0.133427, acc.: 99.22%] [G loss: 2.874702]\n",
      "3407 [D loss: 0.264413, acc.: 98.44%] [G loss: 2.171016]\n",
      "3408 [D loss: 0.009019, acc.: 100.00%] [G loss: 2.295222]\n",
      "3409 [D loss: 0.131895, acc.: 99.22%] [G loss: 2.119189]\n",
      "3410 [D loss: 0.267817, acc.: 98.44%] [G loss: 2.103461]\n",
      "3411 [D loss: 0.009240, acc.: 100.00%] [G loss: 2.216101]\n",
      "3412 [D loss: 0.139394, acc.: 99.22%] [G loss: 2.474710]\n",
      "3413 [D loss: 0.260863, acc.: 98.44%] [G loss: 2.369379]\n",
      "3414 [D loss: 0.133322, acc.: 99.22%] [G loss: 2.717502]\n",
      "3415 [D loss: 0.166316, acc.: 98.44%] [G loss: 2.386336]\n",
      "3416 [D loss: 0.135966, acc.: 99.22%] [G loss: 2.868349]\n",
      "3417 [D loss: 0.008360, acc.: 100.00%] [G loss: 2.288165]\n",
      "3418 [D loss: 0.009678, acc.: 100.00%] [G loss: 2.164745]\n",
      "3419 [D loss: 0.261398, acc.: 98.44%] [G loss: 2.153368]\n",
      "3420 [D loss: 0.012572, acc.: 100.00%] [G loss: 2.266805]\n",
      "3421 [D loss: 0.007505, acc.: 100.00%] [G loss: 2.606069]\n",
      "3422 [D loss: 0.130312, acc.: 99.22%] [G loss: 2.768312]\n",
      "3423 [D loss: 0.139642, acc.: 99.22%] [G loss: 2.436498]\n",
      "3424 [D loss: 0.012656, acc.: 100.00%] [G loss: 2.876642]\n",
      "3425 [D loss: 0.132646, acc.: 99.22%] [G loss: 2.475774]\n",
      "3426 [D loss: 0.006346, acc.: 100.00%] [G loss: 2.403725]\n",
      "3427 [D loss: 0.136167, acc.: 99.22%] [G loss: 2.482009]\n",
      "3428 [D loss: 0.008446, acc.: 100.00%] [G loss: 2.506677]\n",
      "3429 [D loss: 0.134410, acc.: 99.22%] [G loss: 2.712179]\n",
      "3430 [D loss: 0.298354, acc.: 97.66%] [G loss: 3.349027]\n",
      "3431 [D loss: 0.133541, acc.: 99.22%] [G loss: 2.479107]\n",
      "3432 [D loss: 0.009138, acc.: 100.00%] [G loss: 2.372871]\n",
      "3433 [D loss: 0.262047, acc.: 98.44%] [G loss: 2.539916]\n",
      "3434 [D loss: 0.008480, acc.: 100.00%] [G loss: 2.617326]\n",
      "3435 [D loss: 0.145287, acc.: 98.44%] [G loss: 2.340872]\n",
      "3436 [D loss: 0.006664, acc.: 100.00%] [G loss: 2.502428]\n",
      "3437 [D loss: 0.129876, acc.: 99.22%] [G loss: 2.716649]\n",
      "3438 [D loss: 0.260450, acc.: 98.44%] [G loss: 2.686543]\n",
      "3439 [D loss: 0.007030, acc.: 100.00%] [G loss: 2.610246]\n",
      "3440 [D loss: 0.132217, acc.: 99.22%] [G loss: 2.451855]\n",
      "3441 [D loss: 0.137468, acc.: 99.22%] [G loss: 2.395200]\n",
      "3442 [D loss: 0.265838, acc.: 98.44%] [G loss: 2.424158]\n",
      "3443 [D loss: 0.177157, acc.: 98.44%] [G loss: 2.634860]\n",
      "3444 [D loss: 0.131147, acc.: 99.22%] [G loss: 2.687874]\n",
      "3445 [D loss: 0.381611, acc.: 97.66%] [G loss: 2.788412]\n",
      "3446 [D loss: 0.129044, acc.: 99.22%] [G loss: 2.720925]\n",
      "3447 [D loss: 0.005312, acc.: 100.00%] [G loss: 2.686535]\n",
      "3448 [D loss: 0.132446, acc.: 99.22%] [G loss: 2.593606]\n",
      "3449 [D loss: 0.133357, acc.: 99.22%] [G loss: 2.544155]\n",
      "3450 [D loss: 0.133348, acc.: 99.22%] [G loss: 2.482639]\n",
      "3451 [D loss: 0.008376, acc.: 100.00%] [G loss: 2.303101]\n",
      "3452 [D loss: 0.134747, acc.: 99.22%] [G loss: 2.587511]\n",
      "3453 [D loss: 0.256693, acc.: 98.44%] [G loss: 2.830119]\n",
      "3454 [D loss: 0.004608, acc.: 100.00%] [G loss: 2.675227]\n",
      "3455 [D loss: 0.413119, acc.: 96.88%] [G loss: 2.513315]\n",
      "3456 [D loss: 0.005884, acc.: 100.00%] [G loss: 2.336707]\n",
      "3457 [D loss: 0.005243, acc.: 100.00%] [G loss: 2.356722]\n",
      "3458 [D loss: 0.135228, acc.: 99.22%] [G loss: 2.297452]\n",
      "3459 [D loss: 0.133575, acc.: 99.22%] [G loss: 2.155074]\n",
      "3460 [D loss: 0.007913, acc.: 100.00%] [G loss: 2.442384]\n",
      "3461 [D loss: 0.258027, acc.: 98.44%] [G loss: 2.595584]\n",
      "3462 [D loss: 0.507694, acc.: 96.88%] [G loss: 2.834714]\n",
      "3463 [D loss: 0.131659, acc.: 99.22%] [G loss: 2.735361]\n",
      "3464 [D loss: 0.003055, acc.: 100.00%] [G loss: 3.060701]\n",
      "3465 [D loss: 0.006148, acc.: 100.00%] [G loss: 3.096051]\n",
      "3466 [D loss: 0.131071, acc.: 99.22%] [G loss: 2.719241]\n",
      "3467 [D loss: 0.131849, acc.: 99.22%] [G loss: 2.279304]\n",
      "3468 [D loss: 0.135179, acc.: 99.22%] [G loss: 2.316057]\n",
      "3469 [D loss: 0.005085, acc.: 100.00%] [G loss: 2.556081]\n",
      "3470 [D loss: 0.004354, acc.: 100.00%] [G loss: 3.140268]\n",
      "3471 [D loss: 0.129061, acc.: 99.22%] [G loss: 3.777820]\n",
      "3472 [D loss: 0.130667, acc.: 99.22%] [G loss: 2.792403]\n",
      "3473 [D loss: 0.258698, acc.: 98.44%] [G loss: 2.525745]\n",
      "3474 [D loss: 0.261037, acc.: 98.44%] [G loss: 2.490743]\n",
      "3475 [D loss: 0.136784, acc.: 99.22%] [G loss: 2.858438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3476 [D loss: 0.003631, acc.: 100.00%] [G loss: 3.181812]\n",
      "3477 [D loss: 0.131536, acc.: 99.22%] [G loss: 3.406800]\n",
      "3478 [D loss: 0.002325, acc.: 100.00%] [G loss: 3.168016]\n",
      "3479 [D loss: 0.131896, acc.: 99.22%] [G loss: 2.884522]\n",
      "3480 [D loss: 0.131131, acc.: 99.22%] [G loss: 2.632545]\n",
      "3481 [D loss: 0.132290, acc.: 99.22%] [G loss: 2.527045]\n",
      "3482 [D loss: 0.005981, acc.: 100.00%] [G loss: 2.671059]\n",
      "3483 [D loss: 0.131428, acc.: 99.22%] [G loss: 2.451877]\n",
      "3484 [D loss: 0.002711, acc.: 100.00%] [G loss: 2.672624]\n",
      "3485 [D loss: 0.004053, acc.: 100.00%] [G loss: 2.601728]\n",
      "3486 [D loss: 0.381074, acc.: 97.66%] [G loss: 2.845077]\n",
      "3487 [D loss: 0.129991, acc.: 99.22%] [G loss: 3.152950]\n",
      "3488 [D loss: 0.131915, acc.: 99.22%] [G loss: 3.057860]\n",
      "3489 [D loss: 0.257405, acc.: 98.44%] [G loss: 2.703248]\n",
      "3490 [D loss: 0.005480, acc.: 100.00%] [G loss: 2.594514]\n",
      "3491 [D loss: 0.258540, acc.: 98.44%] [G loss: 2.457742]\n",
      "3492 [D loss: 0.635865, acc.: 96.09%] [G loss: 2.475234]\n",
      "3493 [D loss: 0.132656, acc.: 99.22%] [G loss: 2.750645]\n",
      "3494 [D loss: 0.005022, acc.: 100.00%] [G loss: 2.750743]\n",
      "3495 [D loss: 0.129199, acc.: 99.22%] [G loss: 2.954487]\n",
      "3496 [D loss: 0.004141, acc.: 100.00%] [G loss: 3.164591]\n",
      "3497 [D loss: 0.135574, acc.: 98.44%] [G loss: 3.017180]\n",
      "3498 [D loss: 0.004523, acc.: 100.00%] [G loss: 3.199746]\n",
      "3499 [D loss: 0.254719, acc.: 98.44%] [G loss: 2.951808]\n",
      "3500 [D loss: 0.258192, acc.: 98.44%] [G loss: 2.586938]\n",
      "3501 [D loss: 0.132644, acc.: 99.22%] [G loss: 2.624973]\n",
      "3502 [D loss: 0.133246, acc.: 99.22%] [G loss: 2.596521]\n",
      "3503 [D loss: 0.005708, acc.: 100.00%] [G loss: 2.574147]\n",
      "3504 [D loss: 0.255994, acc.: 98.44%] [G loss: 2.763773]\n",
      "3505 [D loss: 0.004114, acc.: 100.00%] [G loss: 3.198152]\n",
      "3506 [D loss: 0.255103, acc.: 98.44%] [G loss: 3.291944]\n",
      "3507 [D loss: 0.381664, acc.: 97.66%] [G loss: 3.107406]\n",
      "3508 [D loss: 0.257732, acc.: 98.44%] [G loss: 2.651045]\n",
      "3509 [D loss: 0.132416, acc.: 99.22%] [G loss: 2.903322]\n",
      "3510 [D loss: 0.129123, acc.: 99.22%] [G loss: 2.865161]\n",
      "3511 [D loss: 0.142288, acc.: 98.44%] [G loss: 3.291813]\n",
      "3512 [D loss: 0.380245, acc.: 97.66%] [G loss: 3.225645]\n",
      "3513 [D loss: 0.003754, acc.: 100.00%] [G loss: 2.844047]\n",
      "3514 [D loss: 0.004786, acc.: 100.00%] [G loss: 2.893482]\n",
      "3515 [D loss: 0.258863, acc.: 98.44%] [G loss: 2.948516]\n",
      "3516 [D loss: 0.132164, acc.: 99.22%] [G loss: 2.756501]\n",
      "3517 [D loss: 0.130389, acc.: 99.22%] [G loss: 2.827169]\n",
      "3518 [D loss: 0.382253, acc.: 97.66%] [G loss: 2.910889]\n",
      "3519 [D loss: 0.131109, acc.: 99.22%] [G loss: 2.807387]\n",
      "3520 [D loss: 0.002546, acc.: 100.00%] [G loss: 3.074801]\n",
      "3521 [D loss: 0.382630, acc.: 97.66%] [G loss: 3.043445]\n",
      "3522 [D loss: 0.129101, acc.: 99.22%] [G loss: 3.076386]\n",
      "3523 [D loss: 0.004429, acc.: 100.00%] [G loss: 2.953526]\n",
      "3524 [D loss: 0.004623, acc.: 100.00%] [G loss: 3.067148]\n",
      "3525 [D loss: 0.255310, acc.: 98.44%] [G loss: 3.028880]\n",
      "3526 [D loss: 0.257606, acc.: 98.44%] [G loss: 2.862347]\n",
      "3527 [D loss: 0.004053, acc.: 100.00%] [G loss: 2.973594]\n",
      "3528 [D loss: 0.254441, acc.: 98.44%] [G loss: 3.405416]\n",
      "3529 [D loss: 0.254728, acc.: 98.44%] [G loss: 3.590194]\n",
      "3530 [D loss: 0.130802, acc.: 99.22%] [G loss: 3.183367]\n",
      "3531 [D loss: 0.129366, acc.: 99.22%] [G loss: 3.121420]\n",
      "3532 [D loss: 0.129293, acc.: 99.22%] [G loss: 2.952843]\n",
      "3533 [D loss: 0.130599, acc.: 99.22%] [G loss: 3.109272]\n",
      "3534 [D loss: 0.131875, acc.: 99.22%] [G loss: 3.071570]\n",
      "3535 [D loss: 0.258916, acc.: 98.44%] [G loss: 3.321544]\n",
      "3536 [D loss: 0.384564, acc.: 97.66%] [G loss: 3.185286]\n",
      "3537 [D loss: 0.130022, acc.: 99.22%] [G loss: 3.106480]\n",
      "3538 [D loss: 0.003951, acc.: 100.00%] [G loss: 3.085628]\n",
      "3539 [D loss: 0.254393, acc.: 98.44%] [G loss: 3.401036]\n",
      "3540 [D loss: 0.256408, acc.: 98.44%] [G loss: 3.229786]\n",
      "3541 [D loss: 0.006795, acc.: 100.00%] [G loss: 3.051251]\n",
      "3542 [D loss: 0.128811, acc.: 99.22%] [G loss: 3.291925]\n",
      "3543 [D loss: 0.004008, acc.: 100.00%] [G loss: 2.885411]\n",
      "3544 [D loss: 0.004255, acc.: 100.00%] [G loss: 3.022754]\n",
      "3545 [D loss: 0.381735, acc.: 97.66%] [G loss: 2.858096]\n",
      "3546 [D loss: 0.129073, acc.: 99.22%] [G loss: 2.927858]\n",
      "3547 [D loss: 0.002719, acc.: 100.00%] [G loss: 3.140657]\n",
      "3548 [D loss: 0.129317, acc.: 99.22%] [G loss: 3.042234]\n",
      "3549 [D loss: 0.129653, acc.: 99.22%] [G loss: 3.191347]\n",
      "3550 [D loss: 0.002825, acc.: 100.00%] [G loss: 3.094198]\n",
      "3551 [D loss: 0.005174, acc.: 100.00%] [G loss: 2.857269]\n",
      "3552 [D loss: 0.382476, acc.: 97.66%] [G loss: 3.232510]\n",
      "3553 [D loss: 0.257148, acc.: 98.44%] [G loss: 2.955593]\n",
      "3554 [D loss: 0.003740, acc.: 100.00%] [G loss: 2.837642]\n",
      "3555 [D loss: 0.128122, acc.: 99.22%] [G loss: 3.040013]\n",
      "3556 [D loss: 0.128473, acc.: 99.22%] [G loss: 3.256038]\n",
      "3557 [D loss: 0.128743, acc.: 99.22%] [G loss: 3.115312]\n",
      "3558 [D loss: 0.255344, acc.: 98.44%] [G loss: 2.943300]\n",
      "3559 [D loss: 0.129728, acc.: 99.22%] [G loss: 2.938099]\n",
      "3560 [D loss: 0.004962, acc.: 100.00%] [G loss: 2.936336]\n",
      "3561 [D loss: 0.004497, acc.: 100.00%] [G loss: 2.702082]\n",
      "3562 [D loss: 0.130118, acc.: 99.22%] [G loss: 2.873070]\n",
      "3563 [D loss: 0.003656, acc.: 100.00%] [G loss: 3.017011]\n",
      "3564 [D loss: 0.255867, acc.: 98.44%] [G loss: 2.970557]\n",
      "3565 [D loss: 0.128031, acc.: 99.22%] [G loss: 3.499669]\n",
      "3566 [D loss: 0.003098, acc.: 100.00%] [G loss: 3.761338]\n",
      "3567 [D loss: 0.128221, acc.: 99.22%] [G loss: 3.144015]\n",
      "3568 [D loss: 0.255072, acc.: 98.44%] [G loss: 2.976503]\n",
      "3569 [D loss: 0.169659, acc.: 98.44%] [G loss: 2.827096]\n",
      "3570 [D loss: 0.022494, acc.: 99.22%] [G loss: 2.696123]\n",
      "3571 [D loss: 0.004799, acc.: 100.00%] [G loss: 2.913128]\n",
      "3572 [D loss: 0.128378, acc.: 99.22%] [G loss: 3.307387]\n",
      "3573 [D loss: 0.128227, acc.: 99.22%] [G loss: 3.400041]\n",
      "3574 [D loss: 0.128410, acc.: 99.22%] [G loss: 3.238959]\n",
      "3575 [D loss: 0.005293, acc.: 100.00%] [G loss: 2.889129]\n",
      "3576 [D loss: 0.004127, acc.: 100.00%] [G loss: 2.923947]\n",
      "3577 [D loss: 0.130575, acc.: 99.22%] [G loss: 3.070872]\n",
      "3578 [D loss: 0.254808, acc.: 98.44%] [G loss: 2.795171]\n",
      "3579 [D loss: 0.128786, acc.: 99.22%] [G loss: 2.747222]\n",
      "3580 [D loss: 0.002763, acc.: 100.00%] [G loss: 2.849167]\n",
      "3581 [D loss: 0.169320, acc.: 98.44%] [G loss: 2.357612]\n",
      "3582 [D loss: 0.007147, acc.: 100.00%] [G loss: 2.438848]\n",
      "3583 [D loss: 0.008770, acc.: 100.00%] [G loss: 2.403215]\n",
      "3584 [D loss: 0.007222, acc.: 100.00%] [G loss: 2.528989]\n",
      "3585 [D loss: 0.257108, acc.: 98.44%] [G loss: 2.844978]\n",
      "3586 [D loss: 0.128823, acc.: 99.22%] [G loss: 2.863085]\n",
      "3587 [D loss: 0.003558, acc.: 100.00%] [G loss: 2.921260]\n",
      "3588 [D loss: 0.004594, acc.: 100.00%] [G loss: 2.765920]\n",
      "3589 [D loss: 0.007108, acc.: 100.00%] [G loss: 2.800215]\n",
      "3590 [D loss: 0.130532, acc.: 99.22%] [G loss: 2.956035]\n",
      "3591 [D loss: 0.005825, acc.: 100.00%] [G loss: 3.012302]\n",
      "3592 [D loss: 0.006242, acc.: 100.00%] [G loss: 2.834013]\n",
      "3593 [D loss: 0.130642, acc.: 99.22%] [G loss: 3.257888]\n",
      "3594 [D loss: 0.128495, acc.: 99.22%] [G loss: 3.213590]\n",
      "3595 [D loss: 0.002571, acc.: 100.00%] [G loss: 3.460112]\n",
      "3596 [D loss: 0.508212, acc.: 96.88%] [G loss: 3.023098]\n",
      "3597 [D loss: 0.256393, acc.: 98.44%] [G loss: 2.816988]\n",
      "3598 [D loss: 0.003870, acc.: 100.00%] [G loss: 2.776013]\n",
      "3599 [D loss: 0.256175, acc.: 98.44%] [G loss: 2.958488]\n",
      "3600 [D loss: 0.004382, acc.: 100.00%] [G loss: 2.988874]\n",
      "3601 [D loss: 0.005067, acc.: 100.00%] [G loss: 2.866486]\n",
      "3602 [D loss: 0.257034, acc.: 98.44%] [G loss: 3.156548]\n",
      "3603 [D loss: 0.003090, acc.: 100.00%] [G loss: 3.015192]\n",
      "3604 [D loss: 0.129870, acc.: 99.22%] [G loss: 3.162009]\n",
      "3605 [D loss: 0.381988, acc.: 97.66%] [G loss: 3.139966]\n",
      "3606 [D loss: 0.130909, acc.: 99.22%] [G loss: 2.881297]\n",
      "3607 [D loss: 0.142007, acc.: 98.44%] [G loss: 2.883047]\n",
      "3608 [D loss: 0.381384, acc.: 97.66%] [G loss: 2.849242]\n",
      "3609 [D loss: 0.130830, acc.: 99.22%] [G loss: 2.858283]\n",
      "3610 [D loss: 0.256495, acc.: 98.44%] [G loss: 3.054056]\n",
      "3611 [D loss: 0.003160, acc.: 100.00%] [G loss: 3.155500]\n",
      "3612 [D loss: 0.003030, acc.: 100.00%] [G loss: 3.131659]\n",
      "3613 [D loss: 0.381651, acc.: 97.66%] [G loss: 3.434455]\n",
      "3614 [D loss: 0.004775, acc.: 100.00%] [G loss: 2.767078]\n",
      "3615 [D loss: 0.005706, acc.: 100.00%] [G loss: 2.748773]\n",
      "3616 [D loss: 0.130519, acc.: 99.22%] [G loss: 2.904941]\n",
      "3617 [D loss: 0.130091, acc.: 99.22%] [G loss: 2.647489]\n",
      "3618 [D loss: 0.005427, acc.: 100.00%] [G loss: 2.710870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3619 [D loss: 0.005055, acc.: 100.00%] [G loss: 2.669683]\n",
      "3620 [D loss: 0.002939, acc.: 100.00%] [G loss: 3.295289]\n",
      "3621 [D loss: 0.003341, acc.: 100.00%] [G loss: 3.095693]\n",
      "3622 [D loss: 0.003238, acc.: 100.00%] [G loss: 3.020647]\n",
      "3623 [D loss: 0.004264, acc.: 100.00%] [G loss: 2.758238]\n",
      "3624 [D loss: 0.131044, acc.: 99.22%] [G loss: 2.558695]\n",
      "3625 [D loss: 0.004361, acc.: 100.00%] [G loss: 2.587635]\n",
      "3626 [D loss: 0.004946, acc.: 100.00%] [G loss: 2.736785]\n",
      "3627 [D loss: 0.130772, acc.: 99.22%] [G loss: 2.702186]\n",
      "3628 [D loss: 0.004013, acc.: 100.00%] [G loss: 2.901571]\n",
      "3629 [D loss: 0.004612, acc.: 100.00%] [G loss: 2.838457]\n",
      "3630 [D loss: 0.015888, acc.: 99.22%] [G loss: 3.129494]\n",
      "3631 [D loss: 0.256441, acc.: 98.44%] [G loss: 2.908387]\n",
      "3632 [D loss: 0.129028, acc.: 99.22%] [G loss: 3.213625]\n",
      "3633 [D loss: 0.005110, acc.: 100.00%] [G loss: 2.933588]\n",
      "3634 [D loss: 0.003862, acc.: 100.00%] [G loss: 2.976747]\n",
      "3635 [D loss: 0.004104, acc.: 100.00%] [G loss: 2.907045]\n",
      "3636 [D loss: 0.003348, acc.: 100.00%] [G loss: 2.799763]\n",
      "3637 [D loss: 0.004745, acc.: 100.00%] [G loss: 2.757683]\n",
      "3638 [D loss: 0.003519, acc.: 100.00%] [G loss: 2.966903]\n",
      "3639 [D loss: 0.129914, acc.: 99.22%] [G loss: 3.071475]\n",
      "3640 [D loss: 0.380366, acc.: 97.66%] [G loss: 2.971089]\n",
      "3641 [D loss: 0.255069, acc.: 98.44%] [G loss: 3.078759]\n",
      "3642 [D loss: 0.005112, acc.: 100.00%] [G loss: 2.803637]\n",
      "3643 [D loss: 0.005637, acc.: 100.00%] [G loss: 2.595268]\n",
      "3644 [D loss: 0.133023, acc.: 99.22%] [G loss: 2.537024]\n",
      "3645 [D loss: 0.131774, acc.: 99.22%] [G loss: 2.690876]\n",
      "3646 [D loss: 0.128828, acc.: 99.22%] [G loss: 2.807629]\n",
      "3647 [D loss: 0.129204, acc.: 99.22%] [G loss: 2.720969]\n",
      "3648 [D loss: 0.008548, acc.: 99.22%] [G loss: 2.965562]\n",
      "3649 [D loss: 0.002290, acc.: 100.00%] [G loss: 3.079996]\n",
      "3650 [D loss: 0.002616, acc.: 100.00%] [G loss: 3.664240]\n",
      "3651 [D loss: 0.003646, acc.: 100.00%] [G loss: 3.360282]\n",
      "3652 [D loss: 0.130085, acc.: 99.22%] [G loss: 3.216717]\n",
      "3653 [D loss: 0.132171, acc.: 99.22%] [G loss: 3.267519]\n",
      "3654 [D loss: 0.134874, acc.: 99.22%] [G loss: 3.219856]\n",
      "3655 [D loss: 0.260182, acc.: 98.44%] [G loss: 2.991462]\n",
      "3656 [D loss: 0.003887, acc.: 100.00%] [G loss: 3.083415]\n",
      "3657 [D loss: 0.003305, acc.: 100.00%] [G loss: 3.099164]\n",
      "3658 [D loss: 0.002502, acc.: 100.00%] [G loss: 3.302242]\n",
      "3659 [D loss: 0.128218, acc.: 99.22%] [G loss: 3.689721]\n",
      "3660 [D loss: 0.254011, acc.: 98.44%] [G loss: 3.868453]\n",
      "3661 [D loss: 0.255075, acc.: 98.44%] [G loss: 3.176730]\n",
      "3662 [D loss: 0.129246, acc.: 99.22%] [G loss: 3.238847]\n",
      "3663 [D loss: 0.254283, acc.: 98.44%] [G loss: 3.452004]\n",
      "3664 [D loss: 0.130349, acc.: 99.22%] [G loss: 3.390089]\n",
      "3665 [D loss: 0.003899, acc.: 100.00%] [G loss: 2.888402]\n",
      "3666 [D loss: 0.004079, acc.: 100.00%] [G loss: 2.811885]\n",
      "3667 [D loss: 0.256658, acc.: 98.44%] [G loss: 2.986892]\n",
      "3668 [D loss: 0.002483, acc.: 100.00%] [G loss: 3.210108]\n",
      "3669 [D loss: 0.003589, acc.: 100.00%] [G loss: 3.055860]\n",
      "3670 [D loss: 0.129657, acc.: 99.22%] [G loss: 2.846705]\n",
      "3671 [D loss: 0.004407, acc.: 100.00%] [G loss: 2.833663]\n",
      "3672 [D loss: 0.134561, acc.: 99.22%] [G loss: 2.680887]\n",
      "3673 [D loss: 0.130017, acc.: 99.22%] [G loss: 2.740648]\n",
      "3674 [D loss: 0.128327, acc.: 99.22%] [G loss: 3.072757]\n",
      "3675 [D loss: 0.003163, acc.: 100.00%] [G loss: 2.848643]\n",
      "3676 [D loss: 0.004224, acc.: 100.00%] [G loss: 2.777019]\n",
      "3677 [D loss: 0.004931, acc.: 100.00%] [G loss: 2.738912]\n",
      "3678 [D loss: 0.256354, acc.: 98.44%] [G loss: 3.205436]\n",
      "3679 [D loss: 0.380626, acc.: 97.66%] [G loss: 2.825884]\n",
      "3680 [D loss: 0.002269, acc.: 100.00%] [G loss: 2.962996]\n",
      "3681 [D loss: 0.128875, acc.: 99.22%] [G loss: 2.885863]\n",
      "3682 [D loss: 0.003387, acc.: 100.00%] [G loss: 2.678302]\n",
      "3683 [D loss: 0.129307, acc.: 99.22%] [G loss: 2.667492]\n",
      "3684 [D loss: 0.003409, acc.: 100.00%] [G loss: 2.462361]\n",
      "3685 [D loss: 0.129196, acc.: 99.22%] [G loss: 2.675267]\n",
      "3686 [D loss: 0.003602, acc.: 100.00%] [G loss: 2.645754]\n",
      "3687 [D loss: 0.003032, acc.: 100.00%] [G loss: 2.958880]\n",
      "3688 [D loss: 0.380997, acc.: 97.66%] [G loss: 2.891115]\n",
      "3689 [D loss: 0.130660, acc.: 99.22%] [G loss: 2.905371]\n",
      "3690 [D loss: 0.255097, acc.: 98.44%] [G loss: 2.730503]\n",
      "3691 [D loss: 0.130397, acc.: 99.22%] [G loss: 2.586548]\n",
      "3692 [D loss: 0.258216, acc.: 98.44%] [G loss: 2.443275]\n",
      "3693 [D loss: 0.506653, acc.: 96.88%] [G loss: 2.648641]\n",
      "3694 [D loss: 0.129221, acc.: 99.22%] [G loss: 2.578057]\n",
      "3695 [D loss: 0.129581, acc.: 99.22%] [G loss: 2.757020]\n",
      "3696 [D loss: 0.003128, acc.: 100.00%] [G loss: 2.960997]\n",
      "3697 [D loss: 0.004889, acc.: 100.00%] [G loss: 2.935596]\n",
      "3698 [D loss: 0.141489, acc.: 98.44%] [G loss: 2.760158]\n",
      "3699 [D loss: 0.003958, acc.: 100.00%] [G loss: 2.844007]\n",
      "3700 [D loss: 0.256751, acc.: 98.44%] [G loss: 2.952849]\n",
      "3701 [D loss: 0.254591, acc.: 98.44%] [G loss: 2.927083]\n",
      "3702 [D loss: 0.129593, acc.: 99.22%] [G loss: 3.011077]\n",
      "3703 [D loss: 0.128431, acc.: 99.22%] [G loss: 2.840470]\n",
      "3704 [D loss: 0.128352, acc.: 99.22%] [G loss: 2.993714]\n",
      "3705 [D loss: 0.002010, acc.: 100.00%] [G loss: 2.952989]\n",
      "3706 [D loss: 0.129199, acc.: 99.22%] [G loss: 2.671154]\n",
      "3707 [D loss: 0.255266, acc.: 98.44%] [G loss: 2.520665]\n",
      "3708 [D loss: 0.129717, acc.: 99.22%] [G loss: 2.675220]\n",
      "3709 [D loss: 0.003080, acc.: 100.00%] [G loss: 2.506549]\n",
      "3710 [D loss: 0.256248, acc.: 98.44%] [G loss: 2.720340]\n",
      "3711 [D loss: 0.002240, acc.: 100.00%] [G loss: 2.943787]\n",
      "3712 [D loss: 0.003367, acc.: 100.00%] [G loss: 2.493246]\n",
      "3713 [D loss: 0.128959, acc.: 99.22%] [G loss: 2.588393]\n",
      "3714 [D loss: 0.131956, acc.: 99.22%] [G loss: 3.141262]\n",
      "3715 [D loss: 0.002422, acc.: 100.00%] [G loss: 3.106981]\n",
      "3716 [D loss: 0.129886, acc.: 99.22%] [G loss: 2.674283]\n",
      "3717 [D loss: 0.130498, acc.: 99.22%] [G loss: 2.478600]\n",
      "3718 [D loss: 0.255199, acc.: 98.44%] [G loss: 2.429086]\n",
      "3719 [D loss: 0.129436, acc.: 99.22%] [G loss: 2.629539]\n",
      "3720 [D loss: 0.254458, acc.: 98.44%] [G loss: 2.933489]\n",
      "3721 [D loss: 0.003160, acc.: 100.00%] [G loss: 3.043065]\n",
      "3722 [D loss: 0.129467, acc.: 99.22%] [G loss: 2.717680]\n",
      "3723 [D loss: 0.384474, acc.: 97.66%] [G loss: 3.174648]\n",
      "3724 [D loss: 0.128998, acc.: 99.22%] [G loss: 3.064325]\n",
      "3725 [D loss: 0.255114, acc.: 98.44%] [G loss: 2.825894]\n",
      "3726 [D loss: 0.002497, acc.: 100.00%] [G loss: 3.007554]\n",
      "3727 [D loss: 0.128910, acc.: 99.22%] [G loss: 3.133614]\n",
      "3728 [D loss: 0.128728, acc.: 99.22%] [G loss: 2.902760]\n",
      "3729 [D loss: 0.130662, acc.: 99.22%] [G loss: 2.771233]\n",
      "3730 [D loss: 0.003434, acc.: 100.00%] [G loss: 3.189777]\n",
      "3731 [D loss: 0.002378, acc.: 100.00%] [G loss: 3.297006]\n",
      "3732 [D loss: 0.129825, acc.: 99.22%] [G loss: 3.174840]\n",
      "3733 [D loss: 0.003013, acc.: 100.00%] [G loss: 2.859526]\n",
      "3734 [D loss: 0.129595, acc.: 99.22%] [G loss: 2.836477]\n",
      "3735 [D loss: 0.254626, acc.: 98.44%] [G loss: 3.038190]\n",
      "3736 [D loss: 0.002503, acc.: 100.00%] [G loss: 3.141439]\n",
      "3737 [D loss: 0.254909, acc.: 98.44%] [G loss: 3.123473]\n",
      "3738 [D loss: 0.131373, acc.: 99.22%] [G loss: 3.018476]\n",
      "3739 [D loss: 0.002938, acc.: 100.00%] [G loss: 3.237029]\n",
      "3740 [D loss: 0.254802, acc.: 98.44%] [G loss: 3.380622]\n",
      "3741 [D loss: 0.128391, acc.: 99.22%] [G loss: 3.239038]\n",
      "3742 [D loss: 0.003371, acc.: 100.00%] [G loss: 3.284418]\n",
      "3743 [D loss: 0.128223, acc.: 99.22%] [G loss: 3.182162]\n",
      "3744 [D loss: 0.255156, acc.: 98.44%] [G loss: 3.243537]\n",
      "3745 [D loss: 0.002936, acc.: 100.00%] [G loss: 3.135124]\n",
      "3746 [D loss: 0.128720, acc.: 99.22%] [G loss: 3.118919]\n",
      "3747 [D loss: 0.005373, acc.: 100.00%] [G loss: 2.864440]\n",
      "3748 [D loss: 0.131436, acc.: 99.22%] [G loss: 2.988193]\n",
      "3749 [D loss: 0.002919, acc.: 100.00%] [G loss: 3.133148]\n",
      "3750 [D loss: 0.002247, acc.: 100.00%] [G loss: 3.431832]\n",
      "3751 [D loss: 0.379955, acc.: 97.66%] [G loss: 3.095889]\n",
      "3752 [D loss: 0.128772, acc.: 99.22%] [G loss: 3.035722]\n",
      "3753 [D loss: 0.128509, acc.: 99.22%] [G loss: 3.028401]\n",
      "3754 [D loss: 0.003098, acc.: 100.00%] [G loss: 3.297237]\n",
      "3755 [D loss: 0.007113, acc.: 100.00%] [G loss: 3.338173]\n",
      "3756 [D loss: 0.002418, acc.: 100.00%] [G loss: 3.335139]\n",
      "3757 [D loss: 0.128162, acc.: 99.22%] [G loss: 3.419764]\n",
      "3758 [D loss: 0.001764, acc.: 100.00%] [G loss: 3.557981]\n",
      "3759 [D loss: 0.128090, acc.: 99.22%] [G loss: 3.308749]\n",
      "3760 [D loss: 0.002335, acc.: 100.00%] [G loss: 3.227306]\n",
      "3761 [D loss: 0.129617, acc.: 99.22%] [G loss: 3.068964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3762 [D loss: 0.129352, acc.: 99.22%] [G loss: 3.013406]\n",
      "3763 [D loss: 0.024065, acc.: 99.22%] [G loss: 2.173384]\n",
      "3764 [D loss: 0.017950, acc.: 100.00%] [G loss: 2.743493]\n",
      "3765 [D loss: 0.038515, acc.: 98.44%] [G loss: 4.637526]\n",
      "3766 [D loss: 0.435385, acc.: 85.16%] [G loss: 7.671311]\n",
      "3767 [D loss: 2.748100, acc.: 81.25%] [G loss: 9.864925]\n",
      "3768 [D loss: 2.195925, acc.: 85.16%] [G loss: 12.257677]\n",
      "3769 [D loss: 0.710827, acc.: 93.75%] [G loss: 14.422079]\n",
      "3770 [D loss: 0.378230, acc.: 96.88%] [G loss: 14.822724]\n",
      "3771 [D loss: 0.189960, acc.: 96.88%] [G loss: 15.676813]\n",
      "3772 [D loss: 0.086569, acc.: 98.44%] [G loss: 16.082630]\n",
      "3773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.110493]\n",
      "3774 [D loss: 0.125923, acc.: 99.22%] [G loss: 16.009245]\n",
      "3775 [D loss: 0.125923, acc.: 99.22%] [G loss: 16.047432]\n",
      "3776 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.636271]\n",
      "3777 [D loss: 0.000174, acc.: 100.00%] [G loss: 15.418091]\n",
      "3778 [D loss: 0.125923, acc.: 99.22%] [G loss: 15.178413]\n",
      "3779 [D loss: 0.000007, acc.: 100.00%] [G loss: 14.756992]\n",
      "3780 [D loss: 0.126023, acc.: 99.22%] [G loss: 14.234487]\n",
      "3781 [D loss: 0.128665, acc.: 99.22%] [G loss: 12.959278]\n",
      "3782 [D loss: 0.022280, acc.: 99.22%] [G loss: 13.449125]\n",
      "3783 [D loss: 0.000139, acc.: 100.00%] [G loss: 13.581834]\n",
      "3784 [D loss: 0.255953, acc.: 98.44%] [G loss: 13.027409]\n",
      "3785 [D loss: 0.006919, acc.: 100.00%] [G loss: 12.552118]\n",
      "3786 [D loss: 0.003874, acc.: 100.00%] [G loss: 11.643169]\n",
      "3787 [D loss: 0.144733, acc.: 98.44%] [G loss: 12.823277]\n",
      "3788 [D loss: 0.503746, acc.: 96.88%] [G loss: 12.697306]\n",
      "3789 [D loss: 0.253009, acc.: 98.44%] [G loss: 10.726212]\n",
      "3790 [D loss: 0.070601, acc.: 96.09%] [G loss: 16.077240]\n",
      "3791 [D loss: 0.000007, acc.: 100.00%] [G loss: 15.933746]\n",
      "3792 [D loss: 0.125974, acc.: 99.22%] [G loss: 15.287279]\n",
      "3793 [D loss: 0.013338, acc.: 99.22%] [G loss: 15.324814]\n",
      "3794 [D loss: 0.000011, acc.: 100.00%] [G loss: 15.199003]\n",
      "3795 [D loss: 0.126539, acc.: 99.22%] [G loss: 14.717072]\n",
      "3796 [D loss: 0.003176, acc.: 100.00%] [G loss: 14.516315]\n",
      "3797 [D loss: 0.046161, acc.: 97.66%] [G loss: 15.012455]\n",
      "3798 [D loss: 0.126425, acc.: 99.22%] [G loss: 14.104166]\n",
      "3799 [D loss: 0.129040, acc.: 99.22%] [G loss: 13.229096]\n",
      "3800 [D loss: 0.152076, acc.: 97.66%] [G loss: 12.890533]\n",
      "3801 [D loss: 0.129464, acc.: 99.22%] [G loss: 11.585047]\n",
      "3802 [D loss: 0.146326, acc.: 99.22%] [G loss: 11.718203]\n",
      "3803 [D loss: 0.253481, acc.: 98.44%] [G loss: 10.078167]\n",
      "3804 [D loss: 0.017673, acc.: 100.00%] [G loss: 9.167532]\n",
      "3805 [D loss: 0.006644, acc.: 100.00%] [G loss: 7.993805]\n",
      "3806 [D loss: 0.144268, acc.: 99.22%] [G loss: 7.864123]\n",
      "3807 [D loss: 0.011435, acc.: 100.00%] [G loss: 7.081923]\n",
      "3808 [D loss: 0.017320, acc.: 100.00%] [G loss: 6.364631]\n",
      "3809 [D loss: 0.255345, acc.: 98.44%] [G loss: 5.888071]\n",
      "3810 [D loss: 0.202784, acc.: 96.88%] [G loss: 8.927935]\n",
      "3811 [D loss: 0.136312, acc.: 99.22%] [G loss: 9.807092]\n",
      "3812 [D loss: 0.008111, acc.: 100.00%] [G loss: 7.808469]\n",
      "3813 [D loss: 0.131414, acc.: 99.22%] [G loss: 5.911537]\n",
      "3814 [D loss: 0.131696, acc.: 99.22%] [G loss: 4.283617]\n",
      "3815 [D loss: 0.012445, acc.: 100.00%] [G loss: 3.577201]\n",
      "3816 [D loss: 0.012731, acc.: 100.00%] [G loss: 3.174793]\n",
      "3817 [D loss: 0.138814, acc.: 99.22%] [G loss: 2.762863]\n",
      "3818 [D loss: 0.265192, acc.: 98.44%] [G loss: 2.592735]\n",
      "3819 [D loss: 0.023835, acc.: 99.22%] [G loss: 2.759122]\n",
      "3820 [D loss: 0.013738, acc.: 100.00%] [G loss: 2.498360]\n",
      "3821 [D loss: 0.140521, acc.: 99.22%] [G loss: 2.841175]\n",
      "3822 [D loss: 0.003330, acc.: 100.00%] [G loss: 2.721084]\n",
      "3823 [D loss: 0.132445, acc.: 99.22%] [G loss: 2.490663]\n",
      "3824 [D loss: 0.133039, acc.: 99.22%] [G loss: 2.104525]\n",
      "3825 [D loss: 0.034871, acc.: 99.22%] [G loss: 1.807220]\n",
      "3826 [D loss: 0.010614, acc.: 100.00%] [G loss: 2.198952]\n",
      "3827 [D loss: 0.132275, acc.: 99.22%] [G loss: 1.903463]\n",
      "3828 [D loss: 0.014295, acc.: 100.00%] [G loss: 2.485259]\n",
      "3829 [D loss: 0.129227, acc.: 99.22%] [G loss: 2.554402]\n",
      "3830 [D loss: 0.005926, acc.: 100.00%] [G loss: 3.074745]\n",
      "3831 [D loss: 0.133084, acc.: 99.22%] [G loss: 2.785065]\n",
      "3832 [D loss: 0.134082, acc.: 99.22%] [G loss: 2.465138]\n",
      "3833 [D loss: 0.134031, acc.: 99.22%] [G loss: 2.972766]\n",
      "3834 [D loss: 0.132998, acc.: 99.22%] [G loss: 2.771564]\n",
      "3835 [D loss: 0.011160, acc.: 100.00%] [G loss: 3.559898]\n",
      "3836 [D loss: 0.129787, acc.: 99.22%] [G loss: 3.634290]\n",
      "3837 [D loss: 0.129462, acc.: 99.22%] [G loss: 3.087192]\n",
      "3838 [D loss: 0.007733, acc.: 100.00%] [G loss: 3.173461]\n",
      "3839 [D loss: 0.258432, acc.: 98.44%] [G loss: 3.027577]\n",
      "3840 [D loss: 0.006646, acc.: 100.00%] [G loss: 2.892179]\n",
      "3841 [D loss: 0.009010, acc.: 99.22%] [G loss: 2.975569]\n",
      "3842 [D loss: 0.003538, acc.: 100.00%] [G loss: 3.298228]\n",
      "3843 [D loss: 0.002986, acc.: 100.00%] [G loss: 3.183073]\n",
      "3844 [D loss: 0.008004, acc.: 100.00%] [G loss: 2.900253]\n",
      "3845 [D loss: 0.009157, acc.: 100.00%] [G loss: 2.638118]\n",
      "3846 [D loss: 0.257590, acc.: 98.44%] [G loss: 2.930365]\n",
      "3847 [D loss: 0.004710, acc.: 100.00%] [G loss: 2.763360]\n",
      "3848 [D loss: 0.003333, acc.: 100.00%] [G loss: 2.885351]\n",
      "3849 [D loss: 0.003006, acc.: 100.00%] [G loss: 2.981822]\n",
      "3850 [D loss: 0.259317, acc.: 98.44%] [G loss: 2.759702]\n",
      "3851 [D loss: 0.004834, acc.: 100.00%] [G loss: 2.459387]\n",
      "3852 [D loss: 0.005120, acc.: 100.00%] [G loss: 2.951060]\n",
      "3853 [D loss: 0.001169, acc.: 100.00%] [G loss: 3.800254]\n",
      "3854 [D loss: 0.003844, acc.: 100.00%] [G loss: 3.281942]\n",
      "3855 [D loss: 0.383755, acc.: 97.66%] [G loss: 2.929842]\n",
      "3856 [D loss: 0.132044, acc.: 99.22%] [G loss: 2.882040]\n",
      "3857 [D loss: 0.130240, acc.: 99.22%] [G loss: 3.323589]\n",
      "3858 [D loss: 0.254173, acc.: 98.44%] [G loss: 3.233769]\n",
      "3859 [D loss: 0.002590, acc.: 100.00%] [G loss: 3.109948]\n",
      "3860 [D loss: 0.254982, acc.: 98.44%] [G loss: 2.967078]\n",
      "3861 [D loss: 0.255779, acc.: 98.44%] [G loss: 2.855634]\n",
      "3862 [D loss: 0.129507, acc.: 99.22%] [G loss: 2.793707]\n",
      "3863 [D loss: 0.013731, acc.: 100.00%] [G loss: 2.607576]\n",
      "3864 [D loss: 0.145996, acc.: 99.22%] [G loss: 5.109531]\n",
      "3865 [D loss: 0.128306, acc.: 99.22%] [G loss: 6.447888]\n",
      "3866 [D loss: 0.001565, acc.: 100.00%] [G loss: 5.803317]\n",
      "3867 [D loss: 0.127169, acc.: 99.22%] [G loss: 5.118498]\n",
      "3868 [D loss: 0.127032, acc.: 99.22%] [G loss: 4.456827]\n",
      "3869 [D loss: 0.002656, acc.: 100.00%] [G loss: 3.941017]\n",
      "3870 [D loss: 0.128022, acc.: 99.22%] [G loss: 3.462856]\n",
      "3871 [D loss: 0.128761, acc.: 99.22%] [G loss: 3.240081]\n",
      "3872 [D loss: 0.002627, acc.: 100.00%] [G loss: 3.336957]\n",
      "3873 [D loss: 0.254837, acc.: 98.44%] [G loss: 3.311481]\n",
      "3874 [D loss: 0.129350, acc.: 99.22%] [G loss: 3.317153]\n",
      "3875 [D loss: 0.002431, acc.: 100.00%] [G loss: 3.714468]\n",
      "3876 [D loss: 0.003012, acc.: 100.00%] [G loss: 3.626995]\n",
      "3877 [D loss: 0.128627, acc.: 99.22%] [G loss: 3.389716]\n",
      "3878 [D loss: 0.130387, acc.: 99.22%] [G loss: 3.326497]\n",
      "3879 [D loss: 0.003869, acc.: 100.00%] [G loss: 3.334049]\n",
      "3880 [D loss: 0.003580, acc.: 100.00%] [G loss: 3.344221]\n",
      "3881 [D loss: 0.004944, acc.: 100.00%] [G loss: 3.354696]\n",
      "3882 [D loss: 0.128954, acc.: 99.22%] [G loss: 3.000406]\n",
      "3883 [D loss: 0.132185, acc.: 99.22%] [G loss: 3.168801]\n",
      "3884 [D loss: 0.129586, acc.: 99.22%] [G loss: 3.188446]\n",
      "3885 [D loss: 0.030002, acc.: 99.22%] [G loss: 3.117480]\n",
      "3886 [D loss: 0.129462, acc.: 99.22%] [G loss: 2.960133]\n",
      "3887 [D loss: 0.129424, acc.: 99.22%] [G loss: 3.065544]\n",
      "3888 [D loss: 0.001968, acc.: 100.00%] [G loss: 3.275976]\n",
      "3889 [D loss: 0.002990, acc.: 100.00%] [G loss: 3.492861]\n",
      "3890 [D loss: 0.130528, acc.: 99.22%] [G loss: 3.260048]\n",
      "3891 [D loss: 0.004079, acc.: 100.00%] [G loss: 3.500868]\n",
      "3892 [D loss: 0.003995, acc.: 100.00%] [G loss: 3.103895]\n",
      "3893 [D loss: 0.254891, acc.: 98.44%] [G loss: 3.099331]\n",
      "3894 [D loss: 0.130417, acc.: 99.22%] [G loss: 3.589153]\n",
      "3895 [D loss: 0.129268, acc.: 99.22%] [G loss: 3.563717]\n",
      "3896 [D loss: 0.002839, acc.: 100.00%] [G loss: 3.505167]\n",
      "3897 [D loss: 0.128969, acc.: 99.22%] [G loss: 3.280070]\n",
      "3898 [D loss: 0.002660, acc.: 100.00%] [G loss: 3.736954]\n",
      "3899 [D loss: 0.130935, acc.: 99.22%] [G loss: 3.347881]\n",
      "3900 [D loss: 0.255397, acc.: 98.44%] [G loss: 3.138151]\n",
      "3901 [D loss: 0.129622, acc.: 99.22%] [G loss: 3.171403]\n",
      "3902 [D loss: 0.129614, acc.: 99.22%] [G loss: 3.243619]\n",
      "3903 [D loss: 0.129254, acc.: 99.22%] [G loss: 3.033902]\n",
      "3904 [D loss: 0.261389, acc.: 97.66%] [G loss: 3.239777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3905 [D loss: 0.129178, acc.: 99.22%] [G loss: 3.313347]\n",
      "3906 [D loss: 0.002955, acc.: 100.00%] [G loss: 3.272816]\n",
      "3907 [D loss: 0.254195, acc.: 98.44%] [G loss: 3.155061]\n",
      "3908 [D loss: 0.002360, acc.: 100.00%] [G loss: 3.206047]\n",
      "3909 [D loss: 0.002182, acc.: 100.00%] [G loss: 3.585164]\n",
      "3910 [D loss: 0.255112, acc.: 98.44%] [G loss: 3.692319]\n",
      "3911 [D loss: 0.003590, acc.: 100.00%] [G loss: 3.481729]\n",
      "3912 [D loss: 0.129042, acc.: 99.22%] [G loss: 3.134404]\n",
      "3913 [D loss: 0.003955, acc.: 100.00%] [G loss: 3.017752]\n",
      "3914 [D loss: 0.255825, acc.: 98.44%] [G loss: 3.223722]\n",
      "3915 [D loss: 0.003632, acc.: 100.00%] [G loss: 3.198283]\n",
      "3916 [D loss: 0.128659, acc.: 99.22%] [G loss: 3.225466]\n",
      "3917 [D loss: 0.128493, acc.: 99.22%] [G loss: 3.318871]\n",
      "3918 [D loss: 0.380769, acc.: 97.66%] [G loss: 3.189687]\n",
      "3919 [D loss: 0.003241, acc.: 100.00%] [G loss: 3.302076]\n",
      "3920 [D loss: 0.002501, acc.: 100.00%] [G loss: 3.559956]\n",
      "3921 [D loss: 0.380996, acc.: 97.66%] [G loss: 3.368517]\n",
      "3922 [D loss: 0.128405, acc.: 99.22%] [G loss: 3.488227]\n",
      "3923 [D loss: 0.003425, acc.: 100.00%] [G loss: 3.687125]\n",
      "3924 [D loss: 0.001749, acc.: 100.00%] [G loss: 3.634286]\n",
      "3925 [D loss: 0.004256, acc.: 100.00%] [G loss: 3.605891]\n",
      "3926 [D loss: 0.381759, acc.: 97.66%] [G loss: 3.168102]\n",
      "3927 [D loss: 0.506407, acc.: 96.88%] [G loss: 3.237082]\n",
      "3928 [D loss: 0.128307, acc.: 99.22%] [G loss: 3.322838]\n",
      "3929 [D loss: 0.506231, acc.: 96.88%] [G loss: 3.244148]\n",
      "3930 [D loss: 0.002222, acc.: 100.00%] [G loss: 3.552577]\n",
      "3931 [D loss: 0.127734, acc.: 99.22%] [G loss: 4.079206]\n",
      "3932 [D loss: 0.255051, acc.: 98.44%] [G loss: 3.462038]\n",
      "3933 [D loss: 0.381480, acc.: 97.66%] [G loss: 3.322834]\n",
      "3934 [D loss: 0.004108, acc.: 100.00%] [G loss: 3.164303]\n",
      "3935 [D loss: 0.004282, acc.: 100.00%] [G loss: 3.139559]\n",
      "3936 [D loss: 0.002939, acc.: 100.00%] [G loss: 3.104875]\n",
      "3937 [D loss: 0.002300, acc.: 100.00%] [G loss: 3.492150]\n",
      "3938 [D loss: 0.127681, acc.: 99.22%] [G loss: 3.677867]\n",
      "3939 [D loss: 0.127176, acc.: 99.22%] [G loss: 3.881397]\n",
      "3940 [D loss: 0.128246, acc.: 99.22%] [G loss: 3.635944]\n",
      "3941 [D loss: 0.255564, acc.: 98.44%] [G loss: 3.525377]\n",
      "3942 [D loss: 0.002127, acc.: 100.00%] [G loss: 3.545261]\n",
      "3943 [D loss: 0.002788, acc.: 100.00%] [G loss: 3.603374]\n",
      "3944 [D loss: 0.002488, acc.: 100.00%] [G loss: 3.354779]\n",
      "3945 [D loss: 0.004837, acc.: 100.00%] [G loss: 3.163955]\n",
      "3946 [D loss: 0.130892, acc.: 99.22%] [G loss: 3.560109]\n",
      "3947 [D loss: 0.001827, acc.: 100.00%] [G loss: 3.678183]\n",
      "3948 [D loss: 0.001923, acc.: 100.00%] [G loss: 3.751123]\n",
      "3949 [D loss: 0.127692, acc.: 99.22%] [G loss: 3.475727]\n",
      "3950 [D loss: 0.002425, acc.: 100.00%] [G loss: 3.817164]\n",
      "3951 [D loss: 0.253345, acc.: 98.44%] [G loss: 4.105092]\n",
      "3952 [D loss: 0.253342, acc.: 98.44%] [G loss: 3.868237]\n",
      "3953 [D loss: 0.129251, acc.: 99.22%] [G loss: 3.429400]\n",
      "3954 [D loss: 0.129138, acc.: 99.22%] [G loss: 3.161083]\n",
      "3955 [D loss: 0.003011, acc.: 100.00%] [G loss: 3.157434]\n",
      "3956 [D loss: 0.254064, acc.: 98.44%] [G loss: 3.446865]\n",
      "3957 [D loss: 0.128303, acc.: 99.22%] [G loss: 3.629569]\n",
      "3958 [D loss: 0.002527, acc.: 100.00%] [G loss: 3.538761]\n",
      "3959 [D loss: 0.129324, acc.: 99.22%] [G loss: 3.518442]\n",
      "3960 [D loss: 0.002335, acc.: 100.00%] [G loss: 3.314291]\n",
      "3961 [D loss: 0.128828, acc.: 99.22%] [G loss: 3.342413]\n",
      "3962 [D loss: 0.002156, acc.: 100.00%] [G loss: 3.417706]\n",
      "3963 [D loss: 0.253968, acc.: 98.44%] [G loss: 3.472895]\n",
      "3964 [D loss: 0.253559, acc.: 98.44%] [G loss: 3.766526]\n",
      "3965 [D loss: 0.127352, acc.: 99.22%] [G loss: 4.070098]\n",
      "3966 [D loss: 0.002143, acc.: 100.00%] [G loss: 3.711726]\n",
      "3967 [D loss: 0.380210, acc.: 97.66%] [G loss: 3.465434]\n",
      "3968 [D loss: 0.002761, acc.: 100.00%] [G loss: 3.168081]\n",
      "3969 [D loss: 0.128731, acc.: 99.22%] [G loss: 3.139667]\n",
      "3970 [D loss: 0.253923, acc.: 98.44%] [G loss: 3.303782]\n",
      "3971 [D loss: 0.505892, acc.: 96.88%] [G loss: 3.601048]\n",
      "3972 [D loss: 0.380371, acc.: 97.66%] [G loss: 3.446293]\n",
      "3973 [D loss: 0.128278, acc.: 99.22%] [G loss: 3.453835]\n",
      "3974 [D loss: 0.254058, acc.: 98.44%] [G loss: 3.593588]\n",
      "3975 [D loss: 0.254689, acc.: 98.44%] [G loss: 3.497830]\n",
      "3976 [D loss: 0.002207, acc.: 100.00%] [G loss: 3.298555]\n",
      "3977 [D loss: 0.128104, acc.: 99.22%] [G loss: 3.272316]\n",
      "3978 [D loss: 0.380327, acc.: 97.66%] [G loss: 3.266988]\n",
      "3979 [D loss: 0.379551, acc.: 97.66%] [G loss: 3.388646]\n",
      "3980 [D loss: 0.127603, acc.: 99.22%] [G loss: 3.993231]\n",
      "3981 [D loss: 0.252974, acc.: 98.44%] [G loss: 4.157243]\n",
      "3982 [D loss: 0.126805, acc.: 99.22%] [G loss: 4.129455]\n",
      "3983 [D loss: 0.128302, acc.: 99.22%] [G loss: 3.534945]\n",
      "3984 [D loss: 0.003500, acc.: 100.00%] [G loss: 3.261787]\n",
      "3985 [D loss: 0.128901, acc.: 99.22%] [G loss: 3.122404]\n",
      "3986 [D loss: 0.254142, acc.: 98.44%] [G loss: 3.090082]\n",
      "3987 [D loss: 0.003163, acc.: 100.00%] [G loss: 3.113512]\n",
      "3988 [D loss: 0.001578, acc.: 100.00%] [G loss: 3.294821]\n",
      "3989 [D loss: 0.001562, acc.: 100.00%] [G loss: 3.538829]\n",
      "3990 [D loss: 0.127339, acc.: 99.22%] [G loss: 4.078901]\n",
      "3991 [D loss: 0.126857, acc.: 99.22%] [G loss: 4.148273]\n",
      "3992 [D loss: 0.506259, acc.: 96.88%] [G loss: 3.318717]\n",
      "3993 [D loss: 0.002713, acc.: 100.00%] [G loss: 3.157261]\n",
      "3994 [D loss: 0.002238, acc.: 100.00%] [G loss: 3.146240]\n",
      "3995 [D loss: 0.254280, acc.: 98.44%] [G loss: 3.327682]\n",
      "3996 [D loss: 0.253926, acc.: 98.44%] [G loss: 3.496553]\n",
      "3997 [D loss: 0.253195, acc.: 98.44%] [G loss: 3.686880]\n",
      "3998 [D loss: 0.253938, acc.: 98.44%] [G loss: 3.754434]\n",
      "3999 [D loss: 0.254060, acc.: 98.44%] [G loss: 3.664692]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=4000, data=normalized_data) # Generate data similar to the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model trained on the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82815, 5260) (82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_seq_trunc.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (8282, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb2, X_valid_emb2, y_train_emb2, y_valid_emb2 = train_test_split(X_test_seq_trunc, y_test, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb2.shape[0] == y_valid_emb2.shape[0]\n",
    "assert X_train_emb2.shape[0] == y_train_emb2.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model2 = Sequential()\n",
    "glove_model2.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model2.add(Flatten())\n",
    "glove_model2.add(Dense(1, activation='sigmoid'))\n",
    "glove_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model2.layers[0].set_weights([emb_matrix])\n",
    "glove_model2.layers[0].trainable = False\n",
    "\n",
    "glove_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74533 samples, validate on 8282 samples\n",
      "Epoch 1/5\n",
      "74533/74533 [==============================] - 339s 5ms/step - loss: 0.4503 - acc: 0.8394 - val_loss: 0.4638 - val_acc: 0.8604\n",
      "Epoch 2/5\n",
      "74533/74533 [==============================] - 394s 5ms/step - loss: 0.3001 - acc: 0.8967 - val_loss: 0.4647 - val_acc: 0.8662\n",
      "Epoch 3/5\n",
      "74533/74533 [==============================] - 712s 10ms/step - loss: 0.2538 - acc: 0.9116 - val_loss: 0.4629 - val_acc: 0.8694\n",
      "Epoch 4/5\n",
      "74533/74533 [==============================] - 613s 8ms/step - loss: 0.2288 - acc: 0.9208 - val_loss: 0.4886 - val_acc: 0.8743\n",
      "Epoch 5/5\n",
      "74533/74533 [==============================] - 303s 4ms/step - loss: 0.2023 - acc: 0.9291 - val_loss: 0.5952 - val_acc: 0.8624\n"
     ]
    }
   ],
   "source": [
    "history2 = glove_model2.fit(X_train_emb2\n",
    "                       , y_train_emb2\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb2, y_valid_emb2)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_model2.predict(X_train_emb[0:1]))\n",
    "# print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model trained on a generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data and labels\n",
    "gen = 100000\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "gen_samp = np.absolute((generator.predict(noise)))\n",
    "prediction = glove_model.predict((gen_samp))\n",
    "prediction = np.round(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(np.round(prediction[0:100]))\n",
    "# print(np.sum(np.round(glove_model.predict(X_train_emb[0:100]))))\n",
    "# print(np.sum(y_train[0:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (30000, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb3, X_valid_emb3, y_train_emb3, y_valid_emb3 = train_test_split(gen_samp, prediction, test_size=0.3, random_state=37)\n",
    "\n",
    "assert X_valid_emb3.shape[0] == y_valid_emb3.shape[0]\n",
    "assert X_train_emb3.shape[0] == y_train_emb3.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model3 = Sequential()\n",
    "glove_model3.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model3.add(Flatten())\n",
    "glove_model3.add(Dense(1, activation='sigmoid'))\n",
    "glove_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model3.layers[0].set_weights([emb_matrix])\n",
    "glove_model3.layers[0].trainable = False\n",
    "\n",
    "glove_model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      "70000/70000 [==============================] - 412s 6ms/step - loss: 0.1680 - acc: 0.9944 - val_loss: 0.0778 - val_acc: 0.9992\n",
      "Epoch 2/5\n",
      "70000/70000 [==============================] - 681s 10ms/step - loss: 0.0477 - acc: 0.9987 - val_loss: 0.0289 - val_acc: 0.9983\n",
      "Epoch 3/5\n",
      "70000/70000 [==============================] - 438s 6ms/step - loss: 0.0186 - acc: 0.9990 - val_loss: 0.0139 - val_acc: 0.9987\n",
      "Epoch 4/5\n",
      "70000/70000 [==============================] - 669s 10ms/step - loss: 0.0095 - acc: 0.9989 - val_loss: 0.0062 - val_acc: 0.9992\n",
      "Epoch 5/5\n",
      "70000/70000 [==============================] - 456s 7ms/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.0048 - val_acc: 0.9990\n"
     ]
    }
   ],
   "source": [
    "history3 = glove_model3.fit(X_train_emb3\n",
    "                       , y_train_emb3\n",
    "                       , epochs=5\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb3, y_valid_emb3)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this area I am going to compare their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb3\n",
    "pred = np.round(glove_model2.predict(X_valid_emb3))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[ 3801  2687]\n",
      " [ 3993 19519]]\n",
      "Accuracy Score : 0.7773333333333333\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.59      0.53      6488\n",
      "         1.0       0.88      0.83      0.85     23512\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     30000\n",
      "   macro avg       0.68      0.71      0.69     30000\n",
      "weighted avg       0.79      0.78      0.78     30000\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[ 6466    22]\n",
      " [    9 23503]]\n",
      "Accuracy Score : 0.9989666666666667\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6488\n",
      "         1.0       1.00      1.00      1.00     23512\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     30000\n",
      "   macro avg       1.00      1.00      1.00     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb2\n",
    "pred = np.round(glove_model2.predict(X_valid_emb2))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[1943  765]\n",
      " [ 375 5199]]\n",
      "Accuracy Score : 0.8623520888674233\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.72      0.77      2708\n",
      "         1.0       0.87      0.93      0.90      5574\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      8282\n",
      "   macro avg       0.85      0.83      0.84      8282\n",
      "weighted avg       0.86      0.86      0.86      8282\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[   1 2707]\n",
      " [   0 5574]]\n",
      "Accuracy Score : 0.6731465829509781\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.00      0.00      2708\n",
      "         1.0       0.67      1.00      0.80      5574\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8282\n",
      "   macro avg       0.84      0.50      0.40      8282\n",
      "weighted avg       0.78      0.67      0.54      8282\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
