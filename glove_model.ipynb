{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM, Embedding, SpatialDropout1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.read_csv('amazon/reviews.csv')\n",
    "df_dataset = pd.read_json('clothing_dataset/renttherunway_final_data.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['title'] + \" \" + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "df_dataset['text'] = df_dataset['review_summary'] + \" \" + df_dataset['review_text']\n",
    "df_dataset['text'] = df_dataset['text'].apply(lambda x: pre_process(str(x)))\n",
    "\n",
    "sub_dataset = df_dataset[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df_idf['text'].append(sub_dataset['text'])\n",
    "all_rating = df_idf['rating'].append(sub_dataset['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_rating\n",
    "y[:len(df_idf)] = y[:len(df_idf)].apply(lambda x: 1 if x > 3.5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y[len(df_idf):] = y[len(df_idf):].apply(lambda x: 1 if x > 5 else 0)#y.apply(lambda x: 1 if x > 3.5 else 0) \n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to use both datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.1, random_state=37)\n",
    "# X_train = df_dataset['text']\n",
    "# y_train = y[len(df_idf):]\n",
    "\n",
    "# X_test = df_idf['text'].to_numpy()\n",
    "# y_test = y[:len(df_idf)]\n",
    "\n",
    "# print('# Train data samples:', X_train.shape)\n",
    "# print('# Test data samples:', X_test.shape)\n",
    "\n",
    "# print('Sample train', X_train[0])\n",
    "# print('\\nSample test', X_test[0])\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# assert X_train.shape[0] == y_train.shape[0]\n",
    "# assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 5260\n",
    "GLOVE_DIM = 300\n",
    "NB_WORDS = 49781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(all_data)\n",
    "\n",
    "all_data_seq = tk.texts_to_sequences(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = all_data_seq[len(df_idf):]\n",
    "X_test_seq = all_data_seq[:len(df_idf)]\n",
    "\n",
    "y_train = y[len(df_idf):]\n",
    "y_test = y[:len(df_idf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    275359.000000\n",
      "mean         63.517557\n",
      "std          69.966379\n",
      "min           1.000000\n",
      "25%          26.000000\n",
      "50%          50.000000\n",
      "75%          82.000000\n",
      "max        5260.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = all_data.apply(lambda x: len(x.split(' ')))\n",
    "print(seq_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0 ...  61  14 316]\n",
      "(192544, 5260)\n",
      "(82815, 5260)\n",
      "(192544,)\n",
      "(82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq_trunc[10])  # Example of padded sequence\n",
    "print(X_train_seq_trunc.shape)\n",
    "print(X_test_seq_trunc.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (19255, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This read the embeddings\n",
    "# glove_file = 'glove.42B.' + str(GLOVE_DIM) + 'd.txt'\n",
    "# emb_dict = {}\n",
    "# glove = open(glove_file)\n",
    "# for line in glove:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     vector = np.asarray(values[1:], dtype='float32')\n",
    "#     emb_dict[word] = vector\n",
    "# glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word car in the dictionary\n",
      "Found the word nice in the dictionary\n",
      "Found the word flight in the dictionary\n",
      "Found the word luggage in the dictionary\n"
     ]
    }
   ],
   "source": [
    "airline_words = ['car', 'nice', 'flight', 'luggage']\n",
    "for w in airline_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary'.format(w))\n",
    "# print(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a matrix that represent words and it corresponding emdg\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model.add(Flatten())\n",
    "glove_model.add(Dense(1, activation='sigmoid'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 173289 samples, validate on 19255 samples\n",
      "Epoch 1/1\n",
      "173289/173289 [==============================] - 640s 4ms/step - loss: 0.1115 - acc: 0.9752 - val_loss: 0.1185 - val_acc: 0.9781\n"
     ]
    }
   ],
   "source": [
    "history = glove_model.fit(X_train_emb\n",
    "                       , y_train_emb\n",
    "                       , epochs=1\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb, y_valid_emb)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9998597]]\n",
      "(5260,)\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.predict(X_train_emb[0:1]))\n",
    "print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we start building the GANs, this model takes the word embedding and generate new embeddings that are similar to the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(shape):\n",
    "    img_shape = shape\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator(shape):\n",
    "\n",
    "    img_shape = shape\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Flatten(input_shape=img_shape)) # is one dimension\n",
    "    model.add(Dense(512, input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def results(pred, actual):\n",
    "    results = confusion_matrix(actual, pred)\n",
    "    print('Confusion Matrix :')\n",
    "    print(results)\n",
    "    print ('Accuracy Score :',accuracy_score(actual, pred))\n",
    "    print ('Report : ')\n",
    "    print(classification_report(actual, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               2693632   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,825,217\n",
      "Trainable params: 2,825,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5260)              5391500   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5260)              0         \n",
      "=================================================================\n",
      "Total params: 6,081,420\n",
      "Trainable params: 6,077,836\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows = 1\n",
    "img_cols = X_train_emb[0].shape\n",
    "img_shape = (img_cols)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(img_shape)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, batch_size=128):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = data #(X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#         X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[1], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = np.round(generator.predict(noise))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.477784, acc.: 82.81%] [G loss: 2.574787]\n",
      "1 [D loss: 0.678478, acc.: 80.47%] [G loss: 3.574183]\n",
      "2 [D loss: 0.463642, acc.: 82.03%] [G loss: 3.972436]\n",
      "3 [D loss: 0.130374, acc.: 94.53%] [G loss: 4.129066]\n",
      "4 [D loss: 0.125625, acc.: 96.09%] [G loss: 3.625206]\n",
      "5 [D loss: 0.077234, acc.: 99.22%] [G loss: 3.083598]\n",
      "6 [D loss: 0.072303, acc.: 98.44%] [G loss: 2.182201]\n",
      "7 [D loss: 0.192370, acc.: 91.41%] [G loss: 2.053828]\n",
      "8 [D loss: 0.291957, acc.: 84.38%] [G loss: 2.562545]\n",
      "9 [D loss: 0.878608, acc.: 73.44%] [G loss: 3.637248]\n",
      "10 [D loss: 0.954615, acc.: 78.91%] [G loss: 4.576241]\n",
      "11 [D loss: 0.277697, acc.: 92.19%] [G loss: 5.083704]\n",
      "12 [D loss: 0.178564, acc.: 92.19%] [G loss: 4.452594]\n",
      "13 [D loss: 0.032658, acc.: 100.00%] [G loss: 3.631996]\n",
      "14 [D loss: 0.082488, acc.: 97.66%] [G loss: 2.667324]\n",
      "15 [D loss: 0.399816, acc.: 93.75%] [G loss: 2.266572]\n",
      "16 [D loss: 0.146342, acc.: 95.31%] [G loss: 2.305400]\n",
      "17 [D loss: 0.261202, acc.: 93.75%] [G loss: 2.228607]\n",
      "18 [D loss: 0.173962, acc.: 91.41%] [G loss: 2.089159]\n",
      "19 [D loss: 0.137634, acc.: 96.09%] [G loss: 2.234943]\n",
      "20 [D loss: 0.399173, acc.: 92.97%] [G loss: 2.292804]\n",
      "21 [D loss: 0.279728, acc.: 93.75%] [G loss: 2.257182]\n",
      "22 [D loss: 0.188103, acc.: 92.19%] [G loss: 2.332724]\n",
      "23 [D loss: 0.281637, acc.: 91.41%] [G loss: 2.355610]\n",
      "24 [D loss: 0.180286, acc.: 94.53%] [G loss: 2.444051]\n",
      "25 [D loss: 0.301659, acc.: 89.84%] [G loss: 2.379801]\n",
      "26 [D loss: 0.558763, acc.: 82.81%] [G loss: 3.116751]\n",
      "27 [D loss: 0.405968, acc.: 79.69%] [G loss: 3.922104]\n",
      "28 [D loss: 0.497709, acc.: 91.41%] [G loss: 4.088437]\n",
      "29 [D loss: 0.261554, acc.: 91.41%] [G loss: 3.626222]\n",
      "30 [D loss: 0.188486, acc.: 99.22%] [G loss: 2.939176]\n",
      "31 [D loss: 0.238582, acc.: 94.53%] [G loss: 2.584431]\n",
      "32 [D loss: 0.382160, acc.: 93.75%] [G loss: 2.488118]\n",
      "33 [D loss: 0.244823, acc.: 96.09%] [G loss: 2.453657]\n",
      "34 [D loss: 0.232195, acc.: 96.88%] [G loss: 2.351889]\n",
      "35 [D loss: 0.420728, acc.: 92.19%] [G loss: 2.137711]\n",
      "36 [D loss: 0.511761, acc.: 85.94%] [G loss: 2.932185]\n",
      "37 [D loss: 0.587527, acc.: 84.38%] [G loss: 3.702191]\n",
      "38 [D loss: 0.343197, acc.: 96.09%] [G loss: 3.589290]\n",
      "39 [D loss: 0.302078, acc.: 98.44%] [G loss: 2.877226]\n",
      "40 [D loss: 0.328527, acc.: 97.66%] [G loss: 2.341128]\n",
      "41 [D loss: 0.621469, acc.: 91.41%] [G loss: 2.280951]\n",
      "42 [D loss: 0.464049, acc.: 97.66%] [G loss: 2.399841]\n",
      "43 [D loss: 0.726174, acc.: 94.53%] [G loss: 2.451553]\n",
      "44 [D loss: 0.272469, acc.: 92.19%] [G loss: 2.946580]\n",
      "45 [D loss: 1.065310, acc.: 71.88%] [G loss: 4.154421]\n",
      "46 [D loss: 1.339864, acc.: 75.00%] [G loss: 5.243287]\n",
      "47 [D loss: 0.493866, acc.: 96.88%] [G loss: 3.247911]\n",
      "48 [D loss: 0.198618, acc.: 90.62%] [G loss: 2.873080]\n",
      "49 [D loss: 0.283708, acc.: 82.81%] [G loss: 3.174071]\n",
      "50 [D loss: 0.241786, acc.: 94.53%] [G loss: 2.826321]\n",
      "51 [D loss: 0.085202, acc.: 99.22%] [G loss: 2.669779]\n",
      "52 [D loss: 0.117876, acc.: 95.31%] [G loss: 2.189494]\n",
      "53 [D loss: 0.108219, acc.: 98.44%] [G loss: 1.952277]\n",
      "54 [D loss: 0.252576, acc.: 97.66%] [G loss: 1.918123]\n",
      "55 [D loss: 0.147607, acc.: 94.53%] [G loss: 1.997775]\n",
      "56 [D loss: 0.169220, acc.: 91.41%] [G loss: 2.121998]\n",
      "57 [D loss: 0.137306, acc.: 94.53%] [G loss: 2.265885]\n",
      "58 [D loss: 0.194944, acc.: 88.28%] [G loss: 2.290654]\n",
      "59 [D loss: 0.292717, acc.: 83.59%] [G loss: 2.601585]\n",
      "60 [D loss: 0.330312, acc.: 81.25%] [G loss: 3.062200]\n",
      "61 [D loss: 0.240602, acc.: 95.31%] [G loss: 3.163256]\n",
      "62 [D loss: 0.676666, acc.: 95.31%] [G loss: 2.612568]\n",
      "63 [D loss: 0.353136, acc.: 96.09%] [G loss: 2.279618]\n",
      "64 [D loss: 0.414864, acc.: 91.41%] [G loss: 2.222177]\n",
      "65 [D loss: 0.306268, acc.: 92.19%] [G loss: 2.233299]\n",
      "66 [D loss: 0.596161, acc.: 86.72%] [G loss: 2.785420]\n",
      "67 [D loss: 0.316890, acc.: 89.06%] [G loss: 2.741771]\n",
      "68 [D loss: 0.249141, acc.: 94.53%] [G loss: 2.379162]\n",
      "69 [D loss: 0.221018, acc.: 97.66%] [G loss: 2.149098]\n",
      "70 [D loss: 0.271065, acc.: 94.53%] [G loss: 2.022398]\n",
      "71 [D loss: 0.404913, acc.: 90.62%] [G loss: 2.032155]\n",
      "72 [D loss: 0.395928, acc.: 92.97%] [G loss: 2.168771]\n",
      "73 [D loss: 0.311547, acc.: 91.41%] [G loss: 2.490732]\n",
      "74 [D loss: 0.335150, acc.: 97.66%] [G loss: 2.404247]\n",
      "75 [D loss: 0.416564, acc.: 92.19%] [G loss: 1.911361]\n",
      "76 [D loss: 0.182904, acc.: 92.19%] [G loss: 2.024740]\n",
      "77 [D loss: 0.410168, acc.: 95.31%] [G loss: 2.137404]\n",
      "78 [D loss: 0.463106, acc.: 87.50%] [G loss: 2.471377]\n",
      "79 [D loss: 0.327779, acc.: 87.50%] [G loss: 3.032730]\n",
      "80 [D loss: 0.454595, acc.: 85.94%] [G loss: 2.977132]\n",
      "81 [D loss: 0.186971, acc.: 93.75%] [G loss: 3.102386]\n",
      "82 [D loss: 0.434338, acc.: 97.66%] [G loss: 2.740268]\n",
      "83 [D loss: 0.072985, acc.: 100.00%] [G loss: 2.255539]\n",
      "84 [D loss: 0.156830, acc.: 94.53%] [G loss: 2.100876]\n",
      "85 [D loss: 0.142461, acc.: 96.88%] [G loss: 2.183699]\n",
      "86 [D loss: 0.757720, acc.: 92.19%] [G loss: 2.258311]\n",
      "87 [D loss: 0.276113, acc.: 93.75%] [G loss: 2.312257]\n",
      "88 [D loss: 0.370381, acc.: 86.72%] [G loss: 2.334150]\n",
      "89 [D loss: 1.235320, acc.: 68.75%] [G loss: 3.542510]\n",
      "90 [D loss: 1.093487, acc.: 74.22%] [G loss: 4.603761]\n",
      "91 [D loss: 0.784578, acc.: 79.69%] [G loss: 5.525021]\n",
      "92 [D loss: 0.137945, acc.: 99.22%] [G loss: 5.277767]\n",
      "93 [D loss: 0.530934, acc.: 96.09%] [G loss: 4.095294]\n",
      "94 [D loss: 0.439141, acc.: 94.53%] [G loss: 3.262443]\n",
      "95 [D loss: 0.179790, acc.: 99.22%] [G loss: 2.546013]\n",
      "96 [D loss: 0.375988, acc.: 96.88%] [G loss: 2.101523]\n",
      "97 [D loss: 0.387306, acc.: 96.88%] [G loss: 2.048569]\n",
      "98 [D loss: 0.132713, acc.: 96.09%] [G loss: 2.492370]\n",
      "99 [D loss: 0.255872, acc.: 94.53%] [G loss: 2.421617]\n",
      "100 [D loss: 0.137589, acc.: 93.75%] [G loss: 2.557050]\n",
      "101 [D loss: 0.363580, acc.: 96.09%] [G loss: 2.253199]\n",
      "102 [D loss: 0.723027, acc.: 95.31%] [G loss: 2.573479]\n",
      "103 [D loss: 0.319111, acc.: 98.44%] [G loss: 2.449517]\n",
      "104 [D loss: 1.098876, acc.: 92.19%] [G loss: 2.403596]\n",
      "105 [D loss: 0.487818, acc.: 94.53%] [G loss: 2.386107]\n",
      "106 [D loss: 0.480940, acc.: 96.09%] [G loss: 2.232085]\n",
      "107 [D loss: 0.390436, acc.: 95.31%] [G loss: 2.450600]\n",
      "108 [D loss: 0.178984, acc.: 93.75%] [G loss: 2.893010]\n",
      "109 [D loss: 0.552753, acc.: 79.69%] [G loss: 3.356186]\n",
      "110 [D loss: 0.989361, acc.: 69.53%] [G loss: 2.211206]\n",
      "111 [D loss: 0.968649, acc.: 76.56%] [G loss: 3.796462]\n",
      "112 [D loss: 1.208675, acc.: 74.22%] [G loss: 4.759147]\n",
      "113 [D loss: 0.547723, acc.: 81.25%] [G loss: 6.333322]\n",
      "114 [D loss: 0.011586, acc.: 100.00%] [G loss: 5.425687]\n",
      "115 [D loss: 0.035330, acc.: 99.22%] [G loss: 3.410311]\n",
      "116 [D loss: 0.163517, acc.: 93.75%] [G loss: 2.510843]\n",
      "117 [D loss: 0.271066, acc.: 93.75%] [G loss: 2.029642]\n",
      "118 [D loss: 0.230320, acc.: 90.62%] [G loss: 1.999237]\n",
      "119 [D loss: 0.412946, acc.: 90.62%] [G loss: 1.910168]\n",
      "120 [D loss: 0.280597, acc.: 94.53%] [G loss: 2.011833]\n",
      "121 [D loss: 0.271380, acc.: 86.72%] [G loss: 2.226076]\n",
      "122 [D loss: 0.175716, acc.: 93.75%] [G loss: 2.403322]\n",
      "123 [D loss: 0.131697, acc.: 96.88%] [G loss: 1.885617]\n",
      "124 [D loss: 0.311319, acc.: 91.41%] [G loss: 2.132739]\n",
      "125 [D loss: 0.246143, acc.: 89.06%] [G loss: 2.150876]\n",
      "126 [D loss: 0.296813, acc.: 84.38%] [G loss: 2.422341]\n",
      "127 [D loss: 0.386818, acc.: 78.12%] [G loss: 2.869014]\n",
      "128 [D loss: 0.331471, acc.: 85.94%] [G loss: 3.021937]\n",
      "129 [D loss: 0.293371, acc.: 90.62%] [G loss: 2.713576]\n",
      "130 [D loss: 0.278769, acc.: 89.84%] [G loss: 2.586835]\n",
      "131 [D loss: 0.296107, acc.: 92.19%] [G loss: 2.282788]\n",
      "132 [D loss: 0.643622, acc.: 91.41%] [G loss: 1.821736]\n",
      "133 [D loss: 0.409164, acc.: 93.75%] [G loss: 1.653805]\n",
      "134 [D loss: 0.237073, acc.: 85.94%] [G loss: 1.823355]\n",
      "135 [D loss: 0.387689, acc.: 82.81%] [G loss: 2.256285]\n",
      "136 [D loss: 0.520967, acc.: 79.69%] [G loss: 2.513407]\n",
      "137 [D loss: 0.282232, acc.: 84.38%] [G loss: 3.090906]\n",
      "138 [D loss: 0.562907, acc.: 76.56%] [G loss: 3.410739]\n",
      "139 [D loss: 0.438817, acc.: 91.41%] [G loss: 3.060375]\n",
      "140 [D loss: 0.636809, acc.: 92.19%] [G loss: 2.634835]\n",
      "141 [D loss: 0.277495, acc.: 91.41%] [G loss: 2.346567]\n",
      "142 [D loss: 0.548659, acc.: 89.84%] [G loss: 2.273352]\n",
      "143 [D loss: 0.677897, acc.: 75.00%] [G loss: 2.076724]\n",
      "144 [D loss: 0.336970, acc.: 79.69%] [G loss: 2.508891]\n",
      "145 [D loss: 0.439206, acc.: 84.38%] [G loss: 2.455794]\n",
      "146 [D loss: 0.402421, acc.: 84.38%] [G loss: 2.501852]\n",
      "147 [D loss: 0.271449, acc.: 84.38%] [G loss: 2.387810]\n",
      "148 [D loss: 0.607962, acc.: 83.59%] [G loss: 2.376400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.304097, acc.: 89.84%] [G loss: 2.373237]\n",
      "150 [D loss: 0.545925, acc.: 90.62%] [G loss: 2.103318]\n",
      "151 [D loss: 0.261496, acc.: 97.66%] [G loss: 1.915032]\n",
      "152 [D loss: 0.624142, acc.: 83.59%] [G loss: 2.041608]\n",
      "153 [D loss: 0.307638, acc.: 91.41%] [G loss: 2.048768]\n",
      "154 [D loss: 0.175036, acc.: 91.41%] [G loss: 2.168442]\n",
      "155 [D loss: 0.119751, acc.: 96.09%] [G loss: 1.873957]\n",
      "156 [D loss: 0.184084, acc.: 93.75%] [G loss: 1.900365]\n",
      "157 [D loss: 0.505717, acc.: 86.72%] [G loss: 2.445364]\n",
      "158 [D loss: 0.114467, acc.: 96.88%] [G loss: 2.246426]\n",
      "159 [D loss: 0.196836, acc.: 92.97%] [G loss: 2.550131]\n",
      "160 [D loss: 0.109289, acc.: 96.88%] [G loss: 2.593992]\n",
      "161 [D loss: 0.122813, acc.: 95.31%] [G loss: 2.254257]\n",
      "162 [D loss: 0.237607, acc.: 90.62%] [G loss: 2.534153]\n",
      "163 [D loss: 0.423726, acc.: 92.19%] [G loss: 2.542685]\n",
      "164 [D loss: 0.285438, acc.: 82.03%] [G loss: 2.851692]\n",
      "165 [D loss: 0.535031, acc.: 72.66%] [G loss: 4.077628]\n",
      "166 [D loss: 0.898252, acc.: 77.34%] [G loss: 4.804823]\n",
      "167 [D loss: 0.410307, acc.: 90.62%] [G loss: 4.803261]\n",
      "168 [D loss: 0.397666, acc.: 90.62%] [G loss: 4.310652]\n",
      "169 [D loss: 0.309566, acc.: 98.44%] [G loss: 2.997548]\n",
      "170 [D loss: 0.305846, acc.: 91.41%] [G loss: 2.470634]\n",
      "171 [D loss: 0.387367, acc.: 93.75%] [G loss: 2.215973]\n",
      "172 [D loss: 0.329093, acc.: 90.62%] [G loss: 2.459455]\n",
      "173 [D loss: 0.248320, acc.: 95.31%] [G loss: 2.438860]\n",
      "174 [D loss: 0.152358, acc.: 90.62%] [G loss: 2.340921]\n",
      "175 [D loss: 0.148652, acc.: 92.97%] [G loss: 2.290271]\n",
      "176 [D loss: 0.273316, acc.: 95.31%] [G loss: 2.159263]\n",
      "177 [D loss: 0.433873, acc.: 91.41%] [G loss: 2.426076]\n",
      "178 [D loss: 0.292870, acc.: 92.19%] [G loss: 2.401907]\n",
      "179 [D loss: 0.105605, acc.: 98.44%] [G loss: 2.426269]\n",
      "180 [D loss: 0.761297, acc.: 91.41%] [G loss: 2.394547]\n",
      "181 [D loss: 0.264272, acc.: 95.31%] [G loss: 2.585513]\n",
      "182 [D loss: 0.644567, acc.: 89.84%] [G loss: 2.340026]\n",
      "183 [D loss: 0.400056, acc.: 92.19%] [G loss: 2.672333]\n",
      "184 [D loss: 0.391186, acc.: 92.19%] [G loss: 2.274674]\n",
      "185 [D loss: 0.207381, acc.: 87.50%] [G loss: 2.812239]\n",
      "186 [D loss: 0.499128, acc.: 91.41%] [G loss: 2.772536]\n",
      "187 [D loss: 0.207905, acc.: 98.44%] [G loss: 2.829896]\n",
      "188 [D loss: 0.370522, acc.: 94.53%] [G loss: 2.857643]\n",
      "189 [D loss: 0.344103, acc.: 94.53%] [G loss: 2.757222]\n",
      "190 [D loss: 0.604623, acc.: 83.59%] [G loss: 2.858634]\n",
      "191 [D loss: 0.337179, acc.: 88.28%] [G loss: 2.749526]\n",
      "192 [D loss: 0.655784, acc.: 83.59%] [G loss: 3.617066]\n",
      "193 [D loss: 0.558783, acc.: 80.47%] [G loss: 4.609375]\n",
      "194 [D loss: 0.157715, acc.: 91.41%] [G loss: 4.850065]\n",
      "195 [D loss: 0.400833, acc.: 97.66%] [G loss: 3.787823]\n",
      "196 [D loss: 0.282705, acc.: 98.44%] [G loss: 3.092843]\n",
      "197 [D loss: 0.612407, acc.: 93.75%] [G loss: 2.594594]\n",
      "198 [D loss: 0.634212, acc.: 92.19%] [G loss: 2.791646]\n",
      "199 [D loss: 0.324934, acc.: 97.66%] [G loss: 2.683007]\n",
      "200 [D loss: 0.861956, acc.: 92.97%] [G loss: 2.510659]\n",
      "201 [D loss: 0.460984, acc.: 95.31%] [G loss: 2.341815]\n",
      "202 [D loss: 0.378336, acc.: 93.75%] [G loss: 2.246714]\n",
      "203 [D loss: 0.407281, acc.: 90.62%] [G loss: 2.517737]\n",
      "204 [D loss: 0.672630, acc.: 89.06%] [G loss: 2.411300]\n",
      "205 [D loss: 0.703810, acc.: 81.25%] [G loss: 3.617121]\n",
      "206 [D loss: 0.924010, acc.: 80.47%] [G loss: 3.671337]\n",
      "207 [D loss: 0.225790, acc.: 94.53%] [G loss: 3.834285]\n",
      "208 [D loss: 0.692105, acc.: 95.31%] [G loss: 3.634470]\n",
      "209 [D loss: 0.160187, acc.: 98.44%] [G loss: 3.113712]\n",
      "210 [D loss: 0.203993, acc.: 97.66%] [G loss: 2.545160]\n",
      "211 [D loss: 0.370078, acc.: 95.31%] [G loss: 2.600892]\n",
      "212 [D loss: 0.467392, acc.: 95.31%] [G loss: 2.491692]\n",
      "213 [D loss: 0.343531, acc.: 95.31%] [G loss: 2.519017]\n",
      "214 [D loss: 0.209936, acc.: 96.88%] [G loss: 2.513952]\n",
      "215 [D loss: 0.304817, acc.: 98.44%] [G loss: 2.808771]\n",
      "216 [D loss: 0.343160, acc.: 94.53%] [G loss: 2.681195]\n",
      "217 [D loss: 0.675982, acc.: 87.50%] [G loss: 2.910178]\n",
      "218 [D loss: 0.201670, acc.: 92.19%] [G loss: 2.827208]\n",
      "219 [D loss: 0.920959, acc.: 89.84%] [G loss: 3.050755]\n",
      "220 [D loss: 0.385343, acc.: 90.62%] [G loss: 3.245711]\n",
      "221 [D loss: 0.284635, acc.: 97.66%] [G loss: 3.319741]\n",
      "222 [D loss: 0.474466, acc.: 93.75%] [G loss: 2.873774]\n",
      "223 [D loss: 0.559377, acc.: 96.09%] [G loss: 2.588441]\n",
      "224 [D loss: 0.562057, acc.: 96.88%] [G loss: 2.426779]\n",
      "225 [D loss: 0.077164, acc.: 98.44%] [G loss: 2.281939]\n",
      "226 [D loss: 0.946108, acc.: 93.75%] [G loss: 2.289864]\n",
      "227 [D loss: 0.091699, acc.: 98.44%] [G loss: 2.314161]\n",
      "228 [D loss: 0.332047, acc.: 95.31%] [G loss: 2.585610]\n",
      "229 [D loss: 0.637798, acc.: 91.41%] [G loss: 3.095921]\n",
      "230 [D loss: 0.678798, acc.: 95.31%] [G loss: 3.120324]\n",
      "231 [D loss: 0.923063, acc.: 94.53%] [G loss: 2.783421]\n",
      "232 [D loss: 0.320786, acc.: 97.66%] [G loss: 2.654804]\n",
      "233 [D loss: 0.214313, acc.: 96.88%] [G loss: 2.575047]\n",
      "234 [D loss: 0.513899, acc.: 92.97%] [G loss: 2.846462]\n",
      "235 [D loss: 0.524902, acc.: 84.38%] [G loss: 3.243137]\n",
      "236 [D loss: 0.602986, acc.: 79.69%] [G loss: 4.723310]\n",
      "237 [D loss: 0.821845, acc.: 82.81%] [G loss: 5.310343]\n",
      "238 [D loss: 0.508328, acc.: 96.88%] [G loss: 4.936191]\n",
      "239 [D loss: 0.401302, acc.: 97.66%] [G loss: 3.964907]\n",
      "240 [D loss: 0.310377, acc.: 97.66%] [G loss: 3.478822]\n",
      "241 [D loss: 0.785155, acc.: 95.31%] [G loss: 3.071155]\n",
      "242 [D loss: 0.560289, acc.: 96.09%] [G loss: 2.900451]\n",
      "243 [D loss: 0.693953, acc.: 95.31%] [G loss: 2.746949]\n",
      "244 [D loss: 0.447951, acc.: 96.09%] [G loss: 2.753864]\n",
      "245 [D loss: 0.435511, acc.: 96.88%] [G loss: 2.792647]\n",
      "246 [D loss: 0.707898, acc.: 93.75%] [G loss: 1.743283]\n",
      "247 [D loss: 0.357739, acc.: 90.62%] [G loss: 2.101609]\n",
      "248 [D loss: 0.497063, acc.: 78.91%] [G loss: 2.927202]\n",
      "249 [D loss: 0.585649, acc.: 84.38%] [G loss: 3.795330]\n",
      "250 [D loss: 0.162679, acc.: 98.44%] [G loss: 4.097532]\n",
      "251 [D loss: 0.100782, acc.: 98.44%] [G loss: 2.790711]\n",
      "252 [D loss: 0.118062, acc.: 96.88%] [G loss: 2.215784]\n",
      "253 [D loss: 0.244171, acc.: 96.88%] [G loss: 2.426699]\n",
      "254 [D loss: 0.153730, acc.: 92.97%] [G loss: 2.731115]\n",
      "255 [D loss: 0.369932, acc.: 88.28%] [G loss: 2.775710]\n",
      "256 [D loss: 0.386252, acc.: 83.59%] [G loss: 3.054616]\n",
      "257 [D loss: 0.252983, acc.: 83.59%] [G loss: 3.458046]\n",
      "258 [D loss: 0.041227, acc.: 99.22%] [G loss: 3.428343]\n",
      "259 [D loss: 0.030931, acc.: 100.00%] [G loss: 2.686656]\n",
      "260 [D loss: 0.061273, acc.: 99.22%] [G loss: 2.118264]\n",
      "261 [D loss: 0.262327, acc.: 92.97%] [G loss: 2.001382]\n",
      "262 [D loss: 0.097204, acc.: 96.88%] [G loss: 1.868148]\n",
      "263 [D loss: 0.151985, acc.: 96.09%] [G loss: 1.952630]\n",
      "264 [D loss: 0.288622, acc.: 80.47%] [G loss: 2.716898]\n",
      "265 [D loss: 1.029666, acc.: 78.12%] [G loss: 2.812247]\n",
      "266 [D loss: 0.971012, acc.: 71.88%] [G loss: 3.848162]\n",
      "267 [D loss: 0.156216, acc.: 91.41%] [G loss: 4.828065]\n",
      "268 [D loss: 0.028969, acc.: 100.00%] [G loss: 3.353010]\n",
      "269 [D loss: 0.678894, acc.: 96.09%] [G loss: 2.767669]\n",
      "270 [D loss: 0.139679, acc.: 96.09%] [G loss: 2.460838]\n",
      "271 [D loss: 0.228327, acc.: 99.22%] [G loss: 2.052200]\n",
      "272 [D loss: 0.304086, acc.: 92.19%] [G loss: 1.934969]\n",
      "273 [D loss: 0.786518, acc.: 63.28%] [G loss: 1.171476]\n",
      "274 [D loss: 0.614529, acc.: 77.34%] [G loss: 1.880754]\n",
      "275 [D loss: 0.566927, acc.: 75.78%] [G loss: 3.399965]\n",
      "276 [D loss: 0.492620, acc.: 78.12%] [G loss: 3.624839]\n",
      "277 [D loss: 0.458745, acc.: 82.03%] [G loss: 3.371635]\n",
      "278 [D loss: 0.447779, acc.: 81.25%] [G loss: 2.906304]\n",
      "279 [D loss: 0.204423, acc.: 93.75%] [G loss: 2.355895]\n",
      "280 [D loss: 0.291266, acc.: 85.94%] [G loss: 2.254894]\n",
      "281 [D loss: 0.316620, acc.: 86.72%] [G loss: 1.740734]\n",
      "282 [D loss: 0.414643, acc.: 79.69%] [G loss: 2.207050]\n",
      "283 [D loss: 0.472997, acc.: 78.91%] [G loss: 2.247536]\n",
      "284 [D loss: 0.300790, acc.: 83.59%] [G loss: 2.473086]\n",
      "285 [D loss: 0.283600, acc.: 85.16%] [G loss: 2.266504]\n",
      "286 [D loss: 0.329163, acc.: 81.25%] [G loss: 1.875613]\n",
      "287 [D loss: 0.369417, acc.: 75.78%] [G loss: 2.104074]\n",
      "288 [D loss: 0.473007, acc.: 81.25%] [G loss: 2.487774]\n",
      "289 [D loss: 0.257293, acc.: 89.06%] [G loss: 2.228464]\n",
      "290 [D loss: 0.184071, acc.: 90.62%] [G loss: 2.026490]\n",
      "291 [D loss: 0.418917, acc.: 82.81%] [G loss: 2.076137]\n",
      "292 [D loss: 0.251111, acc.: 89.06%] [G loss: 1.929990]\n",
      "293 [D loss: 0.434022, acc.: 82.03%] [G loss: 2.057022]\n",
      "294 [D loss: 0.319148, acc.: 82.03%] [G loss: 2.227901]\n",
      "295 [D loss: 0.482782, acc.: 78.12%] [G loss: 2.440845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.233318, acc.: 90.62%] [G loss: 2.696869]\n",
      "297 [D loss: 0.488544, acc.: 80.47%] [G loss: 3.029508]\n",
      "298 [D loss: 0.304855, acc.: 90.62%] [G loss: 3.038410]\n",
      "299 [D loss: 0.243255, acc.: 89.84%] [G loss: 3.007699]\n",
      "300 [D loss: 0.298765, acc.: 82.81%] [G loss: 2.413016]\n",
      "301 [D loss: 0.266468, acc.: 85.16%] [G loss: 2.483364]\n",
      "302 [D loss: 0.619581, acc.: 85.16%] [G loss: 2.549067]\n",
      "303 [D loss: 0.148164, acc.: 93.75%] [G loss: 2.310328]\n",
      "304 [D loss: 0.679864, acc.: 77.34%] [G loss: 3.085741]\n",
      "305 [D loss: 0.259553, acc.: 88.28%] [G loss: 3.411243]\n",
      "306 [D loss: 1.010815, acc.: 75.78%] [G loss: 3.603214]\n",
      "307 [D loss: 0.560024, acc.: 82.81%] [G loss: 3.783799]\n",
      "308 [D loss: 0.250909, acc.: 94.53%] [G loss: 3.459198]\n",
      "309 [D loss: 0.683203, acc.: 82.03%] [G loss: 1.729077]\n",
      "310 [D loss: 0.543766, acc.: 79.69%] [G loss: 2.459729]\n",
      "311 [D loss: 0.280957, acc.: 94.53%] [G loss: 2.777164]\n",
      "312 [D loss: 0.346759, acc.: 87.50%] [G loss: 2.938316]\n",
      "313 [D loss: 0.520501, acc.: 82.03%] [G loss: 3.211484]\n",
      "314 [D loss: 0.446558, acc.: 82.03%] [G loss: 3.637735]\n",
      "315 [D loss: 0.530044, acc.: 75.78%] [G loss: 4.605939]\n",
      "316 [D loss: 0.212712, acc.: 96.88%] [G loss: 4.370711]\n",
      "317 [D loss: 0.356856, acc.: 94.53%] [G loss: 3.717105]\n",
      "318 [D loss: 0.392737, acc.: 91.41%] [G loss: 3.169178]\n",
      "319 [D loss: 0.410731, acc.: 92.97%] [G loss: 3.186459]\n",
      "320 [D loss: 0.527960, acc.: 91.41%] [G loss: 2.578820]\n",
      "321 [D loss: 0.415921, acc.: 89.84%] [G loss: 2.660186]\n",
      "322 [D loss: 0.340319, acc.: 89.84%] [G loss: 2.690159]\n",
      "323 [D loss: 0.515698, acc.: 92.97%] [G loss: 2.628083]\n",
      "324 [D loss: 0.739495, acc.: 95.31%] [G loss: 2.449140]\n",
      "325 [D loss: 0.605513, acc.: 94.53%] [G loss: 2.288393]\n",
      "326 [D loss: 0.168036, acc.: 91.41%] [G loss: 2.356060]\n",
      "327 [D loss: 0.594079, acc.: 87.50%] [G loss: 3.005653]\n",
      "328 [D loss: 0.118530, acc.: 94.53%] [G loss: 2.973863]\n",
      "329 [D loss: 0.625685, acc.: 90.62%] [G loss: 3.048572]\n",
      "330 [D loss: 0.147452, acc.: 93.75%] [G loss: 3.527319]\n",
      "331 [D loss: 0.555774, acc.: 91.41%] [G loss: 3.798305]\n",
      "332 [D loss: 0.547874, acc.: 78.91%] [G loss: 4.628094]\n",
      "333 [D loss: 0.489488, acc.: 85.16%] [G loss: 4.917127]\n",
      "334 [D loss: 0.545020, acc.: 96.09%] [G loss: 4.649199]\n",
      "335 [D loss: 0.607126, acc.: 92.97%] [G loss: 4.059351]\n",
      "336 [D loss: 0.446495, acc.: 95.31%] [G loss: 3.790422]\n",
      "337 [D loss: 0.562574, acc.: 96.09%] [G loss: 3.260220]\n",
      "338 [D loss: 0.263351, acc.: 91.41%] [G loss: 3.276507]\n",
      "339 [D loss: 0.403749, acc.: 91.41%] [G loss: 3.647230]\n",
      "340 [D loss: 0.627242, acc.: 92.19%] [G loss: 3.667705]\n",
      "341 [D loss: 0.716383, acc.: 92.97%] [G loss: 3.329510]\n",
      "342 [D loss: 0.363386, acc.: 96.09%] [G loss: 2.968820]\n",
      "343 [D loss: 0.502454, acc.: 88.28%] [G loss: 2.750357]\n",
      "344 [D loss: 0.324753, acc.: 93.75%] [G loss: 3.156267]\n",
      "345 [D loss: 0.441261, acc.: 81.25%] [G loss: 3.595570]\n",
      "346 [D loss: 0.872911, acc.: 82.03%] [G loss: 4.512666]\n",
      "347 [D loss: 0.356426, acc.: 93.75%] [G loss: 4.460550]\n",
      "348 [D loss: 0.186493, acc.: 98.44%] [G loss: 4.053906]\n",
      "349 [D loss: 0.412863, acc.: 97.66%] [G loss: 3.454090]\n",
      "350 [D loss: 0.188702, acc.: 98.44%] [G loss: 3.177659]\n",
      "351 [D loss: 0.072179, acc.: 99.22%] [G loss: 2.717447]\n",
      "352 [D loss: 0.348107, acc.: 96.09%] [G loss: 2.738890]\n",
      "353 [D loss: 0.347057, acc.: 97.66%] [G loss: 2.862983]\n",
      "354 [D loss: 0.080631, acc.: 96.88%] [G loss: 2.751047]\n",
      "355 [D loss: 0.548827, acc.: 90.62%] [G loss: 3.193507]\n",
      "356 [D loss: 0.663291, acc.: 85.16%] [G loss: 3.906164]\n",
      "357 [D loss: 0.974240, acc.: 74.22%] [G loss: 5.338700]\n",
      "358 [D loss: 0.757757, acc.: 77.34%] [G loss: 6.275377]\n",
      "359 [D loss: 0.255173, acc.: 93.75%] [G loss: 6.495009]\n",
      "360 [D loss: 0.180507, acc.: 96.09%] [G loss: 5.942167]\n",
      "361 [D loss: 0.166728, acc.: 98.44%] [G loss: 5.417213]\n",
      "362 [D loss: 0.043080, acc.: 99.22%] [G loss: 4.517264]\n",
      "363 [D loss: 0.553384, acc.: 96.88%] [G loss: 3.785541]\n",
      "364 [D loss: 0.573076, acc.: 95.31%] [G loss: 3.362438]\n",
      "365 [D loss: 0.338075, acc.: 96.88%] [G loss: 3.131609]\n",
      "366 [D loss: 0.314601, acc.: 98.44%] [G loss: 2.793424]\n",
      "367 [D loss: 0.105902, acc.: 98.44%] [G loss: 2.701082]\n",
      "368 [D loss: 0.356112, acc.: 94.53%] [G loss: 2.742710]\n",
      "369 [D loss: 0.502800, acc.: 91.41%] [G loss: 2.934218]\n",
      "370 [D loss: 0.233169, acc.: 96.09%] [G loss: 3.490949]\n",
      "371 [D loss: 0.323200, acc.: 96.88%] [G loss: 3.557760]\n",
      "372 [D loss: 0.808607, acc.: 94.53%] [G loss: 3.379946]\n",
      "373 [D loss: 0.204100, acc.: 96.88%] [G loss: 2.961061]\n",
      "374 [D loss: 0.372629, acc.: 94.53%] [G loss: 2.961472]\n",
      "375 [D loss: 0.804554, acc.: 85.94%] [G loss: 3.842534]\n",
      "376 [D loss: 0.840410, acc.: 82.03%] [G loss: 5.119119]\n",
      "377 [D loss: 0.807161, acc.: 94.53%] [G loss: 5.086285]\n",
      "378 [D loss: 0.412403, acc.: 89.84%] [G loss: 5.322960]\n",
      "379 [D loss: 0.664108, acc.: 95.31%] [G loss: 5.188903]\n",
      "380 [D loss: 0.562608, acc.: 96.88%] [G loss: 3.966663]\n",
      "381 [D loss: 0.550578, acc.: 96.09%] [G loss: 3.557447]\n",
      "382 [D loss: 0.410929, acc.: 97.66%] [G loss: 3.097676]\n",
      "383 [D loss: 0.567852, acc.: 96.09%] [G loss: 2.678097]\n",
      "384 [D loss: 0.558614, acc.: 95.31%] [G loss: 2.514099]\n",
      "385 [D loss: 0.194081, acc.: 98.44%] [G loss: 2.515046]\n",
      "386 [D loss: 0.837654, acc.: 95.31%] [G loss: 3.080014]\n",
      "387 [D loss: 0.061636, acc.: 98.44%] [G loss: 2.973828]\n",
      "388 [D loss: 0.188982, acc.: 97.66%] [G loss: 2.912927]\n",
      "389 [D loss: 0.507782, acc.: 93.75%] [G loss: 2.872001]\n",
      "390 [D loss: 0.512418, acc.: 93.75%] [G loss: 3.266472]\n",
      "391 [D loss: 0.163437, acc.: 98.44%] [G loss: 3.453501]\n",
      "392 [D loss: 0.411152, acc.: 97.66%] [G loss: 2.899255]\n",
      "393 [D loss: 0.935976, acc.: 94.53%] [G loss: 2.899919]\n",
      "394 [D loss: 0.444188, acc.: 96.88%] [G loss: 3.035317]\n",
      "395 [D loss: 0.423463, acc.: 97.66%] [G loss: 2.925662]\n",
      "396 [D loss: 0.164427, acc.: 99.22%] [G loss: 2.882875]\n",
      "397 [D loss: 0.289747, acc.: 98.44%] [G loss: 2.978358]\n",
      "398 [D loss: 0.460675, acc.: 96.88%] [G loss: 2.364966]\n",
      "399 [D loss: 0.145038, acc.: 94.53%] [G loss: 2.362542]\n",
      "400 [D loss: 0.343457, acc.: 87.50%] [G loss: 4.553836]\n",
      "401 [D loss: 0.763069, acc.: 82.03%] [G loss: 6.135532]\n",
      "402 [D loss: 0.256389, acc.: 98.44%] [G loss: 5.834584]\n",
      "403 [D loss: 0.169901, acc.: 97.66%] [G loss: 4.507524]\n",
      "404 [D loss: 0.405539, acc.: 96.88%] [G loss: 3.886216]\n",
      "405 [D loss: 0.160991, acc.: 99.22%] [G loss: 3.086188]\n",
      "406 [D loss: 0.065571, acc.: 99.22%] [G loss: 2.748903]\n",
      "407 [D loss: 0.186592, acc.: 98.44%] [G loss: 2.590221]\n",
      "408 [D loss: 0.689388, acc.: 94.53%] [G loss: 2.469096]\n",
      "409 [D loss: 0.208797, acc.: 96.88%] [G loss: 2.598386]\n",
      "410 [D loss: 0.163253, acc.: 99.22%] [G loss: 2.764564]\n",
      "411 [D loss: 0.204238, acc.: 97.66%] [G loss: 2.986936]\n",
      "412 [D loss: 0.225320, acc.: 96.88%] [G loss: 2.943873]\n",
      "413 [D loss: 0.093892, acc.: 97.66%] [G loss: 3.174295]\n",
      "414 [D loss: 0.420548, acc.: 96.88%] [G loss: 3.271936]\n",
      "415 [D loss: 0.037422, acc.: 100.00%] [G loss: 3.094193]\n",
      "416 [D loss: 0.462448, acc.: 94.53%] [G loss: 3.075868]\n",
      "417 [D loss: 0.486417, acc.: 92.19%] [G loss: 3.177388]\n",
      "418 [D loss: 0.237757, acc.: 94.53%] [G loss: 3.177785]\n",
      "419 [D loss: 0.456046, acc.: 95.31%] [G loss: 3.361944]\n",
      "420 [D loss: 0.283426, acc.: 98.44%] [G loss: 3.328278]\n",
      "421 [D loss: 0.542512, acc.: 96.88%] [G loss: 2.821775]\n",
      "422 [D loss: 0.432750, acc.: 96.88%] [G loss: 2.530092]\n",
      "423 [D loss: 0.059540, acc.: 100.00%] [G loss: 2.464183]\n",
      "424 [D loss: 0.305611, acc.: 97.66%] [G loss: 2.505765]\n",
      "425 [D loss: 0.424878, acc.: 97.66%] [G loss: 2.712546]\n",
      "426 [D loss: 0.556593, acc.: 96.88%] [G loss: 2.846856]\n",
      "427 [D loss: 0.037142, acc.: 100.00%] [G loss: 3.105742]\n",
      "428 [D loss: 0.320626, acc.: 97.66%] [G loss: 2.983219]\n",
      "429 [D loss: 0.312833, acc.: 97.66%] [G loss: 2.950328]\n",
      "430 [D loss: 0.076235, acc.: 99.22%] [G loss: 2.826569]\n",
      "431 [D loss: 0.098085, acc.: 95.31%] [G loss: 3.279110]\n",
      "432 [D loss: 0.445550, acc.: 90.62%] [G loss: 4.503562]\n",
      "433 [D loss: 0.691281, acc.: 80.47%] [G loss: 4.670182]\n",
      "434 [D loss: 0.444955, acc.: 85.94%] [G loss: 7.423330]\n",
      "435 [D loss: 0.132999, acc.: 99.22%] [G loss: 6.591484]\n",
      "436 [D loss: 0.147380, acc.: 99.22%] [G loss: 5.626694]\n",
      "437 [D loss: 0.151139, acc.: 99.22%] [G loss: 4.611142]\n",
      "438 [D loss: 0.153925, acc.: 99.22%] [G loss: 3.823987]\n",
      "439 [D loss: 0.153288, acc.: 99.22%] [G loss: 3.326972]\n",
      "440 [D loss: 0.533620, acc.: 96.88%] [G loss: 2.854829]\n",
      "441 [D loss: 0.325228, acc.: 97.66%] [G loss: 3.027502]\n",
      "442 [D loss: 0.174954, acc.: 98.44%] [G loss: 2.994065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 0.309423, acc.: 98.44%] [G loss: 2.699585]\n",
      "444 [D loss: 0.174925, acc.: 99.22%] [G loss: 2.549950]\n",
      "445 [D loss: 0.292203, acc.: 98.44%] [G loss: 2.584862]\n",
      "446 [D loss: 0.178544, acc.: 99.22%] [G loss: 2.627130]\n",
      "447 [D loss: 0.294543, acc.: 98.44%] [G loss: 2.443122]\n",
      "448 [D loss: 0.313947, acc.: 98.44%] [G loss: 2.455536]\n",
      "449 [D loss: 0.308894, acc.: 96.88%] [G loss: 2.320000]\n",
      "450 [D loss: 0.336272, acc.: 97.66%] [G loss: 2.507941]\n",
      "451 [D loss: 0.294615, acc.: 98.44%] [G loss: 2.702210]\n",
      "452 [D loss: 0.152754, acc.: 99.22%] [G loss: 2.542645]\n",
      "453 [D loss: 0.414723, acc.: 97.66%] [G loss: 2.784241]\n",
      "454 [D loss: 0.413032, acc.: 97.66%] [G loss: 3.149680]\n",
      "455 [D loss: 0.797377, acc.: 95.31%] [G loss: 3.220088]\n",
      "456 [D loss: 0.558517, acc.: 96.09%] [G loss: 2.643678]\n",
      "457 [D loss: 0.635619, acc.: 93.75%] [G loss: 1.984150]\n",
      "458 [D loss: 0.338425, acc.: 98.44%] [G loss: 2.428814]\n",
      "459 [D loss: 0.305279, acc.: 97.66%] [G loss: 2.671984]\n",
      "460 [D loss: 0.434089, acc.: 96.88%] [G loss: 2.809492]\n",
      "461 [D loss: 0.297796, acc.: 98.44%] [G loss: 2.649642]\n",
      "462 [D loss: 0.183156, acc.: 97.66%] [G loss: 2.669920]\n",
      "463 [D loss: 0.207495, acc.: 97.66%] [G loss: 2.488248]\n",
      "464 [D loss: 0.613198, acc.: 94.53%] [G loss: 3.086751]\n",
      "465 [D loss: 0.589823, acc.: 94.53%] [G loss: 3.499384]\n",
      "466 [D loss: 0.146925, acc.: 99.22%] [G loss: 3.421499]\n",
      "467 [D loss: 0.033128, acc.: 100.00%] [G loss: 3.122516]\n",
      "468 [D loss: 0.025651, acc.: 100.00%] [G loss: 2.687039]\n",
      "469 [D loss: 0.160783, acc.: 99.22%] [G loss: 2.542395]\n",
      "470 [D loss: 0.157636, acc.: 98.44%] [G loss: 2.587425]\n",
      "471 [D loss: 0.155367, acc.: 99.22%] [G loss: 2.620807]\n",
      "472 [D loss: 0.152454, acc.: 99.22%] [G loss: 2.614733]\n",
      "473 [D loss: 0.290169, acc.: 98.44%] [G loss: 2.486308]\n",
      "474 [D loss: 0.162553, acc.: 99.22%] [G loss: 2.527858]\n",
      "475 [D loss: 0.442816, acc.: 95.31%] [G loss: 2.479683]\n",
      "476 [D loss: 0.453637, acc.: 97.66%] [G loss: 2.778370]\n",
      "477 [D loss: 0.099661, acc.: 96.09%] [G loss: 2.452179]\n",
      "478 [D loss: 0.708539, acc.: 94.53%] [G loss: 2.997524]\n",
      "479 [D loss: 0.018911, acc.: 100.00%] [G loss: 3.290940]\n",
      "480 [D loss: 0.277740, acc.: 98.44%] [G loss: 2.783981]\n",
      "481 [D loss: 0.284482, acc.: 98.44%] [G loss: 2.481223]\n",
      "482 [D loss: 0.042656, acc.: 100.00%] [G loss: 2.309039]\n",
      "483 [D loss: 0.171318, acc.: 99.22%] [G loss: 2.400598]\n",
      "484 [D loss: 0.284923, acc.: 98.44%] [G loss: 2.595662]\n",
      "485 [D loss: 0.406800, acc.: 97.66%] [G loss: 2.644989]\n",
      "486 [D loss: 0.793261, acc.: 95.31%] [G loss: 2.830637]\n",
      "487 [D loss: 0.311846, acc.: 98.44%] [G loss: 2.486989]\n",
      "488 [D loss: 0.306162, acc.: 97.66%] [G loss: 2.425658]\n",
      "489 [D loss: 0.062089, acc.: 100.00%] [G loss: 2.590936]\n",
      "490 [D loss: 0.306672, acc.: 98.44%] [G loss: 2.765733]\n",
      "491 [D loss: 0.404522, acc.: 97.66%] [G loss: 2.678606]\n",
      "492 [D loss: 0.159071, acc.: 99.22%] [G loss: 2.671333]\n",
      "493 [D loss: 0.536475, acc.: 96.88%] [G loss: 2.571000]\n",
      "494 [D loss: 0.162993, acc.: 99.22%] [G loss: 2.414272]\n",
      "495 [D loss: 0.194797, acc.: 99.22%] [G loss: 2.689825]\n",
      "496 [D loss: 0.454315, acc.: 95.31%] [G loss: 2.890260]\n",
      "497 [D loss: 0.565340, acc.: 96.09%] [G loss: 2.713653]\n",
      "498 [D loss: 0.025348, acc.: 100.00%] [G loss: 2.760309]\n",
      "499 [D loss: 0.078177, acc.: 99.22%] [G loss: 2.236389]\n",
      "500 [D loss: 0.343009, acc.: 96.88%] [G loss: 2.458810]\n",
      "501 [D loss: 0.605675, acc.: 95.31%] [G loss: 3.116089]\n",
      "502 [D loss: 0.411768, acc.: 89.06%] [G loss: 3.612568]\n",
      "503 [D loss: 0.307041, acc.: 96.88%] [G loss: 3.975577]\n",
      "504 [D loss: 0.388895, acc.: 97.66%] [G loss: 3.484597]\n",
      "505 [D loss: 0.144164, acc.: 99.22%] [G loss: 3.190498]\n",
      "506 [D loss: 0.146505, acc.: 99.22%] [G loss: 2.825300]\n",
      "507 [D loss: 0.402999, acc.: 97.66%] [G loss: 2.867084]\n",
      "508 [D loss: 0.399630, acc.: 97.66%] [G loss: 2.808769]\n",
      "509 [D loss: 0.030666, acc.: 100.00%] [G loss: 2.732396]\n",
      "510 [D loss: 0.034601, acc.: 100.00%] [G loss: 2.697599]\n",
      "511 [D loss: 0.428345, acc.: 96.88%] [G loss: 2.667728]\n",
      "512 [D loss: 0.040177, acc.: 100.00%] [G loss: 2.661419]\n",
      "513 [D loss: 0.045309, acc.: 100.00%] [G loss: 2.382880]\n",
      "514 [D loss: 0.695581, acc.: 96.09%] [G loss: 2.406833]\n",
      "515 [D loss: 0.070310, acc.: 100.00%] [G loss: 2.572104]\n",
      "516 [D loss: 0.031337, acc.: 100.00%] [G loss: 2.708284]\n",
      "517 [D loss: 0.277964, acc.: 98.44%] [G loss: 2.690045]\n",
      "518 [D loss: 0.157050, acc.: 99.22%] [G loss: 2.413142]\n",
      "519 [D loss: 0.288787, acc.: 98.44%] [G loss: 2.212219]\n",
      "520 [D loss: 0.176079, acc.: 98.44%] [G loss: 2.085919]\n",
      "521 [D loss: 0.298246, acc.: 98.44%] [G loss: 2.244780]\n",
      "522 [D loss: 0.161411, acc.: 99.22%] [G loss: 2.490974]\n",
      "523 [D loss: 0.029353, acc.: 100.00%] [G loss: 2.444189]\n",
      "524 [D loss: 0.332358, acc.: 94.53%] [G loss: 2.369484]\n",
      "525 [D loss: 0.511878, acc.: 93.75%] [G loss: 3.294512]\n",
      "526 [D loss: 0.181983, acc.: 98.44%] [G loss: 3.888611]\n",
      "527 [D loss: 0.144566, acc.: 99.22%] [G loss: 3.160368]\n",
      "528 [D loss: 0.030215, acc.: 100.00%] [G loss: 2.799030]\n",
      "529 [D loss: 0.280750, acc.: 98.44%] [G loss: 2.705304]\n",
      "530 [D loss: 0.167972, acc.: 98.44%] [G loss: 2.670514]\n",
      "531 [D loss: 0.282013, acc.: 98.44%] [G loss: 2.454978]\n",
      "532 [D loss: 0.173540, acc.: 99.22%] [G loss: 2.240963]\n",
      "533 [D loss: 0.437385, acc.: 97.66%] [G loss: 2.474080]\n",
      "534 [D loss: 0.342392, acc.: 97.66%] [G loss: 2.789089]\n",
      "535 [D loss: 0.910481, acc.: 94.53%] [G loss: 2.818821]\n",
      "536 [D loss: 0.024123, acc.: 100.00%] [G loss: 2.700403]\n",
      "537 [D loss: 0.270921, acc.: 98.44%] [G loss: 2.998609]\n",
      "538 [D loss: 0.153968, acc.: 99.22%] [G loss: 3.302976]\n",
      "539 [D loss: 0.148890, acc.: 99.22%] [G loss: 2.845811]\n",
      "540 [D loss: 0.170283, acc.: 99.22%] [G loss: 2.448203]\n",
      "541 [D loss: 0.307933, acc.: 98.44%] [G loss: 2.234738]\n",
      "542 [D loss: 0.197880, acc.: 98.44%] [G loss: 2.323966]\n",
      "543 [D loss: 0.433272, acc.: 97.66%] [G loss: 2.477388]\n",
      "544 [D loss: 0.299508, acc.: 98.44%] [G loss: 2.518418]\n",
      "545 [D loss: 0.669543, acc.: 96.09%] [G loss: 2.551182]\n",
      "546 [D loss: 0.290297, acc.: 98.44%] [G loss: 2.619167]\n",
      "547 [D loss: 0.026293, acc.: 100.00%] [G loss: 2.663545]\n",
      "548 [D loss: 0.524025, acc.: 96.88%] [G loss: 2.676643]\n",
      "549 [D loss: 0.020663, acc.: 100.00%] [G loss: 2.689465]\n",
      "550 [D loss: 0.415178, acc.: 97.66%] [G loss: 2.367248]\n",
      "551 [D loss: 0.414941, acc.: 97.66%] [G loss: 2.303126]\n",
      "552 [D loss: 0.178941, acc.: 98.44%] [G loss: 2.136678]\n",
      "553 [D loss: 0.426818, acc.: 97.66%] [G loss: 2.268861]\n",
      "554 [D loss: 0.276149, acc.: 98.44%] [G loss: 2.385330]\n",
      "555 [D loss: 0.280544, acc.: 98.44%] [G loss: 2.565602]\n",
      "556 [D loss: 0.527279, acc.: 96.88%] [G loss: 2.914142]\n",
      "557 [D loss: 0.393744, acc.: 97.66%] [G loss: 3.186118]\n",
      "558 [D loss: 0.391302, acc.: 96.88%] [G loss: 2.048023]\n",
      "559 [D loss: 0.299214, acc.: 91.41%] [G loss: 2.788363]\n",
      "560 [D loss: 0.668858, acc.: 78.91%] [G loss: 4.068824]\n",
      "561 [D loss: 1.119893, acc.: 83.59%] [G loss: 5.374991]\n",
      "562 [D loss: 0.640763, acc.: 82.03%] [G loss: 6.405707]\n",
      "563 [D loss: 0.307883, acc.: 89.06%] [G loss: 7.107655]\n",
      "564 [D loss: 0.001405, acc.: 100.00%] [G loss: 6.528541]\n",
      "565 [D loss: 0.130971, acc.: 99.22%] [G loss: 4.954690]\n",
      "566 [D loss: 0.019230, acc.: 100.00%] [G loss: 4.217019]\n",
      "567 [D loss: 0.139842, acc.: 99.22%] [G loss: 3.622362]\n",
      "568 [D loss: 0.020924, acc.: 100.00%] [G loss: 3.037109]\n",
      "569 [D loss: 0.159889, acc.: 99.22%] [G loss: 2.702039]\n",
      "570 [D loss: 0.036832, acc.: 100.00%] [G loss: 2.377817]\n",
      "571 [D loss: 0.325244, acc.: 97.66%] [G loss: 2.432113]\n",
      "572 [D loss: 0.174040, acc.: 99.22%] [G loss: 2.396559]\n",
      "573 [D loss: 0.289081, acc.: 98.44%] [G loss: 2.418599]\n",
      "574 [D loss: 0.316321, acc.: 98.44%] [G loss: 2.465275]\n",
      "575 [D loss: 0.040870, acc.: 100.00%] [G loss: 2.412750]\n",
      "576 [D loss: 0.050805, acc.: 100.00%] [G loss: 2.274253]\n",
      "577 [D loss: 0.187841, acc.: 99.22%] [G loss: 2.322917]\n",
      "578 [D loss: 0.295774, acc.: 98.44%] [G loss: 2.233897]\n",
      "579 [D loss: 0.306793, acc.: 98.44%] [G loss: 2.325253]\n",
      "580 [D loss: 0.168916, acc.: 99.22%] [G loss: 2.284068]\n",
      "581 [D loss: 0.295796, acc.: 98.44%] [G loss: 2.351904]\n",
      "582 [D loss: 0.053207, acc.: 100.00%] [G loss: 2.421341]\n",
      "583 [D loss: 0.047272, acc.: 100.00%] [G loss: 2.367355]\n",
      "584 [D loss: 0.190501, acc.: 99.22%] [G loss: 2.181198]\n",
      "585 [D loss: 0.198086, acc.: 99.22%] [G loss: 2.623344]\n",
      "586 [D loss: 0.410010, acc.: 97.66%] [G loss: 2.599982]\n",
      "587 [D loss: 0.308226, acc.: 96.88%] [G loss: 2.527500]\n",
      "588 [D loss: 0.308942, acc.: 98.44%] [G loss: 2.476129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589 [D loss: 0.172099, acc.: 99.22%] [G loss: 2.468400]\n",
      "590 [D loss: 0.292647, acc.: 97.66%] [G loss: 2.381739]\n",
      "591 [D loss: 0.170002, acc.: 99.22%] [G loss: 2.214268]\n",
      "592 [D loss: 0.305724, acc.: 98.44%] [G loss: 2.410449]\n",
      "593 [D loss: 0.408136, acc.: 97.66%] [G loss: 2.667385]\n",
      "594 [D loss: 0.157453, acc.: 99.22%] [G loss: 2.777940]\n",
      "595 [D loss: 0.420683, acc.: 97.66%] [G loss: 2.644997]\n",
      "596 [D loss: 0.225868, acc.: 98.44%] [G loss: 2.116907]\n",
      "597 [D loss: 0.082047, acc.: 100.00%] [G loss: 2.089635]\n",
      "598 [D loss: 0.312327, acc.: 97.66%] [G loss: 2.126152]\n",
      "599 [D loss: 0.329679, acc.: 96.09%] [G loss: 2.193193]\n",
      "600 [D loss: 0.132760, acc.: 96.09%] [G loss: 2.693415]\n",
      "601 [D loss: 0.857749, acc.: 78.12%] [G loss: 3.025905]\n",
      "602 [D loss: 1.875918, acc.: 77.34%] [G loss: 3.435950]\n",
      "603 [D loss: 0.766726, acc.: 81.25%] [G loss: 5.240262]\n",
      "604 [D loss: 0.555618, acc.: 82.81%] [G loss: 5.927012]\n",
      "605 [D loss: 0.116163, acc.: 94.53%] [G loss: 5.305459]\n",
      "606 [D loss: 0.136158, acc.: 92.97%] [G loss: 4.405152]\n",
      "607 [D loss: 0.089145, acc.: 96.88%] [G loss: 3.523307]\n",
      "608 [D loss: 0.081765, acc.: 98.44%] [G loss: 2.563917]\n",
      "609 [D loss: 0.261532, acc.: 89.06%] [G loss: 2.251690]\n",
      "610 [D loss: 0.339467, acc.: 82.81%] [G loss: 3.554004]\n",
      "611 [D loss: 0.424779, acc.: 86.72%] [G loss: 3.352027]\n",
      "612 [D loss: 0.350636, acc.: 81.25%] [G loss: 3.437526]\n",
      "613 [D loss: 0.228308, acc.: 85.16%] [G loss: 3.262661]\n",
      "614 [D loss: 0.228714, acc.: 87.50%] [G loss: 3.162051]\n",
      "615 [D loss: 0.362241, acc.: 85.16%] [G loss: 2.930981]\n",
      "616 [D loss: 0.322269, acc.: 82.81%] [G loss: 2.963907]\n",
      "617 [D loss: 0.441845, acc.: 79.69%] [G loss: 3.383000]\n",
      "618 [D loss: 0.580339, acc.: 75.78%] [G loss: 4.104246]\n",
      "619 [D loss: 1.041470, acc.: 78.12%] [G loss: 3.967153]\n",
      "620 [D loss: 0.154537, acc.: 93.75%] [G loss: 3.403006]\n",
      "621 [D loss: 0.519114, acc.: 80.47%] [G loss: 3.816817]\n",
      "622 [D loss: 0.528022, acc.: 82.81%] [G loss: 3.756241]\n",
      "623 [D loss: 0.328407, acc.: 85.16%] [G loss: 3.152733]\n",
      "624 [D loss: 0.403008, acc.: 85.94%] [G loss: 3.226298]\n",
      "625 [D loss: 0.373934, acc.: 87.50%] [G loss: 2.833401]\n",
      "626 [D loss: 0.527966, acc.: 79.69%] [G loss: 3.369454]\n",
      "627 [D loss: 0.098883, acc.: 95.31%] [G loss: 3.089844]\n",
      "628 [D loss: 0.278240, acc.: 86.72%] [G loss: 3.230974]\n",
      "629 [D loss: 0.448286, acc.: 89.06%] [G loss: 3.435459]\n",
      "630 [D loss: 0.217867, acc.: 91.41%] [G loss: 3.382260]\n",
      "631 [D loss: 0.346793, acc.: 85.94%] [G loss: 3.698489]\n",
      "632 [D loss: 0.271471, acc.: 89.84%] [G loss: 4.000999]\n",
      "633 [D loss: 0.488301, acc.: 77.34%] [G loss: 4.741282]\n",
      "634 [D loss: 0.240470, acc.: 94.53%] [G loss: 4.544952]\n",
      "635 [D loss: 0.338927, acc.: 96.09%] [G loss: 3.772504]\n",
      "636 [D loss: 0.346721, acc.: 96.09%] [G loss: 3.061994]\n",
      "637 [D loss: 0.236845, acc.: 87.50%] [G loss: 3.276808]\n",
      "638 [D loss: 0.510536, acc.: 92.97%] [G loss: 3.028095]\n",
      "639 [D loss: 0.120045, acc.: 95.31%] [G loss: 3.053405]\n",
      "640 [D loss: 0.484700, acc.: 85.94%] [G loss: 3.426696]\n",
      "641 [D loss: 0.336843, acc.: 96.88%] [G loss: 3.088976]\n",
      "642 [D loss: 0.518658, acc.: 84.38%] [G loss: 4.089436]\n",
      "643 [D loss: 0.179548, acc.: 98.44%] [G loss: 3.988076]\n",
      "644 [D loss: 0.326927, acc.: 96.09%] [G loss: 3.176675]\n",
      "645 [D loss: 0.700961, acc.: 78.12%] [G loss: 4.370254]\n",
      "646 [D loss: 0.556433, acc.: 95.31%] [G loss: 4.368786]\n",
      "647 [D loss: 0.226037, acc.: 96.88%] [G loss: 3.279099]\n",
      "648 [D loss: 0.225598, acc.: 88.28%] [G loss: 3.429086]\n",
      "649 [D loss: 0.465725, acc.: 86.72%] [G loss: 3.988438]\n",
      "650 [D loss: 0.911300, acc.: 89.84%] [G loss: 4.122456]\n",
      "651 [D loss: 0.385088, acc.: 92.97%] [G loss: 4.259780]\n",
      "652 [D loss: 0.438502, acc.: 96.09%] [G loss: 4.012884]\n",
      "653 [D loss: 0.570324, acc.: 96.09%] [G loss: 3.673697]\n",
      "654 [D loss: 0.341639, acc.: 96.09%] [G loss: 3.352138]\n",
      "655 [D loss: 0.477400, acc.: 96.09%] [G loss: 3.433121]\n",
      "656 [D loss: 0.062869, acc.: 100.00%] [G loss: 3.235933]\n",
      "657 [D loss: 0.462102, acc.: 94.53%] [G loss: 3.324629]\n",
      "658 [D loss: 0.518784, acc.: 90.62%] [G loss: 3.707271]\n",
      "659 [D loss: 0.547998, acc.: 96.09%] [G loss: 3.546524]\n",
      "660 [D loss: 0.330136, acc.: 96.88%] [G loss: 3.657458]\n",
      "661 [D loss: 0.622887, acc.: 92.19%] [G loss: 3.331830]\n",
      "662 [D loss: 0.313795, acc.: 91.41%] [G loss: 4.267628]\n",
      "663 [D loss: 0.280976, acc.: 97.66%] [G loss: 4.238960]\n",
      "664 [D loss: 0.803284, acc.: 93.75%] [G loss: 3.717367]\n",
      "665 [D loss: 1.046997, acc.: 84.38%] [G loss: 3.902935]\n",
      "666 [D loss: 0.774953, acc.: 78.12%] [G loss: 5.149221]\n",
      "667 [D loss: 0.663714, acc.: 84.38%] [G loss: 6.071102]\n",
      "668 [D loss: 0.489678, acc.: 85.94%] [G loss: 7.433683]\n",
      "669 [D loss: 0.254538, acc.: 98.44%] [G loss: 7.180223]\n",
      "670 [D loss: 0.638478, acc.: 96.09%] [G loss: 5.950980]\n",
      "671 [D loss: 0.429316, acc.: 95.31%] [G loss: 4.814612]\n",
      "672 [D loss: 0.902922, acc.: 94.53%] [G loss: 3.856765]\n",
      "673 [D loss: 0.560498, acc.: 96.88%] [G loss: 3.459177]\n",
      "674 [D loss: 0.578155, acc.: 92.97%] [G loss: 3.393390]\n",
      "675 [D loss: 1.065775, acc.: 93.75%] [G loss: 3.329573]\n",
      "676 [D loss: 0.652401, acc.: 92.97%] [G loss: 2.377359]\n",
      "677 [D loss: 0.126524, acc.: 96.09%] [G loss: 2.585197]\n",
      "678 [D loss: 0.092161, acc.: 96.09%] [G loss: 2.769830]\n",
      "679 [D loss: 0.123876, acc.: 98.44%] [G loss: 3.199891]\n",
      "680 [D loss: 0.379989, acc.: 92.97%] [G loss: 3.637475]\n",
      "681 [D loss: 0.323316, acc.: 98.44%] [G loss: 3.622553]\n",
      "682 [D loss: 0.531296, acc.: 96.88%] [G loss: 3.308607]\n",
      "683 [D loss: 0.303603, acc.: 98.44%] [G loss: 3.014441]\n",
      "684 [D loss: 0.052872, acc.: 100.00%] [G loss: 3.072139]\n",
      "685 [D loss: 0.075783, acc.: 96.88%] [G loss: 2.923715]\n",
      "686 [D loss: 0.234883, acc.: 96.09%] [G loss: 3.221483]\n",
      "687 [D loss: 0.208727, acc.: 97.66%] [G loss: 3.211727]\n",
      "688 [D loss: 0.188780, acc.: 98.44%] [G loss: 3.303369]\n",
      "689 [D loss: 0.300385, acc.: 97.66%] [G loss: 2.847826]\n",
      "690 [D loss: 0.059496, acc.: 99.22%] [G loss: 2.812475]\n",
      "691 [D loss: 0.077475, acc.: 98.44%] [G loss: 2.916487]\n",
      "692 [D loss: 0.177936, acc.: 98.44%] [G loss: 2.794203]\n",
      "693 [D loss: 0.181332, acc.: 99.22%] [G loss: 2.627976]\n",
      "694 [D loss: 0.218007, acc.: 98.44%] [G loss: 3.089362]\n",
      "695 [D loss: 0.089471, acc.: 96.09%] [G loss: 2.992373]\n",
      "696 [D loss: 0.477587, acc.: 89.06%] [G loss: 3.929215]\n",
      "697 [D loss: 1.082941, acc.: 75.78%] [G loss: 5.317574]\n",
      "698 [D loss: 0.029170, acc.: 99.22%] [G loss: 5.424610]\n",
      "699 [D loss: 0.296181, acc.: 97.66%] [G loss: 4.557829]\n",
      "700 [D loss: 0.019085, acc.: 100.00%] [G loss: 4.126057]\n",
      "701 [D loss: 0.539762, acc.: 96.09%] [G loss: 3.390895]\n",
      "702 [D loss: 0.063575, acc.: 96.88%] [G loss: 3.304407]\n",
      "703 [D loss: 0.414770, acc.: 97.66%] [G loss: 3.155692]\n",
      "704 [D loss: 0.191735, acc.: 98.44%] [G loss: 2.851303]\n",
      "705 [D loss: 0.317435, acc.: 97.66%] [G loss: 2.807598]\n",
      "706 [D loss: 0.561347, acc.: 96.09%] [G loss: 2.776718]\n",
      "707 [D loss: 0.172343, acc.: 99.22%] [G loss: 2.722616]\n",
      "708 [D loss: 0.473393, acc.: 92.97%] [G loss: 2.740920]\n",
      "709 [D loss: 0.463151, acc.: 86.72%] [G loss: 3.790669]\n",
      "710 [D loss: 0.878177, acc.: 83.59%] [G loss: 4.283175]\n",
      "711 [D loss: 0.523296, acc.: 88.28%] [G loss: 4.743866]\n",
      "712 [D loss: 0.014333, acc.: 100.00%] [G loss: 4.271259]\n",
      "713 [D loss: 0.658856, acc.: 95.31%] [G loss: 4.053521]\n",
      "714 [D loss: 0.551577, acc.: 96.88%] [G loss: 3.888223]\n",
      "715 [D loss: 0.658633, acc.: 96.09%] [G loss: 3.422208]\n",
      "716 [D loss: 0.566987, acc.: 96.09%] [G loss: 3.273047]\n",
      "717 [D loss: 0.158994, acc.: 99.22%] [G loss: 2.834969]\n",
      "718 [D loss: 0.432152, acc.: 96.88%] [G loss: 2.833229]\n",
      "719 [D loss: 0.565085, acc.: 96.09%] [G loss: 2.841532]\n",
      "720 [D loss: 0.575814, acc.: 96.09%] [G loss: 3.123080]\n",
      "721 [D loss: 0.305209, acc.: 98.44%] [G loss: 3.208928]\n",
      "722 [D loss: 0.304967, acc.: 98.44%] [G loss: 2.842300]\n",
      "723 [D loss: 0.453437, acc.: 97.66%] [G loss: 2.837436]\n",
      "724 [D loss: 0.048377, acc.: 100.00%] [G loss: 2.781321]\n",
      "725 [D loss: 0.560889, acc.: 96.88%] [G loss: 2.679864]\n",
      "726 [D loss: 0.360950, acc.: 94.53%] [G loss: 2.841267]\n",
      "727 [D loss: 0.309571, acc.: 96.09%] [G loss: 3.047835]\n",
      "728 [D loss: 0.331222, acc.: 95.31%] [G loss: 3.121105]\n",
      "729 [D loss: 0.360098, acc.: 95.31%] [G loss: 3.584579]\n",
      "730 [D loss: 0.426455, acc.: 97.66%] [G loss: 4.258741]\n",
      "731 [D loss: 0.263297, acc.: 98.44%] [G loss: 3.806918]\n",
      "732 [D loss: 0.163558, acc.: 99.22%] [G loss: 3.105313]\n",
      "733 [D loss: 0.550968, acc.: 96.88%] [G loss: 2.950282]\n",
      "734 [D loss: 0.051664, acc.: 100.00%] [G loss: 2.651578]\n",
      "735 [D loss: 0.180286, acc.: 99.22%] [G loss: 2.664962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736 [D loss: 0.294067, acc.: 98.44%] [G loss: 2.602633]\n",
      "737 [D loss: 0.179680, acc.: 99.22%] [G loss: 2.659523]\n",
      "738 [D loss: 0.415332, acc.: 97.66%] [G loss: 2.734788]\n",
      "739 [D loss: 0.437124, acc.: 96.09%] [G loss: 3.012193]\n",
      "740 [D loss: 0.702173, acc.: 85.16%] [G loss: 3.789816]\n",
      "741 [D loss: 0.656577, acc.: 82.81%] [G loss: 4.797854]\n",
      "742 [D loss: 0.023750, acc.: 100.00%] [G loss: 5.060510]\n",
      "743 [D loss: 0.577953, acc.: 96.88%] [G loss: 5.082690]\n",
      "744 [D loss: 0.274759, acc.: 98.44%] [G loss: 5.152535]\n",
      "745 [D loss: 0.288162, acc.: 98.44%] [G loss: 4.205429]\n",
      "746 [D loss: 0.519040, acc.: 96.88%] [G loss: 3.807043]\n",
      "747 [D loss: 0.271566, acc.: 98.44%] [G loss: 3.268416]\n",
      "748 [D loss: 0.407209, acc.: 97.66%] [G loss: 3.029082]\n",
      "749 [D loss: 0.034847, acc.: 100.00%] [G loss: 2.994687]\n",
      "750 [D loss: 0.282380, acc.: 98.44%] [G loss: 2.781832]\n",
      "751 [D loss: 0.035643, acc.: 100.00%] [G loss: 3.018216]\n",
      "752 [D loss: 0.549118, acc.: 96.88%] [G loss: 2.949282]\n",
      "753 [D loss: 0.193419, acc.: 96.88%] [G loss: 2.623126]\n",
      "754 [D loss: 0.180711, acc.: 99.22%] [G loss: 2.639833]\n",
      "755 [D loss: 0.692011, acc.: 95.31%] [G loss: 2.751177]\n",
      "756 [D loss: 0.302413, acc.: 98.44%] [G loss: 2.865606]\n",
      "757 [D loss: 0.700586, acc.: 96.09%] [G loss: 2.991022]\n",
      "758 [D loss: 0.684524, acc.: 96.09%] [G loss: 3.043545]\n",
      "759 [D loss: 0.156932, acc.: 99.22%] [G loss: 2.580286]\n",
      "760 [D loss: 0.166326, acc.: 99.22%] [G loss: 2.619836]\n",
      "761 [D loss: 0.046745, acc.: 100.00%] [G loss: 2.664143]\n",
      "762 [D loss: 0.152396, acc.: 99.22%] [G loss: 3.030067]\n",
      "763 [D loss: 0.273918, acc.: 98.44%] [G loss: 3.147327]\n",
      "764 [D loss: 0.280275, acc.: 98.44%] [G loss: 2.730009]\n",
      "765 [D loss: 0.322509, acc.: 96.88%] [G loss: 2.370053]\n",
      "766 [D loss: 0.625260, acc.: 94.53%] [G loss: 3.281007]\n",
      "767 [D loss: 0.544031, acc.: 88.28%] [G loss: 4.295876]\n",
      "768 [D loss: 0.211044, acc.: 94.53%] [G loss: 4.259562]\n",
      "769 [D loss: 0.023851, acc.: 100.00%] [G loss: 3.904915]\n",
      "770 [D loss: 0.029500, acc.: 100.00%] [G loss: 3.448369]\n",
      "771 [D loss: 0.152474, acc.: 99.22%] [G loss: 3.190226]\n",
      "772 [D loss: 0.148611, acc.: 99.22%] [G loss: 3.202506]\n",
      "773 [D loss: 0.026445, acc.: 100.00%] [G loss: 2.948860]\n",
      "774 [D loss: 0.277553, acc.: 98.44%] [G loss: 2.920239]\n",
      "775 [D loss: 0.282154, acc.: 98.44%] [G loss: 2.779285]\n",
      "776 [D loss: 0.282546, acc.: 98.44%] [G loss: 2.570758]\n",
      "777 [D loss: 0.550688, acc.: 96.09%] [G loss: 2.394918]\n",
      "778 [D loss: 0.297492, acc.: 98.44%] [G loss: 2.482120]\n",
      "779 [D loss: 0.029349, acc.: 100.00%] [G loss: 2.720158]\n",
      "780 [D loss: 0.274690, acc.: 98.44%] [G loss: 3.138713]\n",
      "781 [D loss: 0.527827, acc.: 96.88%] [G loss: 3.172024]\n",
      "782 [D loss: 0.039974, acc.: 99.22%] [G loss: 2.830570]\n",
      "783 [D loss: 0.180294, acc.: 99.22%] [G loss: 2.906990]\n",
      "784 [D loss: 0.315902, acc.: 98.44%] [G loss: 3.266167]\n",
      "785 [D loss: 0.404645, acc.: 97.66%] [G loss: 3.357882]\n",
      "786 [D loss: 0.147721, acc.: 99.22%] [G loss: 3.026318]\n",
      "787 [D loss: 0.275037, acc.: 98.44%] [G loss: 2.784806]\n",
      "788 [D loss: 0.279998, acc.: 98.44%] [G loss: 2.613319]\n",
      "789 [D loss: 0.027764, acc.: 100.00%] [G loss: 2.517476]\n",
      "790 [D loss: 0.408231, acc.: 97.66%] [G loss: 2.494852]\n",
      "791 [D loss: 0.159854, acc.: 99.22%] [G loss: 2.566026]\n",
      "792 [D loss: 0.159216, acc.: 99.22%] [G loss: 2.717108]\n",
      "793 [D loss: 0.401753, acc.: 97.66%] [G loss: 2.842741]\n",
      "794 [D loss: 0.151427, acc.: 99.22%] [G loss: 2.859033]\n",
      "795 [D loss: 0.022734, acc.: 100.00%] [G loss: 2.797366]\n",
      "796 [D loss: 0.523902, acc.: 96.88%] [G loss: 2.792111]\n",
      "797 [D loss: 0.273705, acc.: 98.44%] [G loss: 2.682294]\n",
      "798 [D loss: 0.283282, acc.: 98.44%] [G loss: 2.602440]\n",
      "799 [D loss: 0.306035, acc.: 98.44%] [G loss: 2.465631]\n",
      "800 [D loss: 0.363989, acc.: 91.41%] [G loss: 3.436317]\n",
      "801 [D loss: 0.749488, acc.: 74.22%] [G loss: 6.001081]\n",
      "802 [D loss: 1.118557, acc.: 83.59%] [G loss: 7.834732]\n",
      "803 [D loss: 0.629712, acc.: 96.09%] [G loss: 8.282195]\n",
      "804 [D loss: 0.300600, acc.: 97.66%] [G loss: 6.285257]\n",
      "805 [D loss: 0.142993, acc.: 98.44%] [G loss: 4.673710]\n",
      "806 [D loss: 0.150392, acc.: 99.22%] [G loss: 3.946133]\n",
      "807 [D loss: 0.013973, acc.: 100.00%] [G loss: 3.447796]\n",
      "808 [D loss: 0.026087, acc.: 100.00%] [G loss: 2.855366]\n",
      "809 [D loss: 0.042726, acc.: 100.00%] [G loss: 2.640207]\n",
      "810 [D loss: 0.029759, acc.: 100.00%] [G loss: 2.691915]\n",
      "811 [D loss: 0.052016, acc.: 99.22%] [G loss: 2.388855]\n",
      "812 [D loss: 0.102382, acc.: 98.44%] [G loss: 3.185446]\n",
      "813 [D loss: 0.112433, acc.: 97.66%] [G loss: 3.227811]\n",
      "814 [D loss: 0.140349, acc.: 99.22%] [G loss: 3.378180]\n",
      "815 [D loss: 0.151789, acc.: 99.22%] [G loss: 2.955846]\n",
      "816 [D loss: 0.023620, acc.: 100.00%] [G loss: 2.847329]\n",
      "817 [D loss: 0.157336, acc.: 99.22%] [G loss: 2.611405]\n",
      "818 [D loss: 0.040903, acc.: 100.00%] [G loss: 2.460729]\n",
      "819 [D loss: 0.040763, acc.: 100.00%] [G loss: 2.369272]\n",
      "820 [D loss: 0.168353, acc.: 99.22%] [G loss: 2.324347]\n",
      "821 [D loss: 0.165829, acc.: 99.22%] [G loss: 2.374829]\n",
      "822 [D loss: 0.063060, acc.: 97.66%] [G loss: 2.173023]\n",
      "823 [D loss: 0.108495, acc.: 96.88%] [G loss: 3.303341]\n",
      "824 [D loss: 0.521764, acc.: 75.00%] [G loss: 3.780134]\n",
      "825 [D loss: 0.570429, acc.: 85.16%] [G loss: 6.011268]\n",
      "826 [D loss: 0.083741, acc.: 94.53%] [G loss: 6.227189]\n",
      "827 [D loss: 0.013948, acc.: 100.00%] [G loss: 5.072400]\n",
      "828 [D loss: 0.141655, acc.: 99.22%] [G loss: 4.080290]\n",
      "829 [D loss: 0.265025, acc.: 92.19%] [G loss: 2.320691]\n",
      "830 [D loss: 0.347936, acc.: 84.38%] [G loss: 1.679754]\n",
      "831 [D loss: 0.682298, acc.: 69.53%] [G loss: 6.471322]\n",
      "832 [D loss: 0.812062, acc.: 85.94%] [G loss: 7.212264]\n",
      "833 [D loss: 0.344586, acc.: 94.53%] [G loss: 6.264105]\n",
      "834 [D loss: 0.042042, acc.: 99.22%] [G loss: 4.334947]\n",
      "835 [D loss: 0.124553, acc.: 93.75%] [G loss: 3.440722]\n",
      "836 [D loss: 0.181607, acc.: 92.97%] [G loss: 2.820152]\n",
      "837 [D loss: 0.501619, acc.: 72.66%] [G loss: 4.699632]\n",
      "838 [D loss: 0.728747, acc.: 88.28%] [G loss: 5.176278]\n",
      "839 [D loss: 0.191214, acc.: 91.41%] [G loss: 3.604132]\n",
      "840 [D loss: 0.732071, acc.: 74.22%] [G loss: 4.868596]\n",
      "841 [D loss: 0.434683, acc.: 83.59%] [G loss: 5.440883]\n",
      "842 [D loss: 0.261545, acc.: 86.72%] [G loss: 4.021016]\n",
      "843 [D loss: 0.242796, acc.: 86.72%] [G loss: 3.667784]\n",
      "844 [D loss: 0.497724, acc.: 80.47%] [G loss: 3.552114]\n",
      "845 [D loss: 0.438339, acc.: 84.38%] [G loss: 3.671251]\n",
      "846 [D loss: 0.378342, acc.: 84.38%] [G loss: 3.035445]\n",
      "847 [D loss: 0.734800, acc.: 76.56%] [G loss: 3.539612]\n",
      "848 [D loss: 0.139643, acc.: 96.09%] [G loss: 3.794253]\n",
      "849 [D loss: 0.261735, acc.: 86.72%] [G loss: 3.283747]\n",
      "850 [D loss: 0.203024, acc.: 91.41%] [G loss: 2.797099]\n",
      "851 [D loss: 0.400922, acc.: 75.00%] [G loss: 3.467673]\n",
      "852 [D loss: 0.254440, acc.: 86.72%] [G loss: 3.377697]\n",
      "853 [D loss: 0.766141, acc.: 72.66%] [G loss: 4.358245]\n",
      "854 [D loss: 0.535416, acc.: 82.81%] [G loss: 4.948730]\n",
      "855 [D loss: 0.345512, acc.: 86.72%] [G loss: 4.913438]\n",
      "856 [D loss: 0.419660, acc.: 97.66%] [G loss: 3.713866]\n",
      "857 [D loss: 0.381159, acc.: 83.59%] [G loss: 3.479791]\n",
      "858 [D loss: 0.142768, acc.: 93.75%] [G loss: 3.446031]\n",
      "859 [D loss: 0.297223, acc.: 91.41%] [G loss: 3.268111]\n",
      "860 [D loss: 0.348382, acc.: 87.50%] [G loss: 3.709218]\n",
      "861 [D loss: 0.203848, acc.: 96.88%] [G loss: 3.668659]\n",
      "862 [D loss: 0.392852, acc.: 93.75%] [G loss: 3.518451]\n",
      "863 [D loss: 0.372035, acc.: 92.97%] [G loss: 3.287578]\n",
      "864 [D loss: 0.325116, acc.: 90.62%] [G loss: 3.961018]\n",
      "865 [D loss: 0.193005, acc.: 99.22%] [G loss: 3.838382]\n",
      "866 [D loss: 0.702159, acc.: 86.72%] [G loss: 4.220294]\n",
      "867 [D loss: 0.492980, acc.: 94.53%] [G loss: 4.059439]\n",
      "868 [D loss: 0.546975, acc.: 88.28%] [G loss: 3.708366]\n",
      "869 [D loss: 0.381019, acc.: 95.31%] [G loss: 3.536131]\n",
      "870 [D loss: 0.493153, acc.: 84.38%] [G loss: 4.346717]\n",
      "871 [D loss: 0.436138, acc.: 88.28%] [G loss: 4.709307]\n",
      "872 [D loss: 0.808654, acc.: 78.91%] [G loss: 5.405618]\n",
      "873 [D loss: 0.556256, acc.: 78.91%] [G loss: 6.293497]\n",
      "874 [D loss: 0.561770, acc.: 89.06%] [G loss: 6.935749]\n",
      "875 [D loss: 0.420478, acc.: 97.66%] [G loss: 6.090888]\n",
      "876 [D loss: 0.314764, acc.: 96.09%] [G loss: 5.537722]\n",
      "877 [D loss: 0.453672, acc.: 95.31%] [G loss: 5.131453]\n",
      "878 [D loss: 0.305957, acc.: 97.66%] [G loss: 5.313378]\n",
      "879 [D loss: 0.706871, acc.: 92.19%] [G loss: 4.953263]\n",
      "880 [D loss: 0.500461, acc.: 92.19%] [G loss: 4.923031]\n",
      "881 [D loss: 0.183897, acc.: 96.09%] [G loss: 3.895925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882 [D loss: 0.144013, acc.: 92.97%] [G loss: 3.897709]\n",
      "883 [D loss: 0.569002, acc.: 94.53%] [G loss: 3.971955]\n",
      "884 [D loss: 0.509371, acc.: 96.88%] [G loss: 3.678460]\n",
      "885 [D loss: 0.319913, acc.: 97.66%] [G loss: 3.780095]\n",
      "886 [D loss: 0.046251, acc.: 100.00%] [G loss: 3.431382]\n",
      "887 [D loss: 0.558218, acc.: 96.88%] [G loss: 3.210096]\n",
      "888 [D loss: 0.339138, acc.: 96.88%] [G loss: 3.110608]\n",
      "889 [D loss: 0.436654, acc.: 97.66%] [G loss: 3.294143]\n",
      "890 [D loss: 0.286949, acc.: 98.44%] [G loss: 3.439409]\n",
      "891 [D loss: 0.706659, acc.: 93.75%] [G loss: 3.361472]\n",
      "892 [D loss: 0.204999, acc.: 99.22%] [G loss: 3.790366]\n",
      "893 [D loss: 0.665570, acc.: 95.31%] [G loss: 3.612642]\n",
      "894 [D loss: 0.205533, acc.: 95.31%] [G loss: 3.216388]\n",
      "895 [D loss: 1.241030, acc.: 71.09%] [G loss: 3.550195]\n",
      "896 [D loss: 1.048190, acc.: 81.25%] [G loss: 5.485571]\n",
      "897 [D loss: 0.729389, acc.: 85.94%] [G loss: 6.251943]\n",
      "898 [D loss: 0.037449, acc.: 100.00%] [G loss: 5.639144]\n",
      "899 [D loss: 0.191133, acc.: 99.22%] [G loss: 4.813396]\n",
      "900 [D loss: 0.035712, acc.: 100.00%] [G loss: 4.051441]\n",
      "901 [D loss: 0.060644, acc.: 99.22%] [G loss: 3.534002]\n",
      "902 [D loss: 0.104511, acc.: 96.88%] [G loss: 2.889973]\n",
      "903 [D loss: 0.328826, acc.: 81.25%] [G loss: 4.824294]\n",
      "904 [D loss: 0.078707, acc.: 95.31%] [G loss: 5.133792]\n",
      "905 [D loss: 0.061363, acc.: 98.44%] [G loss: 4.032679]\n",
      "906 [D loss: 0.124091, acc.: 95.31%] [G loss: 3.411553]\n",
      "907 [D loss: 0.049342, acc.: 100.00%] [G loss: 2.909410]\n",
      "908 [D loss: 0.147404, acc.: 94.53%] [G loss: 2.502438]\n",
      "909 [D loss: 0.316701, acc.: 89.06%] [G loss: 3.323451]\n",
      "910 [D loss: 0.345023, acc.: 92.97%] [G loss: 3.638376]\n",
      "911 [D loss: 0.339219, acc.: 81.25%] [G loss: 4.029304]\n",
      "912 [D loss: 0.394553, acc.: 79.69%] [G loss: 5.471945]\n",
      "913 [D loss: 0.014572, acc.: 100.00%] [G loss: 5.162970]\n",
      "914 [D loss: 0.220737, acc.: 92.97%] [G loss: 4.710380]\n",
      "915 [D loss: 0.121995, acc.: 93.75%] [G loss: 4.875233]\n",
      "916 [D loss: 0.061285, acc.: 99.22%] [G loss: 3.955326]\n",
      "917 [D loss: 0.269694, acc.: 91.41%] [G loss: 3.463927]\n",
      "918 [D loss: 0.106943, acc.: 95.31%] [G loss: 2.981030]\n",
      "919 [D loss: 0.139015, acc.: 92.97%] [G loss: 2.846429]\n",
      "920 [D loss: 0.179882, acc.: 90.62%] [G loss: 3.475673]\n",
      "921 [D loss: 0.107773, acc.: 96.88%] [G loss: 2.857433]\n",
      "922 [D loss: 0.412343, acc.: 82.03%] [G loss: 3.739363]\n",
      "923 [D loss: 0.136664, acc.: 97.66%] [G loss: 4.077058]\n",
      "924 [D loss: 0.235055, acc.: 84.38%] [G loss: 4.304500]\n",
      "925 [D loss: 0.892969, acc.: 72.66%] [G loss: 5.312970]\n",
      "926 [D loss: 0.323550, acc.: 89.84%] [G loss: 6.574734]\n",
      "927 [D loss: 0.297460, acc.: 98.44%] [G loss: 4.741130]\n",
      "928 [D loss: 0.263701, acc.: 92.97%] [G loss: 4.512133]\n",
      "929 [D loss: 0.054721, acc.: 99.22%] [G loss: 4.024720]\n",
      "930 [D loss: 0.064476, acc.: 98.44%] [G loss: 3.345447]\n",
      "931 [D loss: 0.527194, acc.: 92.19%] [G loss: 3.441015]\n",
      "932 [D loss: 0.146363, acc.: 92.97%] [G loss: 3.510128]\n",
      "933 [D loss: 0.370702, acc.: 95.31%] [G loss: 3.897923]\n",
      "934 [D loss: 0.105035, acc.: 97.66%] [G loss: 3.735671]\n",
      "935 [D loss: 0.384305, acc.: 94.53%] [G loss: 3.852157]\n",
      "936 [D loss: 0.182656, acc.: 99.22%] [G loss: 3.827579]\n",
      "937 [D loss: 0.325639, acc.: 96.09%] [G loss: 2.980824]\n",
      "938 [D loss: 0.402864, acc.: 93.75%] [G loss: 2.868473]\n",
      "939 [D loss: 0.361056, acc.: 82.81%] [G loss: 4.767313]\n",
      "940 [D loss: 0.596386, acc.: 88.28%] [G loss: 5.445107]\n",
      "941 [D loss: 0.599935, acc.: 87.50%] [G loss: 5.345806]\n",
      "942 [D loss: 0.365337, acc.: 82.03%] [G loss: 6.222060]\n",
      "943 [D loss: 0.255250, acc.: 98.44%] [G loss: 5.916676]\n",
      "944 [D loss: 0.640062, acc.: 96.09%] [G loss: 4.554976]\n",
      "945 [D loss: 0.172827, acc.: 99.22%] [G loss: 3.394892]\n",
      "946 [D loss: 0.446405, acc.: 96.09%] [G loss: 2.914658]\n",
      "947 [D loss: 0.346743, acc.: 96.88%] [G loss: 2.818992]\n",
      "948 [D loss: 0.457161, acc.: 95.31%] [G loss: 2.775311]\n",
      "949 [D loss: 0.335595, acc.: 96.88%] [G loss: 2.717145]\n",
      "950 [D loss: 0.208894, acc.: 96.88%] [G loss: 3.057632]\n",
      "951 [D loss: 0.443863, acc.: 96.09%] [G loss: 3.337395]\n",
      "952 [D loss: 0.585923, acc.: 94.53%] [G loss: 3.667907]\n",
      "953 [D loss: 0.662489, acc.: 92.19%] [G loss: 3.541617]\n",
      "954 [D loss: 0.680302, acc.: 86.72%] [G loss: 4.184085]\n",
      "955 [D loss: 0.397175, acc.: 97.66%] [G loss: 4.565609]\n",
      "956 [D loss: 0.270225, acc.: 98.44%] [G loss: 3.527853]\n",
      "957 [D loss: 0.422576, acc.: 97.66%] [G loss: 3.260928]\n",
      "958 [D loss: 0.167492, acc.: 99.22%] [G loss: 2.926939]\n",
      "959 [D loss: 0.315331, acc.: 97.66%] [G loss: 2.593905]\n",
      "960 [D loss: 1.099574, acc.: 91.41%] [G loss: 2.823500]\n",
      "961 [D loss: 0.739728, acc.: 89.84%] [G loss: 3.310551]\n",
      "962 [D loss: 0.796102, acc.: 89.84%] [G loss: 4.025904]\n",
      "963 [D loss: 0.395436, acc.: 97.66%] [G loss: 4.438976]\n",
      "964 [D loss: 0.510609, acc.: 96.88%] [G loss: 3.920776]\n",
      "965 [D loss: 0.521073, acc.: 96.88%] [G loss: 3.435588]\n",
      "966 [D loss: 0.671126, acc.: 96.09%] [G loss: 3.164742]\n",
      "967 [D loss: 0.285899, acc.: 97.66%] [G loss: 3.121728]\n",
      "968 [D loss: 0.292331, acc.: 98.44%] [G loss: 2.729505]\n",
      "969 [D loss: 0.696802, acc.: 94.53%] [G loss: 2.629626]\n",
      "970 [D loss: 0.090637, acc.: 97.66%] [G loss: 3.315105]\n",
      "971 [D loss: 0.870710, acc.: 78.12%] [G loss: 3.981001]\n",
      "972 [D loss: 0.494165, acc.: 89.84%] [G loss: 4.964490]\n",
      "973 [D loss: 0.890422, acc.: 94.53%] [G loss: 4.932217]\n",
      "974 [D loss: 0.273911, acc.: 98.44%] [G loss: 3.945130]\n",
      "975 [D loss: 0.394401, acc.: 97.66%] [G loss: 3.588773]\n",
      "976 [D loss: 0.528077, acc.: 96.88%] [G loss: 3.193325]\n",
      "977 [D loss: 0.274282, acc.: 98.44%] [G loss: 3.121886]\n",
      "978 [D loss: 0.408305, acc.: 97.66%] [G loss: 2.999306]\n",
      "979 [D loss: 0.288205, acc.: 98.44%] [G loss: 3.107912]\n",
      "980 [D loss: 0.274836, acc.: 98.44%] [G loss: 3.309081]\n",
      "981 [D loss: 0.423815, acc.: 96.88%] [G loss: 3.113911]\n",
      "982 [D loss: 0.471851, acc.: 94.53%] [G loss: 2.310520]\n",
      "983 [D loss: 0.464476, acc.: 96.09%] [G loss: 2.636217]\n",
      "984 [D loss: 0.358629, acc.: 95.31%] [G loss: 2.809326]\n",
      "985 [D loss: 0.201128, acc.: 96.88%] [G loss: 3.361893]\n",
      "986 [D loss: 0.306837, acc.: 98.44%] [G loss: 4.204586]\n",
      "987 [D loss: 0.266449, acc.: 98.44%] [G loss: 3.724673]\n",
      "988 [D loss: 0.136200, acc.: 99.22%] [G loss: 3.520685]\n",
      "989 [D loss: 0.146886, acc.: 99.22%] [G loss: 2.778925]\n",
      "990 [D loss: 0.281644, acc.: 98.44%] [G loss: 2.648170]\n",
      "991 [D loss: 0.165729, acc.: 99.22%] [G loss: 2.695941]\n",
      "992 [D loss: 0.179718, acc.: 97.66%] [G loss: 2.550577]\n",
      "993 [D loss: 0.073076, acc.: 98.44%] [G loss: 2.419430]\n",
      "994 [D loss: 0.175124, acc.: 99.22%] [G loss: 2.397697]\n",
      "995 [D loss: 0.160305, acc.: 99.22%] [G loss: 2.616564]\n",
      "996 [D loss: 0.280750, acc.: 98.44%] [G loss: 2.772485]\n",
      "997 [D loss: 0.032509, acc.: 100.00%] [G loss: 2.690737]\n",
      "998 [D loss: 0.161207, acc.: 99.22%] [G loss: 2.755805]\n",
      "999 [D loss: 0.204514, acc.: 94.53%] [G loss: 2.736323]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1000, data=X_test_seq_trunc) # Generate data similar to the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model trained on the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82815, 5260) (82815,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_seq_trunc.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (8282, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb2, X_valid_emb2, y_train_emb2, y_valid_emb2 = train_test_split(X_test_seq_trunc, y_test, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb2.shape[0] == y_valid_emb2.shape[0]\n",
    "assert X_train_emb2.shape[0] == y_train_emb2.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model2 = Sequential()\n",
    "glove_model2.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model2.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model2.add(Flatten())\n",
    "glove_model2.add(Dense(1, activation='sigmoid'))\n",
    "glove_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model2.layers[0].set_weights([emb_matrix])\n",
    "glove_model2.layers[0].trainable = False\n",
    "\n",
    "glove_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74533 samples, validate on 8282 samples\n",
      "Epoch 1/1\n",
      "74533/74533 [==============================] - 273s 4ms/step - loss: 0.4480 - acc: 0.8467 - val_loss: 0.4073 - val_acc: 0.8673\n"
     ]
    }
   ],
   "source": [
    "history2 = glove_model2.fit(X_train_emb2\n",
    "                       , y_train_emb2\n",
    "                       , epochs=1\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb2, y_valid_emb2)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9684406]]\n",
      "(5260,)\n"
     ]
    }
   ],
   "source": [
    "# print(glove_model2.predict(X_train_emb[0:1]))\n",
    "# print(X_train_emb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model trained on a generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = 10000\n",
    "noise = np.random.normal(0, 1, (gen, 100))\n",
    "gen_samp = np.absolute((generator.predict(noise)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = glove_model.predict((gen_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "prediction = np.round(prediction)\n",
    "# print(np.round(prediction[0:100]))\n",
    "print(np.sum(np.round(glove_model.predict(X_train_emb[0:100]))))\n",
    "print(np.sum(y_train[0:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (3000, 5260)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb3, X_valid_emb3, y_train_emb3, y_valid_emb3 = train_test_split(gen_samp, prediction, test_size=0.3, random_state=37)\n",
    "\n",
    "assert X_valid_emb3.shape[0] == y_valid_emb3.shape[0]\n",
    "assert X_train_emb3.shape[0] == y_train_emb3.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 5260, 300)         14934300  \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1578000)           0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 1578001   \n",
      "=================================================================\n",
      "Total params: 16,512,301\n",
      "Trainable params: 16,512,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model3 = Sequential()\n",
    "glove_model3.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "# glove_model3.add(LSTM(GLOVE_DIM, return_sequences=True))\n",
    "glove_model3.add(Flatten())\n",
    "glove_model3.add(Dense(1, activation='sigmoid'))\n",
    "glove_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model3.layers[0].set_weights([emb_matrix])\n",
    "glove_model3.layers[0].trainable = False\n",
    "\n",
    "glove_model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 31s 4ms/step - loss: 0.4388 - acc: 0.9861 - val_loss: 0.3956 - val_acc: 0.9957\n"
     ]
    }
   ],
   "source": [
    "history3 = glove_model3.fit(X_train_emb3\n",
    "                       , y_train_emb3\n",
    "                       , epochs=1\n",
    "                       , batch_size=32\n",
    "                       , validation_data=(X_valid_emb3, y_valid_emb3)\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this area I am going to compare their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb3\n",
    "pred = np.round(glove_model2.predict(X_valid_emb3))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[  57    1]\n",
      " [   3 2939]]\n",
      "Accuracy Score : 0.9986666666666667\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.98      0.97        58\n",
      "         1.0       1.00      1.00      1.00      2942\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3000\n",
      "   macro avg       0.97      0.99      0.98      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[  55    3]\n",
      " [  10 2932]]\n",
      "Accuracy Score : 0.9956666666666667\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.95      0.89        58\n",
      "         1.0       1.00      1.00      1.00      2942\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3000\n",
      "   macro avg       0.92      0.97      0.95      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model trained on the generated dataset over the real dataset\n",
    "actual = y_valid_emb2\n",
    "pred = np.round(glove_model2.predict(X_valid_emb2))\n",
    "pred2 = np.round(glove_model3.predict(X_valid_emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[1246 1462]\n",
      " [ 170 5404]]\n",
      "Accuracy Score : 0.802946148273364\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.46      0.60      2708\n",
      "         1.0       0.79      0.97      0.87      5574\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      8282\n",
      "   macro avg       0.83      0.71      0.74      8282\n",
      "weighted avg       0.82      0.80      0.78      8282\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[  53 2655]\n",
      " [  83 5491]]\n",
      "Accuracy Score : 0.6694035257184255\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.02      0.04      2708\n",
      "         1.0       0.67      0.99      0.80      5574\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8282\n",
      "   macro avg       0.53      0.50      0.42      8282\n",
      "weighted avg       0.58      0.67      0.55      8282\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(pred, actual) # trained on original over real\n",
    "results(pred2, actual) # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
